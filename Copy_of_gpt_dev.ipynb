{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagarajanjayabal/NagaAI/blob/main/Copy_of_gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "4d9d714a-6878-44ac-d94f-799e6abd99bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 21:41:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-03-26 21:41:03 (12.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# read it in to inspect it\n",
        "with open(r'input.txt', 'r', encoding='utf-8') as f:\n",
        "\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9",
        "outputId": "f9ff0abc-4346-4fdb-cae1-5d7a156ac2b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 21:41:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-03-26 21:41:20 (12.7 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "c24d7338-377f-4f25-8fdf-f2d6610d8ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "be3a3110-c32f-47f7-8a83-7f773e37e2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "#testchar = set(text)\n",
        "#print('.',testchar)\n",
        "#listtestchar = list(testchar)\n",
        "#print('..',len(sorted(listtestchar)))\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "0b44d252-0bc6-4887-8c6f-f834b293dd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "#test55 = enumerate(chars)\n",
        "#stoi = { i:ch for i,ch in enumerate(chars) }\n",
        "#print('..',test55)\n",
        "#print('##',stoi)\n",
        "#stoi1 = { stoi[c] for c in 'Hi'}\n",
        "#print('##',stoi1)\n",
        "#decode = lambda l: ([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "269c4c51-9e3a-4507-eb99-ba09f0510ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "97b05407-18dc-4268-f3c0-79aada8ebcf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "d0e0d52d-3d04-4a69-99bf-c21be493b4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "0b7f3165-e0fe-433c-b258-7ae6c620105f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    print('data',data)\n",
        "    print('len',len(data) - block_size)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    print('ix',ix)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "9a8e8061-3538-43f2-c4da-52226b6b0d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data tensor([18, 47, 56,  ..., 43, 56, 43])\n",
            "len 1003846\n",
            "ix tensor([ 76049, 234249, 934904, 560986])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer\n",
        "print(yb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "9d60ce2a-26a5-4746-9e81-0a40f67e9dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        #print('idx in FWD',idx)\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        #print('logits in FWD',logits)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            print('logits in FWD',logits)\n",
        "            targets = targets.view(B*T)\n",
        "            print('targets in FWD',targets)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        print('idx',idx)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            print('logits in generate',logits)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            print('logits in generate after -1',logits)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            print('probs in generate after softmax',probs)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            print('idx_next in generate',idx_next)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            print('final idx in generate after cat',idx)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5596cd00-c417-4a20-a2e0-91d16862b511"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits in FWD tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
            "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "        ...,\n",
            "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
            "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "targets in FWD tensor([43, 58,  5, 57,  1, 46, 43, 39, 53, 56,  1, 58, 46, 39, 58,  1, 58,  1,\n",
            "        58, 46, 39, 58,  1, 46, 17, 27, 10,  0, 21,  1, 54, 39])\n",
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "idx tensor([[0]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
            "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
            "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
            "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
            "         -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
            "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
            "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
            "         -0.8345]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0091, 0.0071, 0.0053, 0.0030, 0.0141, 0.0078, 0.0197, 0.0081, 0.0109,\n",
            "         0.0243, 0.0020, 0.0045, 0.0096, 0.0060, 0.0030, 0.0354, 0.0292, 0.0066,\n",
            "         0.0101, 0.0199, 0.0010, 0.0124, 0.0335, 0.0137, 0.0086, 0.0016, 0.0024,\n",
            "         0.0054, 0.0118, 0.0034, 0.0347, 0.0930, 0.0039, 0.0059, 0.0208, 0.0085,\n",
            "         0.0089, 0.0235, 0.0024, 0.0056, 0.0046, 0.0030, 0.0131, 0.0017, 0.0023,\n",
            "         0.0134, 0.0042, 0.0038, 0.0392, 0.0034, 0.0292, 0.0057, 0.0017, 0.0621,\n",
            "         0.1199, 0.0013, 0.0323, 0.0017, 0.0172, 0.0061, 0.0165, 0.0351, 0.0379,\n",
            "         0.0051, 0.0033]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[31]])\n",
            "final idx in generate after cat tensor([[ 0, 31]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
            "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
            "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
            "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
            "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
            "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
            "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
            "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
            "           0.2930,  0.0943]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,\n",
            "          1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726,\n",
            "         -0.4725, -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664,\n",
            "         -0.9238, -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422,\n",
            "         -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,\n",
            "          1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,\n",
            "          1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "          0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,\n",
            "          0.0943]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0027, 0.0042, 0.0052, 0.0105, 0.0153, 0.0028, 0.0028, 0.0026, 0.0292,\n",
            "         0.0140, 0.0026, 0.0021, 0.0034, 0.0204, 0.0384, 0.0072, 0.0048, 0.0024,\n",
            "         0.0140, 0.0285, 0.0037, 0.0022, 0.0152, 0.0370, 0.0031, 0.0070, 0.0016,\n",
            "         0.0065, 0.1867, 0.0068, 0.0085, 0.0133, 0.0042, 0.0130, 0.0655, 0.0019,\n",
            "         0.0033, 0.0213, 0.0148, 0.0099, 0.0272, 0.0073, 0.0029, 0.0047, 0.0060,\n",
            "         0.0051, 0.0036, 0.0231, 0.0357, 0.0022, 0.0222, 0.0129, 0.0114, 0.0032,\n",
            "         0.0297, 0.0818, 0.0086, 0.0022, 0.0171, 0.0036, 0.0067, 0.0042, 0.0212,\n",
            "         0.0104, 0.0085]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[56]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
            "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
            "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
            "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
            "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
            "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
            "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
            "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
            "           0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  0.5926,  1.7734,  1.2618,  0.6474,\n",
            "          -0.3519,  1.0237, -0.1184,  0.1446,  0.0477, -0.4317,  0.0058,\n",
            "          -0.3478, -0.2188,  1.2574, -0.7758,  0.9081, -1.1492, -1.6415,\n",
            "           1.3099,  1.2829, -0.9754,  0.5888, -0.3234, -0.9876, -0.1603,\n",
            "          -0.2273,  0.6294, -0.4703, -0.1420, -1.0257,  0.3648,  0.8021,\n",
            "           0.5142, -1.0679, -0.6295, -0.1167, -0.0337,  0.2609, -0.2877,\n",
            "           1.7954,  0.6843, -0.8268,  1.8204,  0.3783,  0.5864, -0.2330,\n",
            "          -0.3098,  0.7679, -0.0269,  0.6213, -1.3444, -1.3337,  0.2562,\n",
            "          -1.1706, -0.5799, -1.1549,  0.4594,  0.1099, -0.4593,  0.1390,\n",
            "           0.7560,  0.4296]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.6722,  0.2322, -0.1632,  0.5926,  1.7734,  1.2618,  0.6474, -0.3519,\n",
            "          1.0237, -0.1184,  0.1446,  0.0477, -0.4317,  0.0058, -0.3478, -0.2188,\n",
            "          1.2574, -0.7758,  0.9081, -1.1492, -1.6415,  1.3099,  1.2829, -0.9754,\n",
            "          0.5888, -0.3234, -0.9876, -0.1603, -0.2273,  0.6294, -0.4703, -0.1420,\n",
            "         -1.0257,  0.3648,  0.8021,  0.5142, -1.0679, -0.6295, -0.1167, -0.0337,\n",
            "          0.2609, -0.2877,  1.7954,  0.6843, -0.8268,  1.8204,  0.3783,  0.5864,\n",
            "         -0.2330, -0.3098,  0.7679, -0.0269,  0.6213, -1.3444, -1.3337,  0.2562,\n",
            "         -1.1706, -0.5799, -1.1549,  0.4594,  0.1099, -0.4593,  0.1390,  0.7560,\n",
            "          0.4296]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0054, 0.0134, 0.0090, 0.0192, 0.0626, 0.0375, 0.0203, 0.0075, 0.0296,\n",
            "         0.0094, 0.0123, 0.0111, 0.0069, 0.0107, 0.0075, 0.0085, 0.0374, 0.0049,\n",
            "         0.0263, 0.0034, 0.0021, 0.0394, 0.0383, 0.0040, 0.0191, 0.0077, 0.0040,\n",
            "         0.0091, 0.0085, 0.0199, 0.0066, 0.0092, 0.0038, 0.0153, 0.0237, 0.0178,\n",
            "         0.0037, 0.0057, 0.0095, 0.0103, 0.0138, 0.0080, 0.0640, 0.0211, 0.0046,\n",
            "         0.0656, 0.0155, 0.0191, 0.0084, 0.0078, 0.0229, 0.0103, 0.0198, 0.0028,\n",
            "         0.0028, 0.0137, 0.0033, 0.0060, 0.0033, 0.0168, 0.0119, 0.0067, 0.0122,\n",
            "         0.0226, 0.0163]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[12]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
            "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
            "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
            "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
            "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
            "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
            "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
            "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
            "           0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  0.5926,  1.7734,  1.2618,  0.6474,\n",
            "          -0.3519,  1.0237, -0.1184,  0.1446,  0.0477, -0.4317,  0.0058,\n",
            "          -0.3478, -0.2188,  1.2574, -0.7758,  0.9081, -1.1492, -1.6415,\n",
            "           1.3099,  1.2829, -0.9754,  0.5888, -0.3234, -0.9876, -0.1603,\n",
            "          -0.2273,  0.6294, -0.4703, -0.1420, -1.0257,  0.3648,  0.8021,\n",
            "           0.5142, -1.0679, -0.6295, -0.1167, -0.0337,  0.2609, -0.2877,\n",
            "           1.7954,  0.6843, -0.8268,  1.8204,  0.3783,  0.5864, -0.2330,\n",
            "          -0.3098,  0.7679, -0.0269,  0.6213, -1.3444, -1.3337,  0.2562,\n",
            "          -1.1706, -0.5799, -1.1549,  0.4594,  0.1099, -0.4593,  0.1390,\n",
            "           0.7560,  0.4296],\n",
            "         [-2.0333, -0.5538,  0.1696,  0.6985,  0.3744, -1.4854,  0.6502,\n",
            "          -0.2045, -0.1877,  0.4486,  0.5572, -0.5456,  0.1637, -1.8262,\n",
            "           0.6854, -0.8762, -1.4931,  1.6634, -0.4718,  0.5857, -0.9579,\n",
            "           0.9435, -2.1992,  0.4164, -0.3564, -1.0305, -1.0478,  2.1709,\n",
            "          -1.0414, -1.3088,  1.0169,  0.4795, -1.8578,  0.2330, -0.2205,\n",
            "          -1.4869, -0.6272,  2.0144, -0.6938,  1.6325, -0.7595,  0.5849,\n",
            "           1.0116, -0.3803, -0.4396,  0.1469,  1.2890, -0.0208,  0.0698,\n",
            "          -0.7296,  0.1653, -0.3390,  1.5416,  1.0231,  1.3392,  1.6886,\n",
            "           0.7099,  1.3693, -0.7076,  1.5387, -0.9559,  2.0515,  0.0096,\n",
            "           1.1159, -0.2966]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-2.0333, -0.5538,  0.1696,  0.6985,  0.3744, -1.4854,  0.6502, -0.2045,\n",
            "         -0.1877,  0.4486,  0.5572, -0.5456,  0.1637, -1.8262,  0.6854, -0.8762,\n",
            "         -1.4931,  1.6634, -0.4718,  0.5857, -0.9579,  0.9435, -2.1992,  0.4164,\n",
            "         -0.3564, -1.0305, -1.0478,  2.1709, -1.0414, -1.3088,  1.0169,  0.4795,\n",
            "         -1.8578,  0.2330, -0.2205, -1.4869, -0.6272,  2.0144, -0.6938,  1.6325,\n",
            "         -0.7595,  0.5849,  1.0116, -0.3803, -0.4396,  0.1469,  1.2890, -0.0208,\n",
            "          0.0698, -0.7296,  0.1653, -0.3390,  1.5416,  1.0231,  1.3392,  1.6886,\n",
            "          0.7099,  1.3693, -0.7076,  1.5387, -0.9559,  2.0515,  0.0096,  1.1159,\n",
            "         -0.2966]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0011, 0.0049, 0.0101, 0.0172, 0.0124, 0.0019, 0.0164, 0.0070, 0.0071,\n",
            "         0.0134, 0.0149, 0.0050, 0.0101, 0.0014, 0.0170, 0.0036, 0.0019, 0.0452,\n",
            "         0.0053, 0.0154, 0.0033, 0.0220, 0.0009, 0.0130, 0.0060, 0.0031, 0.0030,\n",
            "         0.0750, 0.0030, 0.0023, 0.0237, 0.0138, 0.0013, 0.0108, 0.0069, 0.0019,\n",
            "         0.0046, 0.0641, 0.0043, 0.0438, 0.0040, 0.0154, 0.0235, 0.0058, 0.0055,\n",
            "         0.0099, 0.0311, 0.0084, 0.0092, 0.0041, 0.0101, 0.0061, 0.0400, 0.0238,\n",
            "         0.0327, 0.0463, 0.0174, 0.0337, 0.0042, 0.0399, 0.0033, 0.0666, 0.0086,\n",
            "         0.0261, 0.0064]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[55]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
            "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
            "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
            "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
            "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
            "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
            "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
            "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
            "           0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  0.5926,  1.7734,  1.2618,  0.6474,\n",
            "          -0.3519,  1.0237, -0.1184,  0.1446,  0.0477, -0.4317,  0.0058,\n",
            "          -0.3478, -0.2188,  1.2574, -0.7758,  0.9081, -1.1492, -1.6415,\n",
            "           1.3099,  1.2829, -0.9754,  0.5888, -0.3234, -0.9876, -0.1603,\n",
            "          -0.2273,  0.6294, -0.4703, -0.1420, -1.0257,  0.3648,  0.8021,\n",
            "           0.5142, -1.0679, -0.6295, -0.1167, -0.0337,  0.2609, -0.2877,\n",
            "           1.7954,  0.6843, -0.8268,  1.8204,  0.3783,  0.5864, -0.2330,\n",
            "          -0.3098,  0.7679, -0.0269,  0.6213, -1.3444, -1.3337,  0.2562,\n",
            "          -1.1706, -0.5799, -1.1549,  0.4594,  0.1099, -0.4593,  0.1390,\n",
            "           0.7560,  0.4296],\n",
            "         [-2.0333, -0.5538,  0.1696,  0.6985,  0.3744, -1.4854,  0.6502,\n",
            "          -0.2045, -0.1877,  0.4486,  0.5572, -0.5456,  0.1637, -1.8262,\n",
            "           0.6854, -0.8762, -1.4931,  1.6634, -0.4718,  0.5857, -0.9579,\n",
            "           0.9435, -2.1992,  0.4164, -0.3564, -1.0305, -1.0478,  2.1709,\n",
            "          -1.0414, -1.3088,  1.0169,  0.4795, -1.8578,  0.2330, -0.2205,\n",
            "          -1.4869, -0.6272,  2.0144, -0.6938,  1.6325, -0.7595,  0.5849,\n",
            "           1.0116, -0.3803, -0.4396,  0.1469,  1.2890, -0.0208,  0.0698,\n",
            "          -0.7296,  0.1653, -0.3390,  1.5416,  1.0231,  1.3392,  1.6886,\n",
            "           0.7099,  1.3693, -0.7076,  1.5387, -0.9559,  2.0515,  0.0096,\n",
            "           1.1159, -0.2966],\n",
            "         [-1.2542,  0.0077, -1.5728,  0.6528, -2.0244, -1.3731,  1.8886,\n",
            "           2.6879,  0.9940, -1.9079, -0.8043, -0.3358,  0.4116,  0.5577,\n",
            "          -0.8911, -1.0478, -1.9065, -0.5476, -1.2786, -0.1582,  1.5599,\n",
            "          -0.1496, -1.4406,  0.6488, -1.3412,  0.3569, -0.9536, -1.8299,\n",
            "          -0.2695,  0.1350,  0.7850,  0.5161, -0.3392,  0.4127,  1.1458,\n",
            "          -1.2133, -0.2370,  0.1126,  0.0860, -0.4971,  0.6583, -0.8797,\n",
            "          -0.0450,  0.1431,  1.6021, -0.8150,  0.3507, -0.2239, -2.4013,\n",
            "          -0.7117, -0.3782,  0.9890, -1.2497,  0.2198,  0.9143, -0.1592,\n",
            "          -1.6053, -1.1741,  0.6289, -0.9825, -1.4075, -1.3757, -0.1028,\n",
            "          -1.5216, -0.4975]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.2542,  0.0077, -1.5728,  0.6528, -2.0244, -1.3731,  1.8886,  2.6879,\n",
            "          0.9940, -1.9079, -0.8043, -0.3358,  0.4116,  0.5577, -0.8911, -1.0478,\n",
            "         -1.9065, -0.5476, -1.2786, -0.1582,  1.5599, -0.1496, -1.4406,  0.6488,\n",
            "         -1.3412,  0.3569, -0.9536, -1.8299, -0.2695,  0.1350,  0.7850,  0.5161,\n",
            "         -0.3392,  0.4127,  1.1458, -1.2133, -0.2370,  0.1126,  0.0860, -0.4971,\n",
            "          0.6583, -0.8797, -0.0450,  0.1431,  1.6021, -0.8150,  0.3507, -0.2239,\n",
            "         -2.4013, -0.7117, -0.3782,  0.9890, -1.2497,  0.2198,  0.9143, -0.1592,\n",
            "         -1.6053, -1.1741,  0.6289, -0.9825, -1.4075, -1.3757, -0.1028, -1.5216,\n",
            "         -0.4975]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0033, 0.0117, 0.0024, 0.0223, 0.0015, 0.0029, 0.0769, 0.1710, 0.0314,\n",
            "         0.0017, 0.0052, 0.0083, 0.0176, 0.0203, 0.0048, 0.0041, 0.0017, 0.0067,\n",
            "         0.0032, 0.0099, 0.0554, 0.0100, 0.0028, 0.0223, 0.0030, 0.0166, 0.0045,\n",
            "         0.0019, 0.0089, 0.0133, 0.0255, 0.0195, 0.0083, 0.0176, 0.0366, 0.0035,\n",
            "         0.0092, 0.0130, 0.0127, 0.0071, 0.0225, 0.0048, 0.0111, 0.0134, 0.0577,\n",
            "         0.0051, 0.0165, 0.0093, 0.0011, 0.0057, 0.0080, 0.0313, 0.0033, 0.0145,\n",
            "         0.0290, 0.0099, 0.0023, 0.0036, 0.0218, 0.0044, 0.0028, 0.0029, 0.0105,\n",
            "         0.0025, 0.0071]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[28]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104,\n",
            "          -1.0886,  1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,\n",
            "           1.6030, -0.0726, -0.4725, -1.1616,  0.5962,  1.3058, -0.7422,\n",
            "          -1.2529,  0.6750,  1.5664, -0.9238, -0.0956, -1.5452, -0.1801,\n",
            "           3.1838, -0.1277,  0.0910,  0.5422, -0.6110,  0.5220,  2.1368,\n",
            "          -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,  1.2588, -0.0644,\n",
            "          -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,  1.5294,\n",
            "          -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "           0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,\n",
            "           0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  0.5926,  1.7734,  1.2618,  0.6474,\n",
            "          -0.3519,  1.0237, -0.1184,  0.1446,  0.0477, -0.4317,  0.0058,\n",
            "          -0.3478, -0.2188,  1.2574, -0.7758,  0.9081, -1.1492, -1.6415,\n",
            "           1.3099,  1.2829, -0.9754,  0.5888, -0.3234, -0.9876, -0.1603,\n",
            "          -0.2273,  0.6294, -0.4703, -0.1420, -1.0257,  0.3648,  0.8021,\n",
            "           0.5142, -1.0679, -0.6295, -0.1167, -0.0337,  0.2609, -0.2877,\n",
            "           1.7954,  0.6843, -0.8268,  1.8204,  0.3783,  0.5864, -0.2330,\n",
            "          -0.3098,  0.7679, -0.0269,  0.6213, -1.3444, -1.3337,  0.2562,\n",
            "          -1.1706, -0.5799, -1.1549,  0.4594,  0.1099, -0.4593,  0.1390,\n",
            "           0.7560,  0.4296],\n",
            "         [-2.0333, -0.5538,  0.1696,  0.6985,  0.3744, -1.4854,  0.6502,\n",
            "          -0.2045, -0.1877,  0.4486,  0.5572, -0.5456,  0.1637, -1.8262,\n",
            "           0.6854, -0.8762, -1.4931,  1.6634, -0.4718,  0.5857, -0.9579,\n",
            "           0.9435, -2.1992,  0.4164, -0.3564, -1.0305, -1.0478,  2.1709,\n",
            "          -1.0414, -1.3088,  1.0169,  0.4795, -1.8578,  0.2330, -0.2205,\n",
            "          -1.4869, -0.6272,  2.0144, -0.6938,  1.6325, -0.7595,  0.5849,\n",
            "           1.0116, -0.3803, -0.4396,  0.1469,  1.2890, -0.0208,  0.0698,\n",
            "          -0.7296,  0.1653, -0.3390,  1.5416,  1.0231,  1.3392,  1.6886,\n",
            "           0.7099,  1.3693, -0.7076,  1.5387, -0.9559,  2.0515,  0.0096,\n",
            "           1.1159, -0.2966],\n",
            "         [-1.2542,  0.0077, -1.5728,  0.6528, -2.0244, -1.3731,  1.8886,\n",
            "           2.6879,  0.9940, -1.9079, -0.8043, -0.3358,  0.4116,  0.5577,\n",
            "          -0.8911, -1.0478, -1.9065, -0.5476, -1.2786, -0.1582,  1.5599,\n",
            "          -0.1496, -1.4406,  0.6488, -1.3412,  0.3569, -0.9536, -1.8299,\n",
            "          -0.2695,  0.1350,  0.7850,  0.5161, -0.3392,  0.4127,  1.1458,\n",
            "          -1.2133, -0.2370,  0.1126,  0.0860, -0.4971,  0.6583, -0.8797,\n",
            "          -0.0450,  0.1431,  1.6021, -0.8150,  0.3507, -0.2239, -2.4013,\n",
            "          -0.7117, -0.3782,  0.9890, -1.2497,  0.2198,  0.9143, -0.1592,\n",
            "          -1.6053, -1.1741,  0.6289, -0.9825, -1.4075, -1.3757, -0.1028,\n",
            "          -1.5216, -0.4975],\n",
            "         [ 0.3140,  2.2434,  1.6029, -0.7250,  0.2998, -0.8843,  1.5462,\n",
            "          -0.7646, -0.2466, -0.6231, -0.0477, -2.0922,  0.0285,  0.1288,\n",
            "          -0.7638,  0.0996,  0.2829, -1.2844, -0.0274,  0.1114,  0.9420,\n",
            "           0.0435,  1.2219, -0.7706,  0.6853, -0.6485,  0.6499, -0.0729,\n",
            "          -0.3752, -1.3073, -0.3637, -0.2029,  0.8741,  0.1697, -2.4297,\n",
            "          -0.1310, -1.7969, -0.5603,  0.9169,  0.0423, -0.0205,  0.2081,\n",
            "           0.7121, -0.7543, -1.6761,  0.6430,  0.0780,  0.6150, -0.2703,\n",
            "          -0.2620,  1.6763,  0.9997, -0.1362, -1.2802, -1.4307, -1.2849,\n",
            "          -0.8925,  0.5512, -0.5527, -1.3258, -0.1855, -0.3131,  0.1500,\n",
            "           2.4338, -0.5937]]], grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.3140,  2.2434,  1.6029, -0.7250,  0.2998, -0.8843,  1.5462, -0.7646,\n",
            "         -0.2466, -0.6231, -0.0477, -2.0922,  0.0285,  0.1288, -0.7638,  0.0996,\n",
            "          0.2829, -1.2844, -0.0274,  0.1114,  0.9420,  0.0435,  1.2219, -0.7706,\n",
            "          0.6853, -0.6485,  0.6499, -0.0729, -0.3752, -1.3073, -0.3637, -0.2029,\n",
            "          0.8741,  0.1697, -2.4297, -0.1310, -1.7969, -0.5603,  0.9169,  0.0423,\n",
            "         -0.0205,  0.2081,  0.7121, -0.7543, -1.6761,  0.6430,  0.0780,  0.6150,\n",
            "         -0.2703, -0.2620,  1.6763,  0.9997, -0.1362, -1.2802, -1.4307, -1.2849,\n",
            "         -0.8925,  0.5512, -0.5527, -1.3258, -0.1855, -0.3131,  0.1500,  2.4338,\n",
            "         -0.5937]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0143, 0.0984, 0.0519, 0.0051, 0.0141, 0.0043, 0.0490, 0.0049, 0.0082,\n",
            "         0.0056, 0.0100, 0.0013, 0.0107, 0.0119, 0.0049, 0.0115, 0.0139, 0.0029,\n",
            "         0.0102, 0.0117, 0.0268, 0.0109, 0.0354, 0.0048, 0.0207, 0.0055, 0.0200,\n",
            "         0.0097, 0.0072, 0.0028, 0.0073, 0.0085, 0.0250, 0.0124, 0.0009, 0.0092,\n",
            "         0.0017, 0.0060, 0.0261, 0.0109, 0.0102, 0.0129, 0.0213, 0.0049, 0.0020,\n",
            "         0.0199, 0.0113, 0.0193, 0.0080, 0.0080, 0.0558, 0.0284, 0.0091, 0.0029,\n",
            "         0.0025, 0.0029, 0.0043, 0.0181, 0.0060, 0.0028, 0.0087, 0.0076, 0.0121,\n",
            "         0.1191, 0.0058]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[7]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "         -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "         -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "         -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "         -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "          6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "          5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "         -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "          1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "          1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "          2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "         -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "          4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0131, 0.0020, 0.0161, 0.0170, 0.0048, 0.0035, 0.0103, 0.0103, 0.0111,\n",
            "         0.0126, 0.0021, 0.0012, 0.0032, 0.0111, 0.0198, 0.0062, 0.0083, 0.0106,\n",
            "         0.0095, 0.0104, 0.0030, 0.0324, 0.0021, 0.0412, 0.0213, 0.0196, 0.0074,\n",
            "         0.0036, 0.0256, 0.0377, 0.0179, 0.0065, 0.0059, 0.0339, 0.0053, 0.0102,\n",
            "         0.0032, 0.0167, 0.0146, 0.0058, 0.0360, 0.0052, 0.0153, 0.0125, 0.0012,\n",
            "         0.0548, 0.0099, 0.0092, 0.0296, 0.0101, 0.0132, 0.0171, 0.0042, 0.0615,\n",
            "         0.0113, 0.0071, 0.0064, 0.0056, 0.0644, 0.0138, 0.0108, 0.0872, 0.0052,\n",
            "         0.0021, 0.0090]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[29]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.7248, -1.0435,  1.9399,  0.2701,  1.2115,  0.7865, -0.7084,  0.4292,\n",
            "         -0.7906, -0.0713, -0.0513, -1.0868,  1.7941,  0.2997,  0.0425,  0.0540,\n",
            "         -1.5205,  0.7162,  0.1056,  0.7527, -0.8036, -1.3313,  1.9921, -0.0312,\n",
            "          0.7412, -0.0542,  0.7627, -0.5470, -0.6734,  1.0242, -0.8727, -0.9222,\n",
            "          1.3338,  0.2171, -0.3506,  0.4949,  0.7829,  0.6703, -1.8922, -1.2433,\n",
            "          1.8634,  0.3737, -1.1434, -0.4845,  0.0252,  0.1318,  0.7086, -1.5011,\n",
            "          0.0303,  0.2201,  0.7736,  0.1816,  0.0705,  0.0879, -0.7537,  0.6007,\n",
            "         -0.8228, -1.9710, -0.7388, -1.8158, -0.3450,  0.2426,  0.0761, -0.1408,\n",
            "          0.3923]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0037, 0.0731, 0.0138, 0.0353, 0.0231, 0.0052, 0.0161, 0.0048,\n",
            "         0.0098, 0.0100, 0.0035, 0.0632, 0.0142, 0.0110, 0.0111, 0.0023, 0.0215,\n",
            "         0.0117, 0.0223, 0.0047, 0.0028, 0.0770, 0.0102, 0.0221, 0.0100, 0.0225,\n",
            "         0.0061, 0.0054, 0.0293, 0.0044, 0.0042, 0.0399, 0.0131, 0.0074, 0.0172,\n",
            "         0.0230, 0.0205, 0.0016, 0.0030, 0.0677, 0.0153, 0.0033, 0.0065, 0.0108,\n",
            "         0.0120, 0.0213, 0.0023, 0.0108, 0.0131, 0.0228, 0.0126, 0.0113, 0.0115,\n",
            "         0.0049, 0.0192, 0.0046, 0.0015, 0.0050, 0.0017, 0.0074, 0.0134, 0.0113,\n",
            "         0.0091, 0.0156]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[35]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.7681, -0.0430,  0.1073, -0.2059, -1.2616, -0.2270,  0.1066, -0.1683,\n",
            "          0.2433, -0.5709,  2.3408, -2.5665,  0.9935, -0.0124, -0.6971,  0.7903,\n",
            "          0.5051, -0.6004, -0.3287, -0.2818,  0.2799,  0.9678, -1.0858,  1.0346,\n",
            "          0.0428, -0.3025,  0.5520, -0.5940,  0.7718, -0.2502, -0.4973, -0.1547,\n",
            "          1.4592, -0.4754,  0.0821,  2.2645, -1.2837, -0.4539, -0.9422, -0.6697,\n",
            "          0.0602,  0.1464,  0.3007, -0.6955, -1.0944,  0.9295,  0.3155, -0.3942,\n",
            "         -0.0968,  1.3060, -1.2617, -0.0670,  0.2999, -0.7043, -0.0390, -1.8838,\n",
            "         -0.6024, -0.3366,  0.3025, -0.4242,  0.2360,  0.5425, -0.6499,  0.6144,\n",
            "          0.1669]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0237, 0.0105, 0.0122, 0.0089, 0.0031, 0.0088, 0.0122, 0.0093, 0.0140,\n",
            "         0.0062, 0.1141, 0.0008, 0.0297, 0.0108, 0.0055, 0.0242, 0.0182, 0.0060,\n",
            "         0.0079, 0.0083, 0.0145, 0.0289, 0.0037, 0.0309, 0.0115, 0.0081, 0.0191,\n",
            "         0.0061, 0.0238, 0.0086, 0.0067, 0.0094, 0.0473, 0.0068, 0.0119, 0.1057,\n",
            "         0.0030, 0.0070, 0.0043, 0.0056, 0.0117, 0.0127, 0.0148, 0.0055, 0.0037,\n",
            "         0.0278, 0.0151, 0.0074, 0.0100, 0.0405, 0.0031, 0.0103, 0.0148, 0.0054,\n",
            "         0.0106, 0.0017, 0.0060, 0.0078, 0.0149, 0.0072, 0.0139, 0.0189, 0.0057,\n",
            "         0.0203, 0.0130]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[49]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2950, -0.6511,  1.4937,  1.1173,  0.7356, -0.0165, -0.2420, -0.2702,\n",
            "          0.0598, -0.3667,  0.2255, -0.9411, -1.6868, -0.9292, -1.2395, -1.0842,\n",
            "         -0.3648, -0.1092, -0.1591,  0.6822,  1.1651,  0.9528,  0.1548, -0.0579,\n",
            "         -1.0556, -0.7017,  0.4944,  1.1985, -0.5672, -0.3204,  1.5870,  1.5017,\n",
            "         -2.5039,  0.8525,  0.0561, -0.0418,  0.3568, -1.5907, -0.7374, -1.2256,\n",
            "          0.0682,  1.0599, -0.8286,  0.0083, -0.3394,  0.8296,  0.2789, -1.0439,\n",
            "         -1.4084, -0.2276, -0.6571, -0.5281, -0.0276, -0.9014,  1.4106,  1.1756,\n",
            "         -0.0158, -0.5705,  2.0617,  0.3756, -0.4315, -0.6968, -0.5250,  1.2672,\n",
            "          2.6002]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0071, 0.0050, 0.0427, 0.0293, 0.0200, 0.0094, 0.0075, 0.0073, 0.0102,\n",
            "         0.0066, 0.0120, 0.0037, 0.0018, 0.0038, 0.0028, 0.0032, 0.0067, 0.0086,\n",
            "         0.0082, 0.0190, 0.0308, 0.0249, 0.0112, 0.0091, 0.0033, 0.0048, 0.0157,\n",
            "         0.0318, 0.0054, 0.0070, 0.0469, 0.0431, 0.0008, 0.0225, 0.0101, 0.0092,\n",
            "         0.0137, 0.0020, 0.0046, 0.0028, 0.0103, 0.0277, 0.0042, 0.0097, 0.0068,\n",
            "         0.0220, 0.0127, 0.0034, 0.0023, 0.0076, 0.0050, 0.0057, 0.0093, 0.0039,\n",
            "         0.0393, 0.0311, 0.0094, 0.0054, 0.0754, 0.0140, 0.0062, 0.0048, 0.0057,\n",
            "         0.0341, 0.1292]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[58]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
            "         [ 2.4746e-01, -6.3485e-01, -1.2909e+00,  1.1822e+00,  1.4787e-01,\n",
            "          -4.3331e-01, -8.2693e-01,  7.2802e-02, -1.2982e+00,  3.9600e-01,\n",
            "          -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00, -3.8121e-01,\n",
            "          -8.5146e-01,  1.1918e+00, -8.1080e-01, -1.7326e-01, -4.7029e-01,\n",
            "          -6.0004e-01, -1.3636e+00, -1.0889e+00,  1.0108e+00,  8.5429e-01,\n",
            "          -4.4113e-02,  1.8017e+00,  6.0141e-01, -2.5448e+00, -4.8652e-01,\n",
            "           2.6412e+00,  1.6053e+00,  5.9007e-01,  8.1368e-01, -1.1238e-01,\n",
            "          -3.0501e-01,  1.1426e+00,  6.6372e-01, -7.0001e-01,  9.2620e-01,\n",
            "          -1.1032e+00, -1.2125e+00,  6.0654e-01,  5.8816e-01, -5.4526e-01,\n",
            "           7.6541e-01,  5.6915e-01,  8.8591e-01, -7.0044e-02,  6.7919e-01,\n",
            "          -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,  9.5867e-01,\n",
            "          -3.3781e-02,  5.0831e-01, -2.5017e-01,  2.0734e+00, -2.9941e-01,\n",
            "           4.7293e-02, -9.6258e-01,  1.3064e+00, -2.2557e-01, -1.8305e+00]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.2475, -0.6349, -1.2909,  1.1822,  0.1479, -0.4333, -0.8269,  0.0728,\n",
            "         -1.2982,  0.3960, -1.2460,  0.1458, -0.5699, -1.3561, -0.3812, -0.8515,\n",
            "          1.1918, -0.8108, -0.1733, -0.4703, -0.6000, -1.3636, -1.0889,  1.0108,\n",
            "          0.8543, -0.0441,  1.8017,  0.6014, -2.5448, -0.4865,  2.6412,  1.6053,\n",
            "          0.5901,  0.8137, -0.1124, -0.3050,  1.1426,  0.6637, -0.7000,  0.9262,\n",
            "         -1.1032, -1.2125,  0.6065,  0.5882, -0.5453,  0.7654,  0.5692,  0.8859,\n",
            "         -0.0700,  0.6792, -0.0283, -1.2243, -1.7192,  1.4801,  0.9587, -0.0338,\n",
            "          0.5083, -0.2502,  2.0734, -0.2994,  0.0473, -0.9626,  1.3064, -0.2256,\n",
            "         -1.8305]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0118, 0.0049, 0.0025, 0.0301, 0.0107, 0.0060, 0.0040, 0.0099, 0.0025,\n",
            "         0.0137, 0.0027, 0.0107, 0.0052, 0.0024, 0.0063, 0.0039, 0.0304, 0.0041,\n",
            "         0.0078, 0.0058, 0.0051, 0.0024, 0.0031, 0.0254, 0.0217, 0.0088, 0.0560,\n",
            "         0.0168, 0.0007, 0.0057, 0.1295, 0.0460, 0.0167, 0.0208, 0.0083, 0.0068,\n",
            "         0.0289, 0.0179, 0.0046, 0.0233, 0.0031, 0.0027, 0.0169, 0.0166, 0.0054,\n",
            "         0.0199, 0.0163, 0.0224, 0.0086, 0.0182, 0.0090, 0.0027, 0.0017, 0.0406,\n",
            "         0.0241, 0.0089, 0.0154, 0.0072, 0.0734, 0.0068, 0.0097, 0.0035, 0.0341,\n",
            "         0.0074, 0.0015]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[36]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
            "         [ 2.4746e-01, -6.3485e-01, -1.2909e+00,  1.1822e+00,  1.4787e-01,\n",
            "          -4.3331e-01, -8.2693e-01,  7.2802e-02, -1.2982e+00,  3.9600e-01,\n",
            "          -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00, -3.8121e-01,\n",
            "          -8.5146e-01,  1.1918e+00, -8.1080e-01, -1.7326e-01, -4.7029e-01,\n",
            "          -6.0004e-01, -1.3636e+00, -1.0889e+00,  1.0108e+00,  8.5429e-01,\n",
            "          -4.4113e-02,  1.8017e+00,  6.0141e-01, -2.5448e+00, -4.8652e-01,\n",
            "           2.6412e+00,  1.6053e+00,  5.9007e-01,  8.1368e-01, -1.1238e-01,\n",
            "          -3.0501e-01,  1.1426e+00,  6.6372e-01, -7.0001e-01,  9.2620e-01,\n",
            "          -1.1032e+00, -1.2125e+00,  6.0654e-01,  5.8816e-01, -5.4526e-01,\n",
            "           7.6541e-01,  5.6915e-01,  8.8591e-01, -7.0044e-02,  6.7919e-01,\n",
            "          -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,  9.5867e-01,\n",
            "          -3.3781e-02,  5.0831e-01, -2.5017e-01,  2.0734e+00, -2.9941e-01,\n",
            "           4.7293e-02, -9.6258e-01,  1.3064e+00, -2.2557e-01, -1.8305e+00],\n",
            "         [-4.6495e-01, -4.0384e-01,  5.1750e-01,  3.4527e-01,  1.0466e+00,\n",
            "           2.6479e-01, -9.6930e-01,  5.4509e-01, -2.9051e-01,  6.2907e-01,\n",
            "           8.8252e-01, -2.2943e+00, -2.0391e-01,  4.0566e-01, -1.5495e+00,\n",
            "          -2.1440e-01, -6.5544e-01,  2.1339e+00, -5.9239e-01,  2.4027e-01,\n",
            "          -4.3088e-01,  8.8368e-01,  4.6157e-01,  4.4638e-01,  8.0647e-01,\n",
            "          -9.3765e-02,  1.0844e+00, -1.0433e+00, -1.9601e+00,  2.0837e-01,\n",
            "           3.0050e-01,  8.8042e-01, -8.3029e-01,  6.5888e-01, -3.1582e-01,\n",
            "          -2.0885e-01,  1.4769e+00,  6.8216e-01, -8.8222e-01,  7.7618e-01,\n",
            "          -6.2467e-02,  8.4428e-01, -1.5745e-02,  1.7251e+00, -1.9974e+00,\n",
            "           4.0745e-01, -1.5205e+00,  8.7902e-01, -6.0891e-02, -2.0592e-01,\n",
            "           1.3214e+00,  6.6606e-01,  2.9364e-01, -6.1167e-01, -8.8674e-01,\n",
            "          -1.3853e-01,  5.8954e-01, -2.2584e-01, -9.9184e-01, -1.0729e+00,\n",
            "          -2.4981e-01, -8.8683e-01,  1.5427e+00,  1.0735e+00, -4.7932e-02]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.4650, -0.4038,  0.5175,  0.3453,  1.0466,  0.2648, -0.9693,  0.5451,\n",
            "         -0.2905,  0.6291,  0.8825, -2.2943, -0.2039,  0.4057, -1.5495, -0.2144,\n",
            "         -0.6554,  2.1339, -0.5924,  0.2403, -0.4309,  0.8837,  0.4616,  0.4464,\n",
            "          0.8065, -0.0938,  1.0844, -1.0433, -1.9601,  0.2084,  0.3005,  0.8804,\n",
            "         -0.8303,  0.6589, -0.3158, -0.2089,  1.4769,  0.6822, -0.8822,  0.7762,\n",
            "         -0.0625,  0.8443, -0.0157,  1.7251, -1.9974,  0.4075, -1.5205,  0.8790,\n",
            "         -0.0609, -0.2059,  1.3214,  0.6661,  0.2936, -0.6117, -0.8867, -0.1385,\n",
            "          0.5895, -0.2258, -0.9918, -1.0729, -0.2498, -0.8868,  1.5427,  1.0735,\n",
            "         -0.0479]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0064, 0.0068, 0.0171, 0.0144, 0.0289, 0.0132, 0.0039, 0.0175, 0.0076,\n",
            "         0.0191, 0.0246, 0.0010, 0.0083, 0.0152, 0.0022, 0.0082, 0.0053, 0.0859,\n",
            "         0.0056, 0.0129, 0.0066, 0.0246, 0.0161, 0.0159, 0.0228, 0.0093, 0.0301,\n",
            "         0.0036, 0.0014, 0.0125, 0.0137, 0.0245, 0.0044, 0.0196, 0.0074, 0.0082,\n",
            "         0.0445, 0.0201, 0.0042, 0.0221, 0.0095, 0.0236, 0.0100, 0.0571, 0.0014,\n",
            "         0.0153, 0.0022, 0.0245, 0.0096, 0.0083, 0.0381, 0.0198, 0.0136, 0.0055,\n",
            "         0.0042, 0.0088, 0.0183, 0.0081, 0.0038, 0.0035, 0.0079, 0.0042, 0.0475,\n",
            "         0.0297, 0.0097]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[53]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
            "         [ 2.4746e-01, -6.3485e-01, -1.2909e+00,  1.1822e+00,  1.4787e-01,\n",
            "          -4.3331e-01, -8.2693e-01,  7.2802e-02, -1.2982e+00,  3.9600e-01,\n",
            "          -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00, -3.8121e-01,\n",
            "          -8.5146e-01,  1.1918e+00, -8.1080e-01, -1.7326e-01, -4.7029e-01,\n",
            "          -6.0004e-01, -1.3636e+00, -1.0889e+00,  1.0108e+00,  8.5429e-01,\n",
            "          -4.4113e-02,  1.8017e+00,  6.0141e-01, -2.5448e+00, -4.8652e-01,\n",
            "           2.6412e+00,  1.6053e+00,  5.9007e-01,  8.1368e-01, -1.1238e-01,\n",
            "          -3.0501e-01,  1.1426e+00,  6.6372e-01, -7.0001e-01,  9.2620e-01,\n",
            "          -1.1032e+00, -1.2125e+00,  6.0654e-01,  5.8816e-01, -5.4526e-01,\n",
            "           7.6541e-01,  5.6915e-01,  8.8591e-01, -7.0044e-02,  6.7919e-01,\n",
            "          -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,  9.5867e-01,\n",
            "          -3.3781e-02,  5.0831e-01, -2.5017e-01,  2.0734e+00, -2.9941e-01,\n",
            "           4.7293e-02, -9.6258e-01,  1.3064e+00, -2.2557e-01, -1.8305e+00],\n",
            "         [-4.6495e-01, -4.0384e-01,  5.1750e-01,  3.4527e-01,  1.0466e+00,\n",
            "           2.6479e-01, -9.6930e-01,  5.4509e-01, -2.9051e-01,  6.2907e-01,\n",
            "           8.8252e-01, -2.2943e+00, -2.0391e-01,  4.0566e-01, -1.5495e+00,\n",
            "          -2.1440e-01, -6.5544e-01,  2.1339e+00, -5.9239e-01,  2.4027e-01,\n",
            "          -4.3088e-01,  8.8368e-01,  4.6157e-01,  4.4638e-01,  8.0647e-01,\n",
            "          -9.3765e-02,  1.0844e+00, -1.0433e+00, -1.9601e+00,  2.0837e-01,\n",
            "           3.0050e-01,  8.8042e-01, -8.3029e-01,  6.5888e-01, -3.1582e-01,\n",
            "          -2.0885e-01,  1.4769e+00,  6.8216e-01, -8.8222e-01,  7.7618e-01,\n",
            "          -6.2467e-02,  8.4428e-01, -1.5745e-02,  1.7251e+00, -1.9974e+00,\n",
            "           4.0745e-01, -1.5205e+00,  8.7902e-01, -6.0891e-02, -2.0592e-01,\n",
            "           1.3214e+00,  6.6606e-01,  2.9364e-01, -6.1167e-01, -8.8674e-01,\n",
            "          -1.3853e-01,  5.8954e-01, -2.2584e-01, -9.9184e-01, -1.0729e+00,\n",
            "          -2.4981e-01, -8.8683e-01,  1.5427e+00,  1.0735e+00, -4.7932e-02],\n",
            "         [-1.3237e-01, -5.4889e-01,  1.0244e-01, -6.9162e-01,  3.5075e-01,\n",
            "           1.6147e+00,  1.8203e+00,  5.1224e-01,  1.5810e+00, -2.0063e+00,\n",
            "          -1.2925e+00,  1.2681e-01,  1.1099e+00, -6.5921e-01,  8.0844e-01,\n",
            "           1.9072e+00, -3.2599e-01, -3.4377e-01, -1.4415e+00, -1.8276e-01,\n",
            "          -8.8043e-01, -6.1918e-01, -1.4047e+00, -8.5837e-01, -3.8297e-01,\n",
            "          -5.3723e-01, -1.2176e+00, -1.9403e+00, -3.0937e-01,  1.7895e-01,\n",
            "           1.2859e+00,  3.0392e-01,  1.8110e+00,  6.3502e-01, -8.1953e-02,\n",
            "          -2.1208e+00,  1.2516e+00, -6.8260e-01,  3.8377e-01,  1.5030e-02,\n",
            "          -2.8009e-01,  1.4896e+00, -4.6460e-01, -1.9210e+00, -1.0624e-01,\n",
            "           1.0614e+00,  9.3078e-01,  3.1170e+00, -1.5428e+00, -2.2848e+00,\n",
            "           5.7553e-01, -8.0399e-01,  8.0102e-01,  8.8035e-03, -4.7506e-01,\n",
            "          -9.6295e-01, -5.0778e-01,  1.0184e-01,  1.9141e+00, -1.9252e+00,\n",
            "          -1.5554e+00, -1.8777e-01, -8.5990e-01, -1.6050e+00, -6.9845e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1324, -0.5489,  0.1024, -0.6916,  0.3507,  1.6147,  1.8203,  0.5122,\n",
            "          1.5810, -2.0063, -1.2925,  0.1268,  1.1099, -0.6592,  0.8084,  1.9072,\n",
            "         -0.3260, -0.3438, -1.4415, -0.1828, -0.8804, -0.6192, -1.4047, -0.8584,\n",
            "         -0.3830, -0.5372, -1.2176, -1.9403, -0.3094,  0.1790,  1.2859,  0.3039,\n",
            "          1.8110,  0.6350, -0.0820, -2.1208,  1.2516, -0.6826,  0.3838,  0.0150,\n",
            "         -0.2801,  1.4896, -0.4646, -1.9210, -0.1062,  1.0614,  0.9308,  3.1170,\n",
            "         -1.5428, -2.2848,  0.5755, -0.8040,  0.8010,  0.0088, -0.4751, -0.9630,\n",
            "         -0.5078,  0.1018,  1.9141, -1.9252, -1.5554, -0.1878, -0.8599, -1.6050,\n",
            "         -0.6985]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0075, 0.0049, 0.0095, 0.0043, 0.0121, 0.0430, 0.0528, 0.0143, 0.0415,\n",
            "         0.0011, 0.0023, 0.0097, 0.0259, 0.0044, 0.0192, 0.0576, 0.0062, 0.0061,\n",
            "         0.0020, 0.0071, 0.0035, 0.0046, 0.0021, 0.0036, 0.0058, 0.0050, 0.0025,\n",
            "         0.0012, 0.0063, 0.0102, 0.0309, 0.0116, 0.0523, 0.0161, 0.0079, 0.0010,\n",
            "         0.0299, 0.0043, 0.0125, 0.0087, 0.0065, 0.0379, 0.0054, 0.0013, 0.0077,\n",
            "         0.0247, 0.0217, 0.1930, 0.0018, 0.0009, 0.0152, 0.0038, 0.0190, 0.0086,\n",
            "         0.0053, 0.0033, 0.0051, 0.0095, 0.0580, 0.0012, 0.0018, 0.0071, 0.0036,\n",
            "         0.0017, 0.0043]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
            "         [ 2.4746e-01, -6.3485e-01, -1.2909e+00,  1.1822e+00,  1.4787e-01,\n",
            "          -4.3331e-01, -8.2693e-01,  7.2802e-02, -1.2982e+00,  3.9600e-01,\n",
            "          -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00, -3.8121e-01,\n",
            "          -8.5146e-01,  1.1918e+00, -8.1080e-01, -1.7326e-01, -4.7029e-01,\n",
            "          -6.0004e-01, -1.3636e+00, -1.0889e+00,  1.0108e+00,  8.5429e-01,\n",
            "          -4.4113e-02,  1.8017e+00,  6.0141e-01, -2.5448e+00, -4.8652e-01,\n",
            "           2.6412e+00,  1.6053e+00,  5.9007e-01,  8.1368e-01, -1.1238e-01,\n",
            "          -3.0501e-01,  1.1426e+00,  6.6372e-01, -7.0001e-01,  9.2620e-01,\n",
            "          -1.1032e+00, -1.2125e+00,  6.0654e-01,  5.8816e-01, -5.4526e-01,\n",
            "           7.6541e-01,  5.6915e-01,  8.8591e-01, -7.0044e-02,  6.7919e-01,\n",
            "          -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,  9.5867e-01,\n",
            "          -3.3781e-02,  5.0831e-01, -2.5017e-01,  2.0734e+00, -2.9941e-01,\n",
            "           4.7293e-02, -9.6258e-01,  1.3064e+00, -2.2557e-01, -1.8305e+00],\n",
            "         [-4.6495e-01, -4.0384e-01,  5.1750e-01,  3.4527e-01,  1.0466e+00,\n",
            "           2.6479e-01, -9.6930e-01,  5.4509e-01, -2.9051e-01,  6.2907e-01,\n",
            "           8.8252e-01, -2.2943e+00, -2.0391e-01,  4.0566e-01, -1.5495e+00,\n",
            "          -2.1440e-01, -6.5544e-01,  2.1339e+00, -5.9239e-01,  2.4027e-01,\n",
            "          -4.3088e-01,  8.8368e-01,  4.6157e-01,  4.4638e-01,  8.0647e-01,\n",
            "          -9.3765e-02,  1.0844e+00, -1.0433e+00, -1.9601e+00,  2.0837e-01,\n",
            "           3.0050e-01,  8.8042e-01, -8.3029e-01,  6.5888e-01, -3.1582e-01,\n",
            "          -2.0885e-01,  1.4769e+00,  6.8216e-01, -8.8222e-01,  7.7618e-01,\n",
            "          -6.2467e-02,  8.4428e-01, -1.5745e-02,  1.7251e+00, -1.9974e+00,\n",
            "           4.0745e-01, -1.5205e+00,  8.7902e-01, -6.0891e-02, -2.0592e-01,\n",
            "           1.3214e+00,  6.6606e-01,  2.9364e-01, -6.1167e-01, -8.8674e-01,\n",
            "          -1.3853e-01,  5.8954e-01, -2.2584e-01, -9.9184e-01, -1.0729e+00,\n",
            "          -2.4981e-01, -8.8683e-01,  1.5427e+00,  1.0735e+00, -4.7932e-02],\n",
            "         [-1.3237e-01, -5.4889e-01,  1.0244e-01, -6.9162e-01,  3.5075e-01,\n",
            "           1.6147e+00,  1.8203e+00,  5.1224e-01,  1.5810e+00, -2.0063e+00,\n",
            "          -1.2925e+00,  1.2681e-01,  1.1099e+00, -6.5921e-01,  8.0844e-01,\n",
            "           1.9072e+00, -3.2599e-01, -3.4377e-01, -1.4415e+00, -1.8276e-01,\n",
            "          -8.8043e-01, -6.1918e-01, -1.4047e+00, -8.5837e-01, -3.8297e-01,\n",
            "          -5.3723e-01, -1.2176e+00, -1.9403e+00, -3.0937e-01,  1.7895e-01,\n",
            "           1.2859e+00,  3.0392e-01,  1.8110e+00,  6.3502e-01, -8.1953e-02,\n",
            "          -2.1208e+00,  1.2516e+00, -6.8260e-01,  3.8377e-01,  1.5030e-02,\n",
            "          -2.8009e-01,  1.4896e+00, -4.6460e-01, -1.9210e+00, -1.0624e-01,\n",
            "           1.0614e+00,  9.3078e-01,  3.1170e+00, -1.5428e+00, -2.2848e+00,\n",
            "           5.7553e-01, -8.0399e-01,  8.0102e-01,  8.8035e-03, -4.7506e-01,\n",
            "          -9.6295e-01, -5.0778e-01,  1.0184e-01,  1.9141e+00, -1.9252e+00,\n",
            "          -1.5554e+00, -1.8777e-01, -8.5990e-01, -1.6050e+00, -6.9845e-01],\n",
            "         [-1.5101e+00, -9.4842e-02,  1.0927e+00,  1.5050e-01,  1.6347e+00,\n",
            "          -5.1818e-02,  4.9956e-01,  7.2163e-01, -8.9682e-01, -4.1225e-01,\n",
            "           1.0030e+00,  8.5082e-01,  2.1783e-01,  3.2751e-02, -1.6986e-01,\n",
            "           1.0659e+00, -6.1774e-01,  1.1824e+00,  2.1388e-02, -2.1542e-01,\n",
            "          -1.4623e+00,  2.1707e+00,  1.6242e-01,  1.0296e+00,  4.1543e-01,\n",
            "           6.2067e-01,  2.3406e-01, -3.2617e-02,  1.0124e+00,  1.5122e+00,\n",
            "          -3.3592e-01,  2.4560e-01,  1.8682e+00,  7.5362e-01, -1.1766e-01,\n",
            "          -1.9669e-01, -9.5524e-01, -8.9947e-01, -9.5830e-01, -5.9446e-01,\n",
            "           1.3208e-01, -5.4062e-01,  1.4049e-01, -7.3214e-01,  1.1796e+00,\n",
            "           1.3316e+00, -2.0936e-01,  9.6004e-02,  9.0395e-01, -4.0319e-01,\n",
            "           3.0272e-01, -8.0341e-01, -1.2537e+00, -1.5195e+00,  7.4456e-01,\n",
            "           1.1914e+00, -8.0608e-01, -6.2898e-01,  1.2447e+00, -2.4400e+00,\n",
            "           8.4084e-01, -3.9928e-01, -6.1258e-01, -6.5973e-01,  7.6238e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[4]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4]])\n",
            "logits in generate tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
            "           2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
            "          -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
            "           1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
            "          -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
            "          -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
            "           1.5236e+00,  2.5086e+00, -6.6310e-01, -2.5128e-01,  1.0101e+00,\n",
            "           1.2155e-01,  1.5840e-01,  1.1340e+00, -1.1539e+00, -2.9840e-01,\n",
            "          -5.0754e-01, -9.2392e-01,  5.4671e-01, -1.4948e+00, -1.2057e+00,\n",
            "           5.7182e-01, -5.9735e-01, -6.9368e-01,  1.6455e+00, -8.0299e-01,\n",
            "           1.3514e+00, -2.7592e-01, -1.5108e+00,  2.1048e+00,  2.7630e+00,\n",
            "          -1.7465e+00,  1.4516e+00, -1.5103e+00,  8.2115e-01, -2.1153e-01,\n",
            "           7.7890e-01,  1.5333e+00,  1.6097e+00, -4.0323e-01, -8.3447e-01],\n",
            "         [-1.0699e+00, -6.1188e-01, -4.0340e-01,  3.0253e-01,  6.8522e-01,\n",
            "          -1.0045e+00, -1.0104e+00, -1.0886e+00,  1.3292e+00,  5.9115e-01,\n",
            "          -1.1082e+00, -1.2869e+00, -8.1702e-01,  9.6821e-01,  1.6030e+00,\n",
            "          -7.2557e-02, -4.7249e-01, -1.1616e+00,  5.9623e-01,  1.3058e+00,\n",
            "          -7.4218e-01, -1.2529e+00,  6.7504e-01,  1.5664e+00, -9.2379e-01,\n",
            "          -9.5599e-02, -1.5452e+00, -1.8011e-01,  3.1838e+00, -1.2773e-01,\n",
            "           9.1026e-02,  5.4223e-01, -6.1099e-01,  5.2205e-01,  2.1368e+00,\n",
            "          -1.4166e+00, -8.5569e-01,  1.0129e+00,  6.5031e-01,  2.4319e-01,\n",
            "           1.2588e+00, -6.4436e-02, -9.7071e-01, -4.8801e-01, -2.5501e-01,\n",
            "          -4.0889e-01, -7.6871e-01,  1.0953e+00,  1.5294e+00, -1.2395e+00,\n",
            "           1.0547e+00,  5.1084e-01,  3.8535e-01, -8.8981e-01,  1.3468e+00,\n",
            "           2.3590e+00,  1.0713e-01, -1.2616e+00,  7.9453e-01, -7.7394e-01,\n",
            "          -1.4970e-01, -6.2144e-01,  1.0078e+00,  2.9297e-01,  9.4333e-02],\n",
            "         [-6.7219e-01,  2.3223e-01, -1.6322e-01,  5.9260e-01,  1.7734e+00,\n",
            "           1.2618e+00,  6.4736e-01, -3.5194e-01,  1.0237e+00, -1.1841e-01,\n",
            "           1.4460e-01,  4.7685e-02, -4.3171e-01,  5.8187e-03, -3.4785e-01,\n",
            "          -2.1878e-01,  1.2574e+00, -7.7578e-01,  9.0806e-01, -1.1492e+00,\n",
            "          -1.6415e+00,  1.3099e+00,  1.2829e+00, -9.7543e-01,  5.8884e-01,\n",
            "          -3.2342e-01, -9.8759e-01, -1.6031e-01, -2.2729e-01,  6.2945e-01,\n",
            "          -4.7032e-01, -1.4203e-01, -1.0257e+00,  3.6481e-01,  8.0206e-01,\n",
            "           5.1417e-01, -1.0679e+00, -6.2952e-01, -1.1669e-01, -3.3667e-02,\n",
            "           2.6093e-01, -2.8767e-01,  1.7954e+00,  6.8430e-01, -8.2680e-01,\n",
            "           1.8204e+00,  3.7831e-01,  5.8637e-01, -2.3296e-01, -3.0982e-01,\n",
            "           7.6794e-01, -2.6885e-02,  6.2131e-01, -1.3444e+00, -1.3337e+00,\n",
            "           2.5620e-01, -1.1706e+00, -5.7987e-01, -1.1549e+00,  4.5945e-01,\n",
            "           1.0986e-01, -4.5925e-01,  1.3903e-01,  7.5598e-01,  4.2960e-01],\n",
            "         [-2.0333e+00, -5.5379e-01,  1.6957e-01,  6.9847e-01,  3.7444e-01,\n",
            "          -1.4854e+00,  6.5020e-01, -2.0446e-01, -1.8770e-01,  4.4862e-01,\n",
            "           5.5716e-01, -5.4564e-01,  1.6367e-01, -1.8262e+00,  6.8537e-01,\n",
            "          -8.7624e-01, -1.4931e+00,  1.6634e+00, -4.7180e-01,  5.8567e-01,\n",
            "          -9.5790e-01,  9.4345e-01, -2.1992e+00,  4.1636e-01, -3.5643e-01,\n",
            "          -1.0305e+00, -1.0478e+00,  2.1709e+00, -1.0414e+00, -1.3088e+00,\n",
            "           1.0169e+00,  4.7949e-01, -1.8578e+00,  2.3297e-01, -2.2054e-01,\n",
            "          -1.4869e+00, -6.2718e-01,  2.0144e+00, -6.9383e-01,  1.6325e+00,\n",
            "          -7.5949e-01,  5.8486e-01,  1.0116e+00, -3.8033e-01, -4.3962e-01,\n",
            "           1.4687e-01,  1.2890e+00, -2.0776e-02,  6.9753e-02, -7.2962e-01,\n",
            "           1.6526e-01, -3.3901e-01,  1.5416e+00,  1.0231e+00,  1.3392e+00,\n",
            "           1.6886e+00,  7.0994e-01,  1.3693e+00, -7.0759e-01,  1.5387e+00,\n",
            "          -9.5591e-01,  2.0515e+00,  9.6234e-03,  1.1159e+00, -2.9663e-01],\n",
            "         [-1.2542e+00,  7.7045e-03, -1.5728e+00,  6.5277e-01, -2.0244e+00,\n",
            "          -1.3731e+00,  1.8886e+00,  2.6879e+00,  9.9400e-01, -1.9079e+00,\n",
            "          -8.0425e-01, -3.3584e-01,  4.1157e-01,  5.5767e-01, -8.9111e-01,\n",
            "          -1.0478e+00, -1.9065e+00, -5.4763e-01, -1.2786e+00, -1.5821e-01,\n",
            "           1.5599e+00, -1.4960e-01, -1.4406e+00,  6.4884e-01, -1.3412e+00,\n",
            "           3.5690e-01, -9.5360e-01, -1.8299e+00, -2.6945e-01,  1.3495e-01,\n",
            "           7.8498e-01,  5.1610e-01, -3.3919e-01,  4.1274e-01,  1.1458e+00,\n",
            "          -1.2133e+00, -2.3697e-01,  1.1260e-01,  8.6032e-02, -4.9708e-01,\n",
            "           6.5827e-01, -8.7969e-01, -4.5012e-02,  1.4311e-01,  1.6021e+00,\n",
            "          -8.1496e-01,  3.5066e-01, -2.2391e-01, -2.4013e+00, -7.1166e-01,\n",
            "          -3.7820e-01,  9.8901e-01, -1.2497e+00,  2.1982e-01,  9.1433e-01,\n",
            "          -1.5917e-01, -1.6053e+00, -1.1741e+00,  6.2891e-01, -9.8252e-01,\n",
            "          -1.4075e+00, -1.3757e+00, -1.0283e-01, -1.5216e+00, -4.9747e-01],\n",
            "         [ 3.1397e-01,  2.2434e+00,  1.6029e+00, -7.2497e-01,  2.9983e-01,\n",
            "          -8.8427e-01,  1.5462e+00, -7.6456e-01, -2.4664e-01, -6.2308e-01,\n",
            "          -4.7668e-02, -2.0922e+00,  2.8453e-02,  1.2881e-01, -7.6382e-01,\n",
            "           9.9616e-02,  2.8292e-01, -1.2844e+00, -2.7384e-02,  1.1135e-01,\n",
            "           9.4199e-01,  4.3506e-02,  1.2219e+00, -7.7057e-01,  6.8530e-01,\n",
            "          -6.4845e-01,  6.4989e-01, -7.2935e-02, -3.7522e-01, -1.3073e+00,\n",
            "          -3.6368e-01, -2.0290e-01,  8.7407e-01,  1.6973e-01, -2.4297e+00,\n",
            "          -1.3099e-01, -1.7969e+00, -5.6029e-01,  9.1693e-01,  4.2259e-02,\n",
            "          -2.0503e-02,  2.0810e-01,  7.1212e-01, -7.5433e-01, -1.6761e+00,\n",
            "           6.4304e-01,  7.8008e-02,  6.1496e-01, -2.7030e-01, -2.6203e-01,\n",
            "           1.6763e+00,  9.9972e-01, -1.3624e-01, -1.2802e+00, -1.4307e+00,\n",
            "          -1.2849e+00, -8.9253e-01,  5.5121e-01, -5.5269e-01, -1.3258e+00,\n",
            "          -1.8546e-01, -3.1307e-01,  1.5000e-01,  2.4338e+00, -5.9372e-01],\n",
            "         [ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "          -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "          -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "          -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "          -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "           6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "           5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "          -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "           1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "           1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "           2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "          -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "           4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01],\n",
            "         [-7.2476e-01, -1.0435e+00,  1.9399e+00,  2.7012e-01,  1.2115e+00,\n",
            "           7.8650e-01, -7.0843e-01,  4.2924e-01, -7.9058e-01, -7.1254e-02,\n",
            "          -5.1328e-02, -1.0868e+00,  1.7941e+00,  2.9968e-01,  4.2471e-02,\n",
            "           5.3961e-02, -1.5205e+00,  7.1624e-01,  1.0564e-01,  7.5274e-01,\n",
            "          -8.0357e-01, -1.3313e+00,  1.9921e+00, -3.1164e-02,  7.4122e-01,\n",
            "          -5.4240e-02,  7.6267e-01, -5.4700e-01, -6.7336e-01,  1.0242e+00,\n",
            "          -8.7273e-01, -9.2215e-01,  1.3338e+00,  2.1707e-01, -3.5063e-01,\n",
            "           4.9493e-01,  7.8289e-01,  6.7032e-01, -1.8922e+00, -1.2433e+00,\n",
            "           1.8634e+00,  3.7374e-01, -1.1434e+00, -4.8448e-01,  2.5159e-02,\n",
            "           1.3182e-01,  7.0862e-01, -1.5011e+00,  3.0252e-02,  2.2015e-01,\n",
            "           7.7363e-01,  1.8161e-01,  7.0511e-02,  8.7861e-02, -7.5368e-01,\n",
            "           6.0073e-01, -8.2280e-01, -1.9710e+00, -7.3877e-01, -1.8158e+00,\n",
            "          -3.4499e-01,  2.4259e-01,  7.6081e-02, -1.4083e-01,  3.9228e-01],\n",
            "         [ 7.6813e-01, -4.3027e-02,  1.0727e-01, -2.0594e-01, -1.2616e+00,\n",
            "          -2.2699e-01,  1.0661e-01, -1.6830e-01,  2.4331e-01, -5.7089e-01,\n",
            "           2.3408e+00, -2.5665e+00,  9.9348e-01, -1.2445e-02, -6.9706e-01,\n",
            "           7.9027e-01,  5.0506e-01, -6.0040e-01, -3.2868e-01, -2.8179e-01,\n",
            "           2.7992e-01,  9.6782e-01, -1.0858e+00,  1.0346e+00,  4.2809e-02,\n",
            "          -3.0254e-01,  5.5197e-01, -5.9396e-01,  7.7175e-01, -2.5025e-01,\n",
            "          -4.9726e-01, -1.5470e-01,  1.4592e+00, -4.7539e-01,  8.2144e-02,\n",
            "           2.2645e+00, -1.2837e+00, -4.5395e-01, -9.4220e-01, -6.6965e-01,\n",
            "           6.0243e-02,  1.4642e-01,  3.0068e-01, -6.9550e-01, -1.0944e+00,\n",
            "           9.2950e-01,  3.1552e-01, -3.9422e-01, -9.6779e-02,  1.3060e+00,\n",
            "          -1.2617e+00, -6.6971e-02,  2.9986e-01, -7.0433e-01, -3.8956e-02,\n",
            "          -1.8838e+00, -6.0241e-01, -3.3658e-01,  3.0248e-01, -4.2422e-01,\n",
            "           2.3600e-01,  5.4246e-01, -6.4988e-01,  6.1445e-01,  1.6690e-01],\n",
            "         [-2.9501e-01, -6.5114e-01,  1.4937e+00,  1.1173e+00,  7.3555e-01,\n",
            "          -1.6496e-02, -2.4196e-01, -2.7016e-01,  5.9757e-02, -3.6668e-01,\n",
            "           2.2548e-01, -9.4109e-01, -1.6868e+00, -9.2918e-01, -1.2395e+00,\n",
            "          -1.0842e+00, -3.6475e-01, -1.0916e-01, -1.5911e-01,  6.8217e-01,\n",
            "           1.1651e+00,  9.5281e-01,  1.5480e-01, -5.7851e-02, -1.0556e+00,\n",
            "          -7.0173e-01,  4.9441e-01,  1.1985e+00, -5.6718e-01, -3.2044e-01,\n",
            "           1.5870e+00,  1.5017e+00, -2.5039e+00,  8.5252e-01,  5.6069e-02,\n",
            "          -4.1780e-02,  3.5676e-01, -1.5907e+00, -7.3743e-01, -1.2256e+00,\n",
            "           6.8221e-02,  1.0599e+00, -8.2862e-01,  8.3455e-03, -3.3936e-01,\n",
            "           8.2959e-01,  2.7894e-01, -1.0439e+00, -1.4084e+00, -2.2760e-01,\n",
            "          -6.5707e-01, -5.2813e-01, -2.7606e-02, -9.0137e-01,  1.4106e+00,\n",
            "           1.1756e+00, -1.5821e-02, -5.7049e-01,  2.0617e+00,  3.7564e-01,\n",
            "          -4.3155e-01, -6.9685e-01, -5.2504e-01,  1.2672e+00,  2.6002e+00],\n",
            "         [ 2.4746e-01, -6.3485e-01, -1.2909e+00,  1.1822e+00,  1.4787e-01,\n",
            "          -4.3331e-01, -8.2693e-01,  7.2802e-02, -1.2982e+00,  3.9600e-01,\n",
            "          -1.2460e+00,  1.4583e-01, -5.6994e-01, -1.3561e+00, -3.8121e-01,\n",
            "          -8.5146e-01,  1.1918e+00, -8.1080e-01, -1.7326e-01, -4.7029e-01,\n",
            "          -6.0004e-01, -1.3636e+00, -1.0889e+00,  1.0108e+00,  8.5429e-01,\n",
            "          -4.4113e-02,  1.8017e+00,  6.0141e-01, -2.5448e+00, -4.8652e-01,\n",
            "           2.6412e+00,  1.6053e+00,  5.9007e-01,  8.1368e-01, -1.1238e-01,\n",
            "          -3.0501e-01,  1.1426e+00,  6.6372e-01, -7.0001e-01,  9.2620e-01,\n",
            "          -1.1032e+00, -1.2125e+00,  6.0654e-01,  5.8816e-01, -5.4526e-01,\n",
            "           7.6541e-01,  5.6915e-01,  8.8591e-01, -7.0044e-02,  6.7919e-01,\n",
            "          -2.8304e-02, -1.2243e+00, -1.7192e+00,  1.4801e+00,  9.5867e-01,\n",
            "          -3.3781e-02,  5.0831e-01, -2.5017e-01,  2.0734e+00, -2.9941e-01,\n",
            "           4.7293e-02, -9.6258e-01,  1.3064e+00, -2.2557e-01, -1.8305e+00],\n",
            "         [-4.6495e-01, -4.0384e-01,  5.1750e-01,  3.4527e-01,  1.0466e+00,\n",
            "           2.6479e-01, -9.6930e-01,  5.4509e-01, -2.9051e-01,  6.2907e-01,\n",
            "           8.8252e-01, -2.2943e+00, -2.0391e-01,  4.0566e-01, -1.5495e+00,\n",
            "          -2.1440e-01, -6.5544e-01,  2.1339e+00, -5.9239e-01,  2.4027e-01,\n",
            "          -4.3088e-01,  8.8368e-01,  4.6157e-01,  4.4638e-01,  8.0647e-01,\n",
            "          -9.3765e-02,  1.0844e+00, -1.0433e+00, -1.9601e+00,  2.0837e-01,\n",
            "           3.0050e-01,  8.8042e-01, -8.3029e-01,  6.5888e-01, -3.1582e-01,\n",
            "          -2.0885e-01,  1.4769e+00,  6.8216e-01, -8.8222e-01,  7.7618e-01,\n",
            "          -6.2467e-02,  8.4428e-01, -1.5745e-02,  1.7251e+00, -1.9974e+00,\n",
            "           4.0745e-01, -1.5205e+00,  8.7902e-01, -6.0891e-02, -2.0592e-01,\n",
            "           1.3214e+00,  6.6606e-01,  2.9364e-01, -6.1167e-01, -8.8674e-01,\n",
            "          -1.3853e-01,  5.8954e-01, -2.2584e-01, -9.9184e-01, -1.0729e+00,\n",
            "          -2.4981e-01, -8.8683e-01,  1.5427e+00,  1.0735e+00, -4.7932e-02],\n",
            "         [-1.3237e-01, -5.4889e-01,  1.0244e-01, -6.9162e-01,  3.5075e-01,\n",
            "           1.6147e+00,  1.8203e+00,  5.1224e-01,  1.5810e+00, -2.0063e+00,\n",
            "          -1.2925e+00,  1.2681e-01,  1.1099e+00, -6.5921e-01,  8.0844e-01,\n",
            "           1.9072e+00, -3.2599e-01, -3.4377e-01, -1.4415e+00, -1.8276e-01,\n",
            "          -8.8043e-01, -6.1918e-01, -1.4047e+00, -8.5837e-01, -3.8297e-01,\n",
            "          -5.3723e-01, -1.2176e+00, -1.9403e+00, -3.0937e-01,  1.7895e-01,\n",
            "           1.2859e+00,  3.0392e-01,  1.8110e+00,  6.3502e-01, -8.1953e-02,\n",
            "          -2.1208e+00,  1.2516e+00, -6.8260e-01,  3.8377e-01,  1.5030e-02,\n",
            "          -2.8009e-01,  1.4896e+00, -4.6460e-01, -1.9210e+00, -1.0624e-01,\n",
            "           1.0614e+00,  9.3078e-01,  3.1170e+00, -1.5428e+00, -2.2848e+00,\n",
            "           5.7553e-01, -8.0399e-01,  8.0102e-01,  8.8035e-03, -4.7506e-01,\n",
            "          -9.6295e-01, -5.0778e-01,  1.0184e-01,  1.9141e+00, -1.9252e+00,\n",
            "          -1.5554e+00, -1.8777e-01, -8.5990e-01, -1.6050e+00, -6.9845e-01],\n",
            "         [-1.5101e+00, -9.4842e-02,  1.0927e+00,  1.5050e-01,  1.6347e+00,\n",
            "          -5.1818e-02,  4.9956e-01,  7.2163e-01, -8.9682e-01, -4.1225e-01,\n",
            "           1.0030e+00,  8.5082e-01,  2.1783e-01,  3.2751e-02, -1.6986e-01,\n",
            "           1.0659e+00, -6.1774e-01,  1.1824e+00,  2.1388e-02, -2.1542e-01,\n",
            "          -1.4623e+00,  2.1707e+00,  1.6242e-01,  1.0296e+00,  4.1543e-01,\n",
            "           6.2067e-01,  2.3406e-01, -3.2617e-02,  1.0124e+00,  1.5122e+00,\n",
            "          -3.3592e-01,  2.4560e-01,  1.8682e+00,  7.5362e-01, -1.1766e-01,\n",
            "          -1.9669e-01, -9.5524e-01, -8.9947e-01, -9.5830e-01, -5.9446e-01,\n",
            "           1.3208e-01, -5.4062e-01,  1.4049e-01, -7.3214e-01,  1.1796e+00,\n",
            "           1.3316e+00, -2.0936e-01,  9.6004e-02,  9.0395e-01, -4.0319e-01,\n",
            "           3.0272e-01, -8.0341e-01, -1.2537e+00, -1.5195e+00,  7.4456e-01,\n",
            "           1.1914e+00, -8.0608e-01, -6.2898e-01,  1.2447e+00, -2.4400e+00,\n",
            "           8.4084e-01, -3.9928e-01, -6.1258e-01, -6.5973e-01,  7.6238e-01],\n",
            "         [ 8.1485e-01, -6.4297e-02,  1.4237e+00,  2.6173e-01, -1.8528e+00,\n",
            "           2.0186e-01, -1.1787e+00, -1.0358e-01, -1.7830e+00, -8.3234e-01,\n",
            "          -4.3462e-01, -1.2480e+00, -2.8797e-01,  8.8086e-01, -7.1896e-01,\n",
            "           1.7449e-01,  7.5198e-01, -6.2878e-02, -7.1113e-01,  9.8100e-01,\n",
            "          -7.2443e-01, -1.5010e+00, -2.8348e+00, -2.8272e+00, -1.7358e-01,\n",
            "           5.1187e-02, -6.5764e-01, -2.5729e+00,  2.1011e-02,  1.0060e+00,\n",
            "          -1.2492e+00,  2.4413e-01, -6.3866e-01, -3.1861e-01, -1.2942e+00,\n",
            "          -1.0726e+00,  2.2901e-01, -9.0008e-01,  6.6140e-01,  5.1178e-01,\n",
            "           6.7622e-01, -1.3639e+00,  5.4861e-01,  8.9502e-02,  3.5746e-01,\n",
            "          -1.6521e+00, -7.5838e-01,  6.9533e-02,  9.9369e-01, -2.8205e-01,\n",
            "           1.1088e+00, -1.9881e+00, -1.3916e+00,  1.2734e+00, -1.1732e+00,\n",
            "           5.8200e-01, -1.3185e+00,  7.8586e-01, -1.1501e+00,  1.3132e+00,\n",
            "           2.2007e+00, -2.1945e-01,  5.4272e-01,  2.5867e+00, -4.6874e-01]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.8148, -0.0643,  1.4237,  0.2617, -1.8528,  0.2019, -1.1787, -0.1036,\n",
            "         -1.7830, -0.8323, -0.4346, -1.2480, -0.2880,  0.8809, -0.7190,  0.1745,\n",
            "          0.7520, -0.0629, -0.7111,  0.9810, -0.7244, -1.5010, -2.8348, -2.8272,\n",
            "         -0.1736,  0.0512, -0.6576, -2.5729,  0.0210,  1.0060, -1.2492,  0.2441,\n",
            "         -0.6387, -0.3186, -1.2942, -1.0726,  0.2290, -0.9001,  0.6614,  0.5118,\n",
            "          0.6762, -1.3639,  0.5486,  0.0895,  0.3575, -1.6521, -0.7584,  0.0695,\n",
            "          0.9937, -0.2821,  1.1088, -1.9881, -1.3916,  1.2734, -1.1732,  0.5820,\n",
            "         -1.3185,  0.7859, -1.1501,  1.3132,  2.2007, -0.2195,  0.5427,  2.5867,\n",
            "         -0.4687]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0245, 0.0102, 0.0450, 0.0141, 0.0017, 0.0133, 0.0033, 0.0098, 0.0018,\n",
            "         0.0047, 0.0070, 0.0031, 0.0081, 0.0261, 0.0053, 0.0129, 0.0230, 0.0102,\n",
            "         0.0053, 0.0289, 0.0052, 0.0024, 0.0006, 0.0006, 0.0091, 0.0114, 0.0056,\n",
            "         0.0008, 0.0111, 0.0296, 0.0031, 0.0138, 0.0057, 0.0079, 0.0030, 0.0037,\n",
            "         0.0136, 0.0044, 0.0210, 0.0181, 0.0213, 0.0028, 0.0187, 0.0118, 0.0155,\n",
            "         0.0021, 0.0051, 0.0116, 0.0293, 0.0082, 0.0328, 0.0015, 0.0027, 0.0387,\n",
            "         0.0034, 0.0194, 0.0029, 0.0238, 0.0034, 0.0403, 0.0978, 0.0087, 0.0186,\n",
            "         0.1439, 0.0068]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[48]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [ 0.8148, -0.0643,  1.4237,  ...,  0.5427,  2.5867, -0.4687],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.5377, -0.2542, -1.8516,  1.3382, -1.0219, -0.9356,  0.8454, -1.3798,\n",
            "         -0.1421,  0.7084, -0.2751,  1.2128,  1.3650, -1.3301, -1.4832, -0.9809,\n",
            "         -1.5012, -1.7006,  1.2642, -1.1078,  0.5398, -0.7718,  0.6175,  2.1793,\n",
            "         -0.1047, -0.7940,  1.1206, -0.9039,  0.4935,  0.5804,  1.2005, -1.5786,\n",
            "         -0.5037, -0.7478, -1.3617, -0.3347,  0.7188, -1.7258, -0.0902, -0.0148,\n",
            "          0.9630, -1.7663, -0.3839, -0.0170, -1.0878,  0.6621, -0.8364, -0.8048,\n",
            "          0.0204, -0.5749, -0.4291,  1.1989,  0.0905, -0.4063,  0.1165, -1.2079,\n",
            "         -1.1770,  0.6919, -1.3267,  2.5186, -1.0516, -1.0338,  2.1974,  0.3053,\n",
            "         -1.1890]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0172, 0.0078, 0.0016, 0.0384, 0.0036, 0.0039, 0.0234, 0.0025, 0.0087,\n",
            "         0.0204, 0.0076, 0.0338, 0.0394, 0.0027, 0.0023, 0.0038, 0.0022, 0.0018,\n",
            "         0.0356, 0.0033, 0.0173, 0.0047, 0.0187, 0.0889, 0.0091, 0.0045, 0.0309,\n",
            "         0.0041, 0.0165, 0.0180, 0.0334, 0.0021, 0.0061, 0.0048, 0.0026, 0.0072,\n",
            "         0.0206, 0.0018, 0.0092, 0.0099, 0.0264, 0.0017, 0.0069, 0.0099, 0.0034,\n",
            "         0.0195, 0.0044, 0.0045, 0.0103, 0.0057, 0.0066, 0.0334, 0.0110, 0.0067,\n",
            "         0.0113, 0.0030, 0.0031, 0.0201, 0.0027, 0.1249, 0.0035, 0.0036, 0.0906,\n",
            "         0.0137, 0.0031]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.8148, -0.0643,  1.4237,  ...,  0.5427,  2.5867, -0.4687],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[16]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.9280, -0.2083, -0.6648, -0.2641,  1.7087, -1.3229, -0.1711,  0.6874,\n",
            "          1.9919,  1.2724,  0.2194, -1.2614, -1.0279,  0.4589, -0.6532,  1.1831,\n",
            "         -0.3666,  0.4472, -0.2843,  0.2079,  1.5236,  0.7154,  0.6585,  0.4589,\n",
            "         -0.5357,  1.2628, -1.5580,  0.4196, -0.3806, -0.7044, -0.3884, -1.8927,\n",
            "          0.8618,  1.2285, -0.1141,  1.7779,  0.8297,  0.3457, -0.3843, -1.5953,\n",
            "         -0.1820, -0.5678,  0.6511, -0.4463, -0.2109, -0.9229,  0.2308, -0.7586,\n",
            "         -1.6410, -1.3728, -1.3389, -0.5246,  1.9338,  0.3034,  0.2951,  1.5918,\n",
            "         -1.6319, -1.1146, -0.3941,  0.4972, -2.0602, -1.8428,  0.8573,  1.3835,\n",
            "          0.4217]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0014, 0.0079, 0.0050, 0.0075, 0.0541, 0.0026, 0.0082, 0.0195, 0.0717,\n",
            "         0.0349, 0.0122, 0.0028, 0.0035, 0.0155, 0.0051, 0.0320, 0.0068, 0.0153,\n",
            "         0.0074, 0.0121, 0.0449, 0.0200, 0.0189, 0.0155, 0.0057, 0.0346, 0.0021,\n",
            "         0.0149, 0.0067, 0.0048, 0.0066, 0.0015, 0.0232, 0.0334, 0.0087, 0.0579,\n",
            "         0.0224, 0.0138, 0.0067, 0.0020, 0.0082, 0.0055, 0.0188, 0.0063, 0.0079,\n",
            "         0.0039, 0.0123, 0.0046, 0.0019, 0.0025, 0.0026, 0.0058, 0.0677, 0.0133,\n",
            "         0.0131, 0.0481, 0.0019, 0.0032, 0.0066, 0.0161, 0.0012, 0.0016, 0.0231,\n",
            "         0.0390, 0.0149]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[22]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-0.0405, -0.0815, -0.3447,  ..., -0.5547,  0.2084,  0.4000]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.0405, -0.0815, -0.3447,  0.8249,  0.7040, -0.4801, -1.8527,  0.2914,\n",
            "          0.3634, -1.0550,  0.1752,  0.7511,  0.6185,  1.6865,  0.7570, -1.2772,\n",
            "         -0.3714, -0.7821,  1.4152,  1.4454,  0.1491, -1.3717,  0.8696,  0.7952,\n",
            "         -0.4211,  1.6545, -0.0754,  0.8137,  0.1569, -0.8836, -1.3572,  0.2519,\n",
            "         -0.7181,  1.2180, -0.6745,  1.0277,  1.2858, -0.5710, -0.2638,  1.1981,\n",
            "          0.7819, -0.1348,  1.0729, -1.1304, -1.2815,  1.2645,  0.7293,  1.2863,\n",
            "          1.0577, -0.5415,  0.4685,  1.2207,  0.0814, -0.8279,  0.8513,  0.1214,\n",
            "          1.2588,  0.6469,  0.1959,  0.4521,  0.1621, -0.1879, -0.5547,  0.2084,\n",
            "          0.4000]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0088, 0.0084, 0.0065, 0.0208, 0.0184, 0.0056, 0.0014, 0.0122, 0.0131,\n",
            "         0.0032, 0.0109, 0.0193, 0.0169, 0.0492, 0.0194, 0.0025, 0.0063, 0.0042,\n",
            "         0.0375, 0.0387, 0.0106, 0.0023, 0.0217, 0.0202, 0.0060, 0.0477, 0.0085,\n",
            "         0.0206, 0.0107, 0.0038, 0.0023, 0.0117, 0.0044, 0.0308, 0.0046, 0.0255,\n",
            "         0.0330, 0.0051, 0.0070, 0.0302, 0.0199, 0.0080, 0.0266, 0.0029, 0.0025,\n",
            "         0.0323, 0.0189, 0.0330, 0.0262, 0.0053, 0.0146, 0.0309, 0.0099, 0.0040,\n",
            "         0.0214, 0.0103, 0.0321, 0.0174, 0.0111, 0.0143, 0.0107, 0.0076, 0.0052,\n",
            "         0.0112, 0.0136]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[45]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-0.0405, -0.0815, -0.3447,  ..., -0.5547,  0.2084,  0.4000],\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 6.6348e-01,  2.6726e-01, -4.0968e-02, -4.1045e-01, -7.9258e-01,\n",
            "         -4.5052e-01, -1.2630e+00, -1.1049e-01, -1.5258e+00, -2.4088e+00,\n",
            "          6.2567e-01, -7.8628e-01, -1.3341e-01, -5.0673e-01,  4.9312e-01,\n",
            "          3.1957e+00, -6.9719e-01,  1.4158e-01,  1.1991e+00,  8.4574e-01,\n",
            "          1.1119e+00,  7.5411e-01,  5.6716e-01,  1.0343e+00,  4.2398e-01,\n",
            "         -3.9114e-01, -2.2213e+00,  1.7533e+00, -1.2363e+00,  1.1138e+00,\n",
            "          1.7952e+00,  2.8732e-01, -1.9710e-01,  1.2285e+00,  9.5273e-03,\n",
            "          2.2227e-01,  1.9963e+00,  1.3765e+00,  1.0229e+00, -1.3248e-03,\n",
            "          7.7858e-01, -2.9392e-01,  1.2563e+00, -7.8401e-01,  8.0610e-01,\n",
            "         -3.7246e-01, -8.1083e-01,  8.6826e-01,  7.9161e-01,  6.6330e-01,\n",
            "          1.8970e-01,  1.7075e+00,  7.7272e-01, -2.6976e-01, -7.1044e-01,\n",
            "          1.7779e+00, -7.2955e-01, -8.2731e-01, -2.5742e+00, -3.9104e-01,\n",
            "          2.3160e-02,  8.5039e-01, -5.8610e-01, -1.0893e+00,  1.9482e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0144, 0.0097, 0.0071, 0.0049, 0.0034, 0.0047, 0.0021, 0.0066, 0.0016,\n",
            "         0.0007, 0.0139, 0.0034, 0.0065, 0.0045, 0.0122, 0.1813, 0.0037, 0.0086,\n",
            "         0.0246, 0.0173, 0.0226, 0.0158, 0.0131, 0.0209, 0.0113, 0.0050, 0.0008,\n",
            "         0.0429, 0.0022, 0.0226, 0.0447, 0.0099, 0.0061, 0.0254, 0.0075, 0.0093,\n",
            "         0.0546, 0.0294, 0.0206, 0.0074, 0.0162, 0.0055, 0.0261, 0.0034, 0.0166,\n",
            "         0.0051, 0.0033, 0.0177, 0.0164, 0.0144, 0.0090, 0.0409, 0.0161, 0.0057,\n",
            "         0.0036, 0.0439, 0.0036, 0.0032, 0.0006, 0.0050, 0.0076, 0.0174, 0.0041,\n",
            "         0.0025, 0.0090]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[27]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.0405, -0.0815, -0.3447,  ..., -0.5547,  0.2084,  0.4000],\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948],\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1600,  1.3981, -0.7047, -0.1852, -0.6243,  1.2858,  0.1368, -0.2757,\n",
            "         -0.4958, -0.4759,  0.6607, -1.4100, -1.8479, -0.4986,  0.2404,  0.3062,\n",
            "         -1.3790,  0.6002,  0.1567, -0.1926, -0.1057,  1.2514,  0.7167,  0.6792,\n",
            "          1.7737,  0.7020, -0.1789,  0.1785, -1.0140, -0.1913,  0.2432, -1.2693,\n",
            "          0.9723, -0.2344,  0.2829,  0.4270,  0.6493, -0.3012, -0.4901, -1.3679,\n",
            "          2.2490,  0.5682,  1.5880, -0.7335, -1.6787, -0.0336, -1.5213,  0.3886,\n",
            "          1.0050, -1.2381,  1.3319,  0.1538,  0.6376, -0.7428,  1.6414, -0.2680,\n",
            "         -0.4543,  0.7176,  0.3635, -1.1256,  0.9422,  0.8838, -1.9908,  0.8574,\n",
            "         -2.1603]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0084, 0.0397, 0.0049, 0.0082, 0.0053, 0.0355, 0.0113, 0.0075, 0.0060,\n",
            "         0.0061, 0.0190, 0.0024, 0.0015, 0.0060, 0.0125, 0.0133, 0.0025, 0.0179,\n",
            "         0.0115, 0.0081, 0.0088, 0.0343, 0.0201, 0.0194, 0.0579, 0.0198, 0.0082,\n",
            "         0.0117, 0.0036, 0.0081, 0.0125, 0.0028, 0.0260, 0.0078, 0.0130, 0.0150,\n",
            "         0.0188, 0.0073, 0.0060, 0.0025, 0.0931, 0.0173, 0.0480, 0.0047, 0.0018,\n",
            "         0.0095, 0.0021, 0.0145, 0.0268, 0.0028, 0.0372, 0.0115, 0.0186, 0.0047,\n",
            "         0.0507, 0.0075, 0.0062, 0.0201, 0.0141, 0.0032, 0.0252, 0.0238, 0.0013,\n",
            "         0.0231, 0.0011]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948],\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[34]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.3402, -0.7501,  0.2942,  0.7626,  2.6536,  0.4730,  0.0314, -0.1513,\n",
            "          1.5333,  1.0515, -0.4613,  2.0802,  0.8309, -0.8416,  0.7062, -1.0877,\n",
            "          0.0698, -1.1470, -0.5624, -0.1978,  0.8101, -1.0031,  1.5461,  1.2420,\n",
            "          0.5707, -0.0135, -1.0993,  1.3919,  0.9944,  0.5453,  0.0429,  0.9108,\n",
            "          1.5571, -0.7094,  0.6623, -1.6373,  0.5799, -0.0682,  0.2936, -1.1519,\n",
            "          1.6110,  0.5486, -2.8114,  0.1064, -1.1193,  0.0351, -0.1004, -0.6131,\n",
            "          2.0785,  1.1399,  1.3345, -0.0741, -1.3989, -0.8832,  0.4431,  0.0717,\n",
            "          1.3737,  0.1206,  0.6658,  0.1704,  0.6585,  1.0239,  0.4602,  0.3275,\n",
            "          2.1779]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0034, 0.0097, 0.0155, 0.1025, 0.0116, 0.0074, 0.0062, 0.0334,\n",
            "         0.0207, 0.0046, 0.0578, 0.0166, 0.0031, 0.0146, 0.0024, 0.0077, 0.0023,\n",
            "         0.0041, 0.0059, 0.0162, 0.0026, 0.0339, 0.0250, 0.0128, 0.0071, 0.0024,\n",
            "         0.0290, 0.0195, 0.0125, 0.0075, 0.0179, 0.0342, 0.0036, 0.0140, 0.0014,\n",
            "         0.0129, 0.0067, 0.0097, 0.0023, 0.0361, 0.0125, 0.0004, 0.0080, 0.0024,\n",
            "         0.0075, 0.0065, 0.0039, 0.0577, 0.0226, 0.0274, 0.0067, 0.0018, 0.0030,\n",
            "         0.0112, 0.0078, 0.0285, 0.0081, 0.0140, 0.0086, 0.0139, 0.0201, 0.0114,\n",
            "         0.0100, 0.0637]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[64]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.4322, -0.2810, -2.2789, -1.5010, -0.5178, -0.0930,  0.7448,  0.2769,\n",
            "         -1.3683, -0.1367,  0.5261,  0.8502,  0.5255, -1.4073, -0.8778,  1.5681,\n",
            "          0.5790, -1.0601, -0.1289,  0.0574, -2.1171,  0.5979, -0.8894, -0.1832,\n",
            "          2.1316,  0.4207, -1.9636, -0.4431,  2.0773, -0.8678,  0.4456, -0.8511,\n",
            "         -0.9897, -0.1547, -0.3183,  0.9285,  0.7569,  0.9505, -1.4028, -0.5422,\n",
            "          0.3932, -1.1699,  0.9138,  1.2533, -0.5639, -0.4533, -0.5694, -1.3843,\n",
            "         -0.1265,  1.6687,  0.4180,  1.1220, -0.4981,  1.7805, -0.3438,  0.0917,\n",
            "          1.4146, -0.9541,  0.4243, -0.4152, -0.9518, -0.9530, -0.5551,  1.0666,\n",
            "          0.5364]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0077, 0.0010, 0.0023, 0.0060, 0.0092, 0.0214, 0.0134, 0.0026,\n",
            "         0.0089, 0.0172, 0.0237, 0.0172, 0.0025, 0.0042, 0.0487, 0.0181, 0.0035,\n",
            "         0.0089, 0.0107, 0.0012, 0.0185, 0.0042, 0.0084, 0.0855, 0.0155, 0.0014,\n",
            "         0.0065, 0.0810, 0.0043, 0.0158, 0.0043, 0.0038, 0.0087, 0.0074, 0.0257,\n",
            "         0.0216, 0.0263, 0.0025, 0.0059, 0.0150, 0.0031, 0.0253, 0.0355, 0.0058,\n",
            "         0.0064, 0.0057, 0.0025, 0.0089, 0.0538, 0.0154, 0.0312, 0.0062, 0.0602,\n",
            "         0.0072, 0.0111, 0.0418, 0.0039, 0.0155, 0.0067, 0.0039, 0.0039, 0.0058,\n",
            "         0.0295, 0.0173]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[5]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.1338,  0.3899, -0.2884,  ..., -0.5512,  1.0477,  1.6187]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1338,  0.3899, -0.2884, -1.4651,  0.0101, -0.3004, -1.5733,  0.0148,\n",
            "         -0.0447, -0.5367, -0.5223, -0.2181, -2.1608,  0.7865,  0.6854, -1.2576,\n",
            "          0.6094, -2.0551, -0.4431, -0.6499, -0.6870,  0.2567, -1.2669,  0.2645,\n",
            "         -0.6445,  1.0834, -0.7995,  0.2922,  1.3143,  1.2607, -0.3505, -2.0660,\n",
            "          1.0575, -1.0572,  0.9911, -0.0797,  1.0751,  0.2381,  0.5757,  1.6685,\n",
            "          0.5976, -1.8736,  1.2910, -0.3753, -1.8943,  0.5557,  0.8567, -0.8461,\n",
            "          0.5015, -0.9656, -0.7255,  0.0990,  0.5928, -0.0422, -0.9566,  1.4424,\n",
            "          0.4341, -0.4292,  0.3666,  0.1275, -0.0560,  0.8315, -0.5512,  1.0477,\n",
            "          1.6187]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0097, 0.0164, 0.0083, 0.0026, 0.0112, 0.0082, 0.0023, 0.0112, 0.0106,\n",
            "         0.0065, 0.0066, 0.0089, 0.0013, 0.0243, 0.0220, 0.0031, 0.0204, 0.0014,\n",
            "         0.0071, 0.0058, 0.0056, 0.0143, 0.0031, 0.0144, 0.0058, 0.0327, 0.0050,\n",
            "         0.0148, 0.0412, 0.0391, 0.0078, 0.0014, 0.0319, 0.0038, 0.0298, 0.0102,\n",
            "         0.0325, 0.0141, 0.0197, 0.0588, 0.0201, 0.0017, 0.0403, 0.0076, 0.0017,\n",
            "         0.0193, 0.0261, 0.0048, 0.0183, 0.0042, 0.0054, 0.0122, 0.0200, 0.0106,\n",
            "         0.0043, 0.0469, 0.0171, 0.0072, 0.0160, 0.0126, 0.0105, 0.0254, 0.0064,\n",
            "         0.0316, 0.0559]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[30]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.1338,  0.3899, -0.2884,  ..., -0.5512,  1.0477,  1.6187],\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1919, -0.1515, -0.6027, -0.7786, -0.8146, -0.0140, -0.2557, -1.1456,\n",
            "         -0.0650, -0.8981,  1.1770,  0.9207,  0.4289, -0.9033, -1.8009,  0.7865,\n",
            "         -1.8513, -0.2894,  1.1134,  0.4341, -2.0746,  0.1995,  1.2089, -0.7427,\n",
            "          0.1015, -0.7365, -1.6778, -0.6234, -0.9012, -0.7467, -1.0201,  2.0264,\n",
            "          0.7080, -0.0137, -0.6840, -0.4390, -0.7137, -1.7636, -0.7221, -0.9141,\n",
            "         -0.8221, -1.6358, -0.4992, -0.8292, -0.7503, -0.9921, -0.2697,  0.1637,\n",
            "          2.1167,  0.1598,  1.1173,  0.7453, -0.6453, -0.2113,  0.8426,  0.6110,\n",
            "          0.6414, -1.5403,  0.0589, -1.0928,  0.5961,  1.0248, -1.3679,  1.4416,\n",
            "         -1.8638]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0392, 0.0102, 0.0065, 0.0055, 0.0053, 0.0117, 0.0092, 0.0038, 0.0112,\n",
            "         0.0049, 0.0386, 0.0299, 0.0183, 0.0048, 0.0020, 0.0262, 0.0019, 0.0089,\n",
            "         0.0363, 0.0184, 0.0015, 0.0145, 0.0399, 0.0057, 0.0132, 0.0057, 0.0022,\n",
            "         0.0064, 0.0048, 0.0056, 0.0043, 0.0904, 0.0242, 0.0117, 0.0060, 0.0077,\n",
            "         0.0058, 0.0020, 0.0058, 0.0048, 0.0052, 0.0023, 0.0072, 0.0052, 0.0056,\n",
            "         0.0044, 0.0091, 0.0140, 0.0989, 0.0140, 0.0364, 0.0251, 0.0062, 0.0096,\n",
            "         0.0277, 0.0219, 0.0226, 0.0026, 0.0126, 0.0040, 0.0216, 0.0332, 0.0030,\n",
            "         0.0504, 0.0018]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[21]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1338,  0.3899, -0.2884,  ..., -0.5512,  1.0477,  1.6187],\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638],\n",
            "         [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-2.1910, -0.7574,  1.9656,  0.9325,  2.3248,  1.5786,  1.2663,  0.8314,\n",
            "          0.6424,  0.9386, -0.7626, -2.2617, -0.7029, -0.1605,  2.2966, -1.9428,\n",
            "          0.4490,  0.2858,  1.1526, -0.0320, -0.6505,  0.7824,  1.2790,  0.9869,\n",
            "         -0.5710,  1.1954,  1.1698,  0.1744,  0.0439,  0.7519,  0.4101, -1.8032,\n",
            "          0.0174,  0.3876,  2.1369,  0.7943, -0.5535, -1.3704, -0.9535,  0.1101,\n",
            "          0.0302,  1.6893,  1.5554, -1.2172,  1.2531,  1.7768, -0.8903,  0.4690,\n",
            "         -0.8725,  1.5689,  1.2495,  0.1315,  1.0818,  2.3450, -1.2294,  0.9712,\n",
            "         -0.8952,  1.2868,  0.8233,  0.0931,  0.3371, -1.4081, -0.3580,  0.8585,\n",
            "         -0.6161]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0007, 0.0030, 0.0464, 0.0165, 0.0664, 0.0315, 0.0230, 0.0149, 0.0123,\n",
            "         0.0166, 0.0030, 0.0007, 0.0032, 0.0055, 0.0645, 0.0009, 0.0102, 0.0086,\n",
            "         0.0206, 0.0063, 0.0034, 0.0142, 0.0233, 0.0174, 0.0037, 0.0215, 0.0209,\n",
            "         0.0077, 0.0068, 0.0138, 0.0098, 0.0011, 0.0066, 0.0096, 0.0550, 0.0144,\n",
            "         0.0037, 0.0016, 0.0025, 0.0072, 0.0067, 0.0352, 0.0308, 0.0019, 0.0227,\n",
            "         0.0384, 0.0027, 0.0104, 0.0027, 0.0312, 0.0227, 0.0074, 0.0192, 0.0678,\n",
            "         0.0019, 0.0172, 0.0027, 0.0235, 0.0148, 0.0071, 0.0091, 0.0016, 0.0045,\n",
            "         0.0153, 0.0035]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[53]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638],\n",
            "         [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1324, -0.5489,  0.1024, -0.6916,  0.3507,  1.6147,  1.8203,  0.5122,\n",
            "          1.5810, -2.0063, -1.2925,  0.1268,  1.1099, -0.6592,  0.8084,  1.9072,\n",
            "         -0.3260, -0.3438, -1.4415, -0.1828, -0.8804, -0.6192, -1.4047, -0.8584,\n",
            "         -0.3830, -0.5372, -1.2176, -1.9403, -0.3094,  0.1790,  1.2859,  0.3039,\n",
            "          1.8110,  0.6350, -0.0820, -2.1208,  1.2516, -0.6826,  0.3838,  0.0150,\n",
            "         -0.2801,  1.4896, -0.4646, -1.9210, -0.1062,  1.0614,  0.9308,  3.1170,\n",
            "         -1.5428, -2.2848,  0.5755, -0.8040,  0.8010,  0.0088, -0.4751, -0.9630,\n",
            "         -0.5078,  0.1018,  1.9141, -1.9252, -1.5554, -0.1878, -0.8599, -1.6050,\n",
            "         -0.6985]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0075, 0.0049, 0.0095, 0.0043, 0.0121, 0.0430, 0.0528, 0.0143, 0.0415,\n",
            "         0.0011, 0.0023, 0.0097, 0.0259, 0.0044, 0.0192, 0.0576, 0.0062, 0.0061,\n",
            "         0.0020, 0.0071, 0.0035, 0.0046, 0.0021, 0.0036, 0.0058, 0.0050, 0.0025,\n",
            "         0.0012, 0.0063, 0.0102, 0.0309, 0.0116, 0.0523, 0.0161, 0.0079, 0.0010,\n",
            "         0.0299, 0.0043, 0.0125, 0.0087, 0.0065, 0.0379, 0.0054, 0.0013, 0.0077,\n",
            "         0.0247, 0.0217, 0.1930, 0.0018, 0.0009, 0.0152, 0.0038, 0.0190, 0.0086,\n",
            "         0.0053, 0.0033, 0.0051, 0.0095, 0.0580, 0.0012, 0.0018, 0.0071, 0.0036,\n",
            "         0.0017, 0.0043]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[16]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.9280, -0.2083, -0.6648, -0.2641,  1.7087, -1.3229, -0.1711,  0.6874,\n",
            "          1.9919,  1.2724,  0.2194, -1.2614, -1.0279,  0.4589, -0.6532,  1.1831,\n",
            "         -0.3666,  0.4472, -0.2843,  0.2079,  1.5236,  0.7154,  0.6585,  0.4589,\n",
            "         -0.5357,  1.2628, -1.5580,  0.4196, -0.3806, -0.7044, -0.3884, -1.8927,\n",
            "          0.8618,  1.2285, -0.1141,  1.7779,  0.8297,  0.3457, -0.3843, -1.5953,\n",
            "         -0.1820, -0.5678,  0.6511, -0.4463, -0.2109, -0.9229,  0.2308, -0.7586,\n",
            "         -1.6410, -1.3728, -1.3389, -0.5246,  1.9338,  0.3034,  0.2951,  1.5918,\n",
            "         -1.6319, -1.1146, -0.3941,  0.4972, -2.0602, -1.8428,  0.8573,  1.3835,\n",
            "          0.4217]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0014, 0.0079, 0.0050, 0.0075, 0.0541, 0.0026, 0.0082, 0.0195, 0.0717,\n",
            "         0.0349, 0.0122, 0.0028, 0.0035, 0.0155, 0.0051, 0.0320, 0.0068, 0.0153,\n",
            "         0.0074, 0.0121, 0.0449, 0.0200, 0.0189, 0.0155, 0.0057, 0.0346, 0.0021,\n",
            "         0.0149, 0.0067, 0.0048, 0.0066, 0.0015, 0.0232, 0.0334, 0.0087, 0.0579,\n",
            "         0.0224, 0.0138, 0.0067, 0.0020, 0.0082, 0.0055, 0.0188, 0.0063, 0.0079,\n",
            "         0.0039, 0.0123, 0.0046, 0.0019, 0.0025, 0.0026, 0.0058, 0.0677, 0.0133,\n",
            "         0.0131, 0.0481, 0.0019, 0.0032, 0.0066, 0.0161, 0.0012, 0.0016, 0.0231,\n",
            "         0.0390, 0.0149]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[55]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.2542,  0.0077, -1.5728,  0.6528, -2.0244, -1.3731,  1.8886,  2.6879,\n",
            "          0.9940, -1.9079, -0.8043, -0.3358,  0.4116,  0.5577, -0.8911, -1.0478,\n",
            "         -1.9065, -0.5476, -1.2786, -0.1582,  1.5599, -0.1496, -1.4406,  0.6488,\n",
            "         -1.3412,  0.3569, -0.9536, -1.8299, -0.2695,  0.1350,  0.7850,  0.5161,\n",
            "         -0.3392,  0.4127,  1.1458, -1.2133, -0.2370,  0.1126,  0.0860, -0.4971,\n",
            "          0.6583, -0.8797, -0.0450,  0.1431,  1.6021, -0.8150,  0.3507, -0.2239,\n",
            "         -2.4013, -0.7117, -0.3782,  0.9890, -1.2497,  0.2198,  0.9143, -0.1592,\n",
            "         -1.6053, -1.1741,  0.6289, -0.9825, -1.4075, -1.3757, -0.1028, -1.5216,\n",
            "         -0.4975]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0033, 0.0117, 0.0024, 0.0223, 0.0015, 0.0029, 0.0769, 0.1710, 0.0314,\n",
            "         0.0017, 0.0052, 0.0083, 0.0176, 0.0203, 0.0048, 0.0041, 0.0017, 0.0067,\n",
            "         0.0032, 0.0099, 0.0554, 0.0100, 0.0028, 0.0223, 0.0030, 0.0166, 0.0045,\n",
            "         0.0019, 0.0089, 0.0133, 0.0255, 0.0195, 0.0083, 0.0176, 0.0366, 0.0035,\n",
            "         0.0092, 0.0130, 0.0127, 0.0071, 0.0225, 0.0048, 0.0111, 0.0134, 0.0577,\n",
            "         0.0051, 0.0165, 0.0093, 0.0011, 0.0057, 0.0080, 0.0313, 0.0033, 0.0145,\n",
            "         0.0290, 0.0099, 0.0023, 0.0036, 0.0218, 0.0044, 0.0028, 0.0029, 0.0105,\n",
            "         0.0025, 0.0071]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[20]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975],\n",
            "         [-1.7352, -1.8096, -1.1181,  ...,  1.6543, -0.1434,  0.5471]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.7352e+00, -1.8096e+00, -1.1181e+00, -9.0906e-01, -8.8019e-01,\n",
            "         -7.6333e-01, -6.9146e-01,  1.4519e+00, -1.3416e-01,  1.7128e-01,\n",
            "         -3.1569e-01,  6.8166e-01, -2.9132e-01,  1.7912e+00, -5.2887e-04,\n",
            "          3.0944e-01, -1.2951e-01,  4.0631e-01, -7.1904e-02, -1.7640e+00,\n",
            "         -3.9474e-01, -2.1613e-01, -6.6376e-01, -8.0282e-01, -8.2567e-01,\n",
            "         -3.0989e-01,  4.3128e-01,  3.7248e-01, -3.1515e-01, -3.8546e-01,\n",
            "          5.0671e-01, -2.6067e-01, -6.3520e-01,  4.9584e-01,  1.0032e+00,\n",
            "          6.2256e-01,  2.7133e-01,  1.3517e-01,  2.6208e-01, -6.3080e-01,\n",
            "         -1.6120e+00, -5.9288e-01,  8.3394e-01,  5.7746e-01, -7.3926e-01,\n",
            "         -3.7309e-02, -8.1585e-01, -2.0201e+00, -7.8394e-01,  4.1542e-01,\n",
            "         -9.6011e-01, -1.5174e+00,  6.2092e-01,  9.9957e-01,  2.7974e-01,\n",
            "          2.2847e-01,  1.3942e-01, -1.1975e+00,  2.7833e-01, -1.1303e+00,\n",
            "         -1.3977e+00, -7.4817e-01,  1.6543e+00, -1.4340e-01,  5.4715e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0022, 0.0044, 0.0055, 0.0056, 0.0063, 0.0068, 0.0579, 0.0118,\n",
            "         0.0161, 0.0099, 0.0268, 0.0101, 0.0813, 0.0135, 0.0185, 0.0119, 0.0203,\n",
            "         0.0126, 0.0023, 0.0091, 0.0109, 0.0070, 0.0061, 0.0059, 0.0099, 0.0209,\n",
            "         0.0197, 0.0099, 0.0092, 0.0225, 0.0104, 0.0072, 0.0222, 0.0370, 0.0253,\n",
            "         0.0178, 0.0155, 0.0176, 0.0072, 0.0027, 0.0075, 0.0312, 0.0241, 0.0065,\n",
            "         0.0131, 0.0060, 0.0018, 0.0062, 0.0205, 0.0052, 0.0030, 0.0252, 0.0368,\n",
            "         0.0179, 0.0170, 0.0156, 0.0041, 0.0179, 0.0044, 0.0033, 0.0064, 0.0709,\n",
            "         0.0117, 0.0234]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[42]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975],\n",
            "         [-1.7352, -1.8096, -1.1181,  ...,  1.6543, -0.1434,  0.5471],\n",
            "         [ 1.0726,  0.7295, -0.6665,  ...,  0.3115, -1.7675,  0.6818]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.0726,  0.7295, -0.6665, -0.4071,  1.0580,  0.7095,  1.8728, -0.8542,\n",
            "          0.4089, -0.6644, -0.7699, -0.5311,  1.3702, -0.6511, -1.3560, -1.5675,\n",
            "         -0.5239,  0.6937, -0.1874, -0.4551, -0.0516, -0.4424,  0.1604, -1.1646,\n",
            "          0.6482, -1.6310, -0.5834, -1.7104,  0.6697, -1.2521, -0.3970,  0.3250,\n",
            "         -1.0091, -0.0558, -0.4422, -0.1064,  0.4973,  0.6844,  1.6967,  1.2412,\n",
            "          1.2617,  1.3472,  1.3725, -0.3011,  0.6546,  0.1393,  0.9892, -1.0955,\n",
            "         -0.7337, -0.5563,  1.1861,  1.0686, -0.8841, -1.4150, -0.2771, -0.3126,\n",
            "         -0.9772, -0.5400, -0.2355,  0.8232, -0.5872, -0.7521,  0.3115, -1.7675,\n",
            "          0.6818]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0318, 0.0226, 0.0056, 0.0073, 0.0314, 0.0221, 0.0709, 0.0046, 0.0164,\n",
            "         0.0056, 0.0050, 0.0064, 0.0429, 0.0057, 0.0028, 0.0023, 0.0065, 0.0218,\n",
            "         0.0090, 0.0069, 0.0103, 0.0070, 0.0128, 0.0034, 0.0208, 0.0021, 0.0061,\n",
            "         0.0020, 0.0213, 0.0031, 0.0073, 0.0151, 0.0040, 0.0103, 0.0070, 0.0098,\n",
            "         0.0179, 0.0216, 0.0594, 0.0377, 0.0385, 0.0419, 0.0430, 0.0081, 0.0210,\n",
            "         0.0125, 0.0293, 0.0036, 0.0052, 0.0062, 0.0357, 0.0317, 0.0045, 0.0026,\n",
            "         0.0083, 0.0080, 0.0041, 0.0063, 0.0086, 0.0248, 0.0061, 0.0051, 0.0149,\n",
            "         0.0019, 0.0215]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[46]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.7352, -1.8096, -1.1181,  ...,  1.6543, -0.1434,  0.5471],\n",
            "         [ 1.0726,  0.7295, -0.6665,  ...,  0.3115, -1.7675,  0.6818],\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.0901,  0.2170, -2.9996,  1.4690, -0.1948, -0.1507,  0.2601, -0.9647,\n",
            "          0.1162, -0.8295, -0.2266,  0.0219, -0.2785, -0.4851, -1.8023, -0.7330,\n",
            "         -1.2828,  0.8863,  1.0515, -0.9823, -1.6369, -1.3499,  0.1830,  0.0532,\n",
            "         -1.1438, -0.2829, -0.5979,  1.4757,  0.4655, -3.0346,  0.5516,  1.3107,\n",
            "          0.1240, -1.8046,  0.2700, -0.4322,  0.2784, -0.5599,  1.2502,  0.7051,\n",
            "         -1.0169,  0.4854, -1.0808, -0.3128, -0.4189, -0.5718,  0.8215,  1.7384,\n",
            "          0.5578,  0.6167,  1.5260, -0.3508, -1.5615,  0.4548, -0.8935,  0.3642,\n",
            "          0.5714,  2.7072, -1.5443,  1.1288, -1.1217, -1.7328, -0.5472, -0.8017,\n",
            "          0.7761]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0303, 0.0126, 0.0005, 0.0442, 0.0084, 0.0087, 0.0132, 0.0039, 0.0114,\n",
            "         0.0044, 0.0081, 0.0104, 0.0077, 0.0063, 0.0017, 0.0049, 0.0028, 0.0247,\n",
            "         0.0291, 0.0038, 0.0020, 0.0026, 0.0122, 0.0107, 0.0032, 0.0077, 0.0056,\n",
            "         0.0445, 0.0162, 0.0005, 0.0177, 0.0377, 0.0115, 0.0017, 0.0133, 0.0066,\n",
            "         0.0134, 0.0058, 0.0355, 0.0206, 0.0037, 0.0165, 0.0035, 0.0074, 0.0067,\n",
            "         0.0057, 0.0231, 0.0579, 0.0178, 0.0188, 0.0468, 0.0072, 0.0021, 0.0160,\n",
            "         0.0042, 0.0146, 0.0180, 0.1524, 0.0022, 0.0315, 0.0033, 0.0018, 0.0059,\n",
            "         0.0046, 0.0221]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.0726,  0.7295, -0.6665,  ...,  0.3115, -1.7675,  0.6818],\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[34]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.3402, -0.7501,  0.2942,  0.7626,  2.6536,  0.4730,  0.0314, -0.1513,\n",
            "          1.5333,  1.0515, -0.4613,  2.0802,  0.8309, -0.8416,  0.7062, -1.0877,\n",
            "          0.0698, -1.1470, -0.5624, -0.1978,  0.8101, -1.0031,  1.5461,  1.2420,\n",
            "          0.5707, -0.0135, -1.0993,  1.3919,  0.9944,  0.5453,  0.0429,  0.9108,\n",
            "          1.5571, -0.7094,  0.6623, -1.6373,  0.5799, -0.0682,  0.2936, -1.1519,\n",
            "          1.6110,  0.5486, -2.8114,  0.1064, -1.1193,  0.0351, -0.1004, -0.6131,\n",
            "          2.0785,  1.1399,  1.3345, -0.0741, -1.3989, -0.8832,  0.4431,  0.0717,\n",
            "          1.3737,  0.1206,  0.6658,  0.1704,  0.6585,  1.0239,  0.4602,  0.3275,\n",
            "          2.1779]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0034, 0.0097, 0.0155, 0.1025, 0.0116, 0.0074, 0.0062, 0.0334,\n",
            "         0.0207, 0.0046, 0.0578, 0.0166, 0.0031, 0.0146, 0.0024, 0.0077, 0.0023,\n",
            "         0.0041, 0.0059, 0.0162, 0.0026, 0.0339, 0.0250, 0.0128, 0.0071, 0.0024,\n",
            "         0.0290, 0.0195, 0.0125, 0.0075, 0.0179, 0.0342, 0.0036, 0.0140, 0.0014,\n",
            "         0.0129, 0.0067, 0.0097, 0.0023, 0.0361, 0.0125, 0.0004, 0.0080, 0.0024,\n",
            "         0.0075, 0.0065, 0.0039, 0.0577, 0.0226, 0.0274, 0.0067, 0.0018, 0.0030,\n",
            "         0.0112, 0.0078, 0.0285, 0.0081, 0.0140, 0.0086, 0.0139, 0.0201, 0.0114,\n",
            "         0.0100, 0.0637]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[4]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779],\n",
            "         [ 0.8148, -0.0643,  1.4237,  ...,  0.5427,  2.5867, -0.4687]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.8148, -0.0643,  1.4237,  0.2617, -1.8528,  0.2019, -1.1787, -0.1036,\n",
            "         -1.7830, -0.8323, -0.4346, -1.2480, -0.2880,  0.8809, -0.7190,  0.1745,\n",
            "          0.7520, -0.0629, -0.7111,  0.9810, -0.7244, -1.5010, -2.8348, -2.8272,\n",
            "         -0.1736,  0.0512, -0.6576, -2.5729,  0.0210,  1.0060, -1.2492,  0.2441,\n",
            "         -0.6387, -0.3186, -1.2942, -1.0726,  0.2290, -0.9001,  0.6614,  0.5118,\n",
            "          0.6762, -1.3639,  0.5486,  0.0895,  0.3575, -1.6521, -0.7584,  0.0695,\n",
            "          0.9937, -0.2821,  1.1088, -1.9881, -1.3916,  1.2734, -1.1732,  0.5820,\n",
            "         -1.3185,  0.7859, -1.1501,  1.3132,  2.2007, -0.2195,  0.5427,  2.5867,\n",
            "         -0.4687]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0245, 0.0102, 0.0450, 0.0141, 0.0017, 0.0133, 0.0033, 0.0098, 0.0018,\n",
            "         0.0047, 0.0070, 0.0031, 0.0081, 0.0261, 0.0053, 0.0129, 0.0230, 0.0102,\n",
            "         0.0053, 0.0289, 0.0052, 0.0024, 0.0006, 0.0006, 0.0091, 0.0114, 0.0056,\n",
            "         0.0008, 0.0111, 0.0296, 0.0031, 0.0138, 0.0057, 0.0079, 0.0030, 0.0037,\n",
            "         0.0136, 0.0044, 0.0210, 0.0181, 0.0213, 0.0028, 0.0187, 0.0118, 0.0155,\n",
            "         0.0021, 0.0051, 0.0116, 0.0293, 0.0082, 0.0328, 0.0015, 0.0027, 0.0387,\n",
            "         0.0034, 0.0194, 0.0029, 0.0238, 0.0034, 0.0403, 0.0978, 0.0087, 0.0186,\n",
            "         0.1439, 0.0068]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[60]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.3402, -0.7501,  0.2942,  ...,  0.4602,  0.3275,  2.1779],\n",
            "         [ 0.8148, -0.0643,  1.4237,  ...,  0.5427,  2.5867, -0.4687],\n",
            "         [-0.1679,  0.5602,  0.6467,  ...,  0.1522,  0.5109,  0.0990]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1679,  0.5602,  0.6467,  0.6577,  1.0027,  1.9283, -0.6259, -0.0545,\n",
            "          0.6774, -0.1059, -0.3820, -0.5626, -0.2442, -0.5031, -0.2845,  0.5390,\n",
            "         -0.2896,  0.5122, -0.6999, -0.2165,  0.1656, -1.5764, -0.4159, -0.1375,\n",
            "          0.1683,  0.1388,  0.4582,  0.3286, -0.5309, -0.8250, -0.7942,  0.9514,\n",
            "          0.2845,  0.9817,  1.3525,  1.2716, -0.0270, -0.1488,  0.5580, -2.3034,\n",
            "          1.0189, -0.0113, -1.0838, -0.1644,  0.4633, -1.2487,  0.4323,  2.2017,\n",
            "          0.8862,  0.7818, -1.3255,  1.2300,  0.4367, -0.8123,  0.1588,  0.1032,\n",
            "          1.2126, -1.1706, -1.1219, -1.0335,  2.1941, -0.9940,  0.1522,  0.5109,\n",
            "          0.0990]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0081, 0.0168, 0.0183, 0.0185, 0.0262, 0.0660, 0.0051, 0.0091, 0.0189,\n",
            "         0.0086, 0.0065, 0.0055, 0.0075, 0.0058, 0.0072, 0.0164, 0.0072, 0.0160,\n",
            "         0.0048, 0.0077, 0.0113, 0.0020, 0.0063, 0.0084, 0.0114, 0.0110, 0.0152,\n",
            "         0.0133, 0.0056, 0.0042, 0.0043, 0.0248, 0.0128, 0.0256, 0.0371, 0.0342,\n",
            "         0.0093, 0.0083, 0.0168, 0.0010, 0.0266, 0.0095, 0.0032, 0.0081, 0.0152,\n",
            "         0.0028, 0.0148, 0.0867, 0.0233, 0.0210, 0.0025, 0.0328, 0.0148, 0.0043,\n",
            "         0.0112, 0.0106, 0.0323, 0.0030, 0.0031, 0.0034, 0.0861, 0.0036, 0.0112,\n",
            "         0.0160, 0.0106]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.8148, -0.0643,  1.4237,  ...,  0.5427,  2.5867, -0.4687],\n",
            "         [-0.1679,  0.5602,  0.6467,  ...,  0.1522,  0.5109,  0.0990],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1679,  0.5602,  0.6467,  ...,  0.1522,  0.5109,  0.0990],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[62]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4222, -1.8111, -1.0118, -0.4569,  0.0202, -0.1555, -0.1721,  0.2664,\n",
            "         -0.2054, -1.3252,  0.6271,  1.4733,  0.9470, -1.0751,  0.9042, -1.4850,\n",
            "         -0.3449,  1.3128, -0.5799, -0.6507, -0.1817,  0.9045,  1.2905,  0.6977,\n",
            "         -1.1612,  0.0093, -1.0707,  0.1294, -0.4570,  0.9330, -0.2858, -0.9957,\n",
            "          0.5773, -0.5476, -2.0319, -0.1269, -0.5162,  0.2046,  1.4801,  1.6253,\n",
            "         -0.8076,  0.8477, -1.0219, -1.9241,  1.4480, -1.8130, -0.0638, -0.1206,\n",
            "          1.5924, -0.1682,  1.7089, -0.1853,  0.1268, -1.2130, -0.4556,  1.1074,\n",
            "          1.4224, -1.8213, -0.0451, -0.1755, -0.9631, -0.5043,  0.5462,  0.2788,\n",
            "          0.7280]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0161, 0.0017, 0.0038, 0.0067, 0.0108, 0.0090, 0.0089, 0.0138, 0.0086,\n",
            "         0.0028, 0.0197, 0.0460, 0.0272, 0.0036, 0.0261, 0.0024, 0.0075, 0.0392,\n",
            "         0.0059, 0.0055, 0.0088, 0.0261, 0.0383, 0.0212, 0.0033, 0.0106, 0.0036,\n",
            "         0.0120, 0.0067, 0.0268, 0.0079, 0.0039, 0.0188, 0.0061, 0.0014, 0.0093,\n",
            "         0.0063, 0.0129, 0.0463, 0.0536, 0.0047, 0.0246, 0.0038, 0.0015, 0.0449,\n",
            "         0.0017, 0.0099, 0.0093, 0.0518, 0.0089, 0.0583, 0.0088, 0.0120, 0.0031,\n",
            "         0.0067, 0.0319, 0.0437, 0.0017, 0.0101, 0.0088, 0.0040, 0.0064, 0.0182,\n",
            "         0.0139, 0.0218]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[39]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1513,  1.0539,  3.4105, -0.9621, -1.1720,  0.5953, -0.4098,  1.4256,\n",
            "         -1.2171, -1.6845,  0.5385,  1.8967, -0.2745,  0.2787, -0.6473, -2.6276,\n",
            "         -1.3731, -1.2415,  0.7076, -0.4946,  1.1809,  0.5424, -0.8578,  0.5198,\n",
            "          0.1509, -0.0399,  1.0038, -1.1435,  1.8040, -0.0290, -0.8131,  0.9093,\n",
            "         -1.1375,  0.5140, -0.4895, -0.0806,  0.9151, -0.5481,  1.1071, -0.3505,\n",
            "          0.6674, -0.0894,  0.2723,  0.6034,  0.2319,  1.5473, -0.6886, -0.4414,\n",
            "          1.2790, -0.9959, -0.4363, -0.8700, -0.0538,  1.1496,  1.0411,  0.0580,\n",
            "         -1.6868,  0.4005,  1.0880, -0.4828, -0.0709,  1.0966, -0.5686,  0.9079,\n",
            "         -0.1701]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0243, 0.0220, 0.2327, 0.0029, 0.0024, 0.0139, 0.0051, 0.0320, 0.0023,\n",
            "         0.0014, 0.0132, 0.0512, 0.0058, 0.0102, 0.0040, 0.0006, 0.0019, 0.0022,\n",
            "         0.0156, 0.0047, 0.0250, 0.0132, 0.0033, 0.0129, 0.0089, 0.0074, 0.0210,\n",
            "         0.0024, 0.0467, 0.0075, 0.0034, 0.0191, 0.0025, 0.0128, 0.0047, 0.0071,\n",
            "         0.0192, 0.0044, 0.0232, 0.0054, 0.0150, 0.0070, 0.0101, 0.0141, 0.0097,\n",
            "         0.0361, 0.0039, 0.0049, 0.0276, 0.0028, 0.0050, 0.0032, 0.0073, 0.0243,\n",
            "         0.0218, 0.0081, 0.0014, 0.0115, 0.0228, 0.0047, 0.0072, 0.0230, 0.0044,\n",
            "         0.0191, 0.0065]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[58]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.2475, -0.6349, -1.2909,  1.1822,  0.1479, -0.4333, -0.8269,  0.0728,\n",
            "         -1.2982,  0.3960, -1.2460,  0.1458, -0.5699, -1.3561, -0.3812, -0.8515,\n",
            "          1.1918, -0.8108, -0.1733, -0.4703, -0.6000, -1.3636, -1.0889,  1.0108,\n",
            "          0.8543, -0.0441,  1.8017,  0.6014, -2.5448, -0.4865,  2.6412,  1.6053,\n",
            "          0.5901,  0.8137, -0.1124, -0.3050,  1.1426,  0.6637, -0.7000,  0.9262,\n",
            "         -1.1032, -1.2125,  0.6065,  0.5882, -0.5453,  0.7654,  0.5692,  0.8859,\n",
            "         -0.0700,  0.6792, -0.0283, -1.2243, -1.7192,  1.4801,  0.9587, -0.0338,\n",
            "          0.5083, -0.2502,  2.0734, -0.2994,  0.0473, -0.9626,  1.3064, -0.2256,\n",
            "         -1.8305]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0118, 0.0049, 0.0025, 0.0301, 0.0107, 0.0060, 0.0040, 0.0099, 0.0025,\n",
            "         0.0137, 0.0027, 0.0107, 0.0052, 0.0024, 0.0063, 0.0039, 0.0304, 0.0041,\n",
            "         0.0078, 0.0058, 0.0051, 0.0024, 0.0031, 0.0254, 0.0217, 0.0088, 0.0560,\n",
            "         0.0168, 0.0007, 0.0057, 0.1295, 0.0460, 0.0167, 0.0208, 0.0083, 0.0068,\n",
            "         0.0289, 0.0179, 0.0046, 0.0233, 0.0031, 0.0027, 0.0169, 0.0166, 0.0054,\n",
            "         0.0199, 0.0163, 0.0224, 0.0086, 0.0182, 0.0090, 0.0027, 0.0017, 0.0406,\n",
            "         0.0241, 0.0089, 0.0154, 0.0072, 0.0734, 0.0068, 0.0097, 0.0035, 0.0341,\n",
            "         0.0074, 0.0015]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[48]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.5377, -0.2542, -1.8516,  1.3382, -1.0219, -0.9356,  0.8454, -1.3798,\n",
            "         -0.1421,  0.7084, -0.2751,  1.2128,  1.3650, -1.3301, -1.4832, -0.9809,\n",
            "         -1.5012, -1.7006,  1.2642, -1.1078,  0.5398, -0.7718,  0.6175,  2.1793,\n",
            "         -0.1047, -0.7940,  1.1206, -0.9039,  0.4935,  0.5804,  1.2005, -1.5786,\n",
            "         -0.5037, -0.7478, -1.3617, -0.3347,  0.7188, -1.7258, -0.0902, -0.0148,\n",
            "          0.9630, -1.7663, -0.3839, -0.0170, -1.0878,  0.6621, -0.8364, -0.8048,\n",
            "          0.0204, -0.5749, -0.4291,  1.1989,  0.0905, -0.4063,  0.1165, -1.2079,\n",
            "         -1.1770,  0.6919, -1.3267,  2.5186, -1.0516, -1.0338,  2.1974,  0.3053,\n",
            "         -1.1890]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0172, 0.0078, 0.0016, 0.0384, 0.0036, 0.0039, 0.0234, 0.0025, 0.0087,\n",
            "         0.0204, 0.0076, 0.0338, 0.0394, 0.0027, 0.0023, 0.0038, 0.0022, 0.0018,\n",
            "         0.0356, 0.0033, 0.0173, 0.0047, 0.0187, 0.0889, 0.0091, 0.0045, 0.0309,\n",
            "         0.0041, 0.0165, 0.0180, 0.0334, 0.0021, 0.0061, 0.0048, 0.0026, 0.0072,\n",
            "         0.0206, 0.0018, 0.0092, 0.0099, 0.0264, 0.0017, 0.0069, 0.0099, 0.0034,\n",
            "         0.0195, 0.0044, 0.0045, 0.0103, 0.0057, 0.0066, 0.0334, 0.0110, 0.0067,\n",
            "         0.0113, 0.0030, 0.0031, 0.0201, 0.0027, 0.1249, 0.0035, 0.0036, 0.0906,\n",
            "         0.0137, 0.0031]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[41]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [-1.1895, -0.8407,  0.3134,  ...,  0.6707, -0.5969,  0.7567]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.1895, -0.8407,  0.3134, -0.6709, -0.8176,  0.6929, -0.6374,  0.3174,\n",
            "          0.4837, -0.0073, -1.5924,  1.8606, -1.2910, -0.1594,  0.3111, -0.1536,\n",
            "         -0.3414, -0.0170, -0.1633,  0.2794,  0.6755,  0.7066, -1.6665, -1.0184,\n",
            "          2.2253,  0.5137,  0.0381,  1.9234,  1.8400, -0.3183,  0.1240, -0.5258,\n",
            "         -0.9057,  1.3275, -0.6232,  0.1452, -1.3049, -0.2188, -1.8588,  0.0626,\n",
            "          0.1966, -2.2531,  0.4659, -0.3311,  1.0631, -0.4966, -1.2999, -0.1232,\n",
            "         -0.2542,  1.1509,  0.8772,  1.4667,  0.4530,  0.8588,  0.0272,  0.4963,\n",
            "         -0.2183,  0.4316,  0.5556,  0.1456, -0.9434, -0.5827,  0.6707, -0.5969,\n",
            "          0.7567]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0030, 0.0043, 0.0137, 0.0051, 0.0044, 0.0200, 0.0053, 0.0137, 0.0162,\n",
            "         0.0099, 0.0020, 0.0642, 0.0027, 0.0085, 0.0136, 0.0086, 0.0071, 0.0098,\n",
            "         0.0085, 0.0132, 0.0196, 0.0203, 0.0019, 0.0036, 0.0925, 0.0167, 0.0104,\n",
            "         0.0684, 0.0629, 0.0073, 0.0113, 0.0059, 0.0040, 0.0377, 0.0054, 0.0116,\n",
            "         0.0027, 0.0080, 0.0016, 0.0106, 0.0122, 0.0010, 0.0159, 0.0072, 0.0289,\n",
            "         0.0061, 0.0027, 0.0088, 0.0077, 0.0316, 0.0240, 0.0433, 0.0157, 0.0236,\n",
            "         0.0103, 0.0164, 0.0080, 0.0154, 0.0174, 0.0116, 0.0039, 0.0056, 0.0195,\n",
            "         0.0055, 0.0213]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[25]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [-1.1895, -0.8407,  0.3134,  ...,  0.6707, -0.5969,  0.7567],\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.0691,  0.2990, -1.4717,  0.9950,  0.3608,  0.3161,  0.3504, -1.7823,\n",
            "          0.1339, -2.0973,  1.9108,  1.6555,  2.0254,  0.6044, -0.7006,  0.8141,\n",
            "          0.2263, -0.8224, -1.1513,  0.1186, -0.3123, -0.6024, -0.1058, -0.5325,\n",
            "          0.1415, -0.0339, -0.6461,  0.5560, -0.0698, -0.7516, -1.7028, -0.6811,\n",
            "         -1.2044, -0.2007,  1.3154, -0.4974, -0.2338, -0.9047,  0.4135, -0.6663,\n",
            "          1.2759,  0.3141, -1.1177, -1.1179, -0.4851,  1.4299, -0.2522, -1.0614,\n",
            "          1.2459,  2.1203,  1.7902,  0.8941,  0.0293, -0.2685,  1.7898,  0.6121,\n",
            "         -0.3263,  0.5659, -1.1200,  0.3157, -1.4606, -0.6966,  0.1517,  0.8528,\n",
            "          0.0604]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0100, 0.0126, 0.0021, 0.0253, 0.0134, 0.0128, 0.0133, 0.0016, 0.0107,\n",
            "         0.0011, 0.0631, 0.0489, 0.0708, 0.0171, 0.0046, 0.0211, 0.0117, 0.0041,\n",
            "         0.0030, 0.0105, 0.0068, 0.0051, 0.0084, 0.0055, 0.0108, 0.0090, 0.0049,\n",
            "         0.0163, 0.0087, 0.0044, 0.0017, 0.0047, 0.0028, 0.0076, 0.0348, 0.0057,\n",
            "         0.0074, 0.0038, 0.0141, 0.0048, 0.0334, 0.0128, 0.0031, 0.0031, 0.0057,\n",
            "         0.0390, 0.0073, 0.0032, 0.0325, 0.0778, 0.0559, 0.0228, 0.0096, 0.0071,\n",
            "         0.0559, 0.0172, 0.0067, 0.0164, 0.0030, 0.0128, 0.0022, 0.0047, 0.0109,\n",
            "         0.0219, 0.0099]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[54]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.1895, -0.8407,  0.3134,  ...,  0.6707, -0.5969,  0.7567],\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
            "         [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.6787,  0.8662, -1.6433,  1.9448, -0.1631, -0.1714,  0.9309, -0.8550,\n",
            "          0.7524,  0.2626, -1.3360, -0.9757, -0.5134,  0.7116, -0.0196,  0.4353,\n",
            "          0.8279,  0.5487, -0.4363,  0.5519,  0.6782,  0.2535,  1.6661, -0.6964,\n",
            "         -0.3170,  0.8487,  0.4341, -0.2240, -1.0157, -0.1161,  0.2876,  0.4203,\n",
            "          0.3379,  0.8016, -0.1731,  0.5809,  0.1622,  1.3970,  0.3307,  0.2771,\n",
            "          0.7410,  0.2727,  0.2346, -0.1117, -0.5691,  1.1829,  1.9441, -0.4155,\n",
            "         -0.3620, -0.1765, -0.9348,  1.5461, -0.8577, -0.1006,  0.3560, -1.6589,\n",
            "          0.6543, -1.3299,  1.1929,  0.4855, -0.5721,  1.0813,  2.3671, -0.7775,\n",
            "         -0.2586]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0045, 0.0212, 0.0017, 0.0623, 0.0076, 0.0075, 0.0226, 0.0038, 0.0189,\n",
            "         0.0116, 0.0023, 0.0034, 0.0053, 0.0182, 0.0087, 0.0138, 0.0204, 0.0154,\n",
            "         0.0058, 0.0155, 0.0176, 0.0115, 0.0471, 0.0044, 0.0065, 0.0208, 0.0138,\n",
            "         0.0071, 0.0032, 0.0079, 0.0119, 0.0136, 0.0125, 0.0199, 0.0075, 0.0159,\n",
            "         0.0105, 0.0360, 0.0124, 0.0118, 0.0187, 0.0117, 0.0113, 0.0080, 0.0050,\n",
            "         0.0291, 0.0623, 0.0059, 0.0062, 0.0075, 0.0035, 0.0418, 0.0038, 0.0081,\n",
            "         0.0127, 0.0017, 0.0171, 0.0024, 0.0294, 0.0145, 0.0050, 0.0263, 0.0950,\n",
            "         0.0041, 0.0069]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[61]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
            "         [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586],\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4897,  0.0655,  1.0370,  1.0680,  0.1121,  1.1379,  0.1987, -0.6042,\n",
            "          0.1626,  0.8063,  0.7040,  0.2668,  0.0244,  0.1020, -0.3747, -1.2485,\n",
            "         -1.6522,  0.4945,  0.2456, -0.2416, -0.4212,  0.3810,  0.2634,  0.6288,\n",
            "          0.1788,  0.1912,  1.6184,  0.9607, -0.2439,  0.3508, -0.4618, -1.0462,\n",
            "         -1.3135,  0.0185, -0.5131,  1.5241,  0.5632, -1.1132,  0.0729,  0.4962,\n",
            "          0.8535,  0.2138, -1.4130, -0.6337,  1.9594, -1.0523, -0.5277, -0.7341,\n",
            "         -1.9469, -0.0727, -0.3491, -1.3596, -0.3679,  0.7067, -0.1839, -1.0535,\n",
            "         -1.1778, -0.3159, -0.3419, -1.1736, -0.6405,  0.0713,  0.4397, -0.7343,\n",
            "         -0.1916]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0196, 0.0128, 0.0338, 0.0349, 0.0134, 0.0374, 0.0146, 0.0066, 0.0141,\n",
            "         0.0269, 0.0243, 0.0157, 0.0123, 0.0133, 0.0082, 0.0034, 0.0023, 0.0197,\n",
            "         0.0153, 0.0094, 0.0079, 0.0176, 0.0156, 0.0225, 0.0143, 0.0145, 0.0605,\n",
            "         0.0314, 0.0094, 0.0170, 0.0076, 0.0042, 0.0032, 0.0122, 0.0072, 0.0551,\n",
            "         0.0211, 0.0039, 0.0129, 0.0197, 0.0282, 0.0149, 0.0029, 0.0064, 0.0851,\n",
            "         0.0042, 0.0071, 0.0058, 0.0017, 0.0112, 0.0085, 0.0031, 0.0083, 0.0243,\n",
            "         0.0100, 0.0042, 0.0037, 0.0087, 0.0085, 0.0037, 0.0063, 0.0129, 0.0186,\n",
            "         0.0058, 0.0099]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[24]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586],\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
            "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
            "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
            "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
            "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
            "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
            "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
            "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
            "          0.7624]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
            "         0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
            "         0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
            "         0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
            "         0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
            "         0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
            "         0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
            "         0.0048, 0.0201]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[17]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916],\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.4892, -2.5589,  1.4134,  0.3562, -0.9866, -0.9082, -1.6532,  0.8884,\n",
            "         -0.4134,  1.3855, -0.3127,  0.7171,  0.9241, -0.5131, -0.7612,  0.6070,\n",
            "          0.2605, -0.7458, -1.6201,  1.1018,  2.0153, -0.4154,  1.6371,  0.3695,\n",
            "         -0.8532, -0.4395,  0.1850, -0.8069,  0.1648,  1.6064,  2.0446, -0.8769,\n",
            "          0.3839, -0.7857, -0.3680, -0.6815,  1.6723,  0.5269,  0.8404,  1.2201,\n",
            "          0.4875,  0.2428, -1.2750,  0.4207, -1.2408, -2.2630, -0.2638, -0.2505,\n",
            "         -0.3959, -0.9208,  0.2549, -0.0755,  0.4162,  0.5739, -1.0490,  0.7784,\n",
            "          2.0459,  1.6576,  0.6625, -0.5261, -0.0290,  1.0471, -1.4296,  0.2347,\n",
            "         -1.2034]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0055, 0.0007, 0.0367, 0.0128, 0.0033, 0.0036, 0.0017, 0.0217, 0.0059,\n",
            "         0.0357, 0.0065, 0.0183, 0.0225, 0.0053, 0.0042, 0.0164, 0.0116, 0.0042,\n",
            "         0.0018, 0.0269, 0.0670, 0.0059, 0.0459, 0.0129, 0.0038, 0.0058, 0.0107,\n",
            "         0.0040, 0.0105, 0.0445, 0.0690, 0.0037, 0.0131, 0.0041, 0.0062, 0.0045,\n",
            "         0.0475, 0.0151, 0.0207, 0.0303, 0.0145, 0.0114, 0.0025, 0.0136, 0.0026,\n",
            "         0.0009, 0.0069, 0.0070, 0.0060, 0.0036, 0.0115, 0.0083, 0.0135, 0.0159,\n",
            "         0.0031, 0.0195, 0.0691, 0.0469, 0.0173, 0.0053, 0.0087, 0.0254, 0.0021,\n",
            "         0.0113, 0.0027]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[30]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1919, -0.1515, -0.6027, -0.7786, -0.8146, -0.0140, -0.2557, -1.1456,\n",
            "         -0.0650, -0.8981,  1.1770,  0.9207,  0.4289, -0.9033, -1.8009,  0.7865,\n",
            "         -1.8513, -0.2894,  1.1134,  0.4341, -2.0746,  0.1995,  1.2089, -0.7427,\n",
            "          0.1015, -0.7365, -1.6778, -0.6234, -0.9012, -0.7467, -1.0201,  2.0264,\n",
            "          0.7080, -0.0137, -0.6840, -0.4390, -0.7137, -1.7636, -0.7221, -0.9141,\n",
            "         -0.8221, -1.6358, -0.4992, -0.8292, -0.7503, -0.9921, -0.2697,  0.1637,\n",
            "          2.1167,  0.1598,  1.1173,  0.7453, -0.6453, -0.2113,  0.8426,  0.6110,\n",
            "          0.6414, -1.5403,  0.0589, -1.0928,  0.5961,  1.0248, -1.3679,  1.4416,\n",
            "         -1.8638]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0392, 0.0102, 0.0065, 0.0055, 0.0053, 0.0117, 0.0092, 0.0038, 0.0112,\n",
            "         0.0049, 0.0386, 0.0299, 0.0183, 0.0048, 0.0020, 0.0262, 0.0019, 0.0089,\n",
            "         0.0363, 0.0184, 0.0015, 0.0145, 0.0399, 0.0057, 0.0132, 0.0057, 0.0022,\n",
            "         0.0064, 0.0048, 0.0056, 0.0043, 0.0904, 0.0242, 0.0117, 0.0060, 0.0077,\n",
            "         0.0058, 0.0020, 0.0058, 0.0048, 0.0052, 0.0023, 0.0072, 0.0052, 0.0056,\n",
            "         0.0044, 0.0091, 0.0140, 0.0989, 0.0140, 0.0364, 0.0251, 0.0062, 0.0096,\n",
            "         0.0277, 0.0219, 0.0226, 0.0026, 0.0126, 0.0040, 0.0216, 0.0332, 0.0030,\n",
            "         0.0504, 0.0018]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[31]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,\n",
            "          1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726,\n",
            "         -0.4725, -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664,\n",
            "         -0.9238, -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422,\n",
            "         -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,\n",
            "          1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,\n",
            "          1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
            "          0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,\n",
            "          0.0943]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0027, 0.0042, 0.0052, 0.0105, 0.0153, 0.0028, 0.0028, 0.0026, 0.0292,\n",
            "         0.0140, 0.0026, 0.0021, 0.0034, 0.0204, 0.0384, 0.0072, 0.0048, 0.0024,\n",
            "         0.0140, 0.0285, 0.0037, 0.0022, 0.0152, 0.0370, 0.0031, 0.0070, 0.0016,\n",
            "         0.0065, 0.1867, 0.0068, 0.0085, 0.0133, 0.0042, 0.0130, 0.0655, 0.0019,\n",
            "         0.0033, 0.0213, 0.0148, 0.0099, 0.0272, 0.0073, 0.0029, 0.0047, 0.0060,\n",
            "         0.0051, 0.0036, 0.0231, 0.0357, 0.0022, 0.0222, 0.0129, 0.0114, 0.0032,\n",
            "         0.0297, 0.0818, 0.0086, 0.0022, 0.0171, 0.0036, 0.0067, 0.0042, 0.0212,\n",
            "         0.0104, 0.0085]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[28]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1919, -0.1515, -0.6027,  ..., -1.3679,  1.4416, -1.8638],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [ 0.3140,  2.2434,  1.6029,  ...,  0.1500,  2.4338, -0.5937]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.3140,  2.2434,  1.6029, -0.7250,  0.2998, -0.8843,  1.5462, -0.7646,\n",
            "         -0.2466, -0.6231, -0.0477, -2.0922,  0.0285,  0.1288, -0.7638,  0.0996,\n",
            "          0.2829, -1.2844, -0.0274,  0.1114,  0.9420,  0.0435,  1.2219, -0.7706,\n",
            "          0.6853, -0.6485,  0.6499, -0.0729, -0.3752, -1.3073, -0.3637, -0.2029,\n",
            "          0.8741,  0.1697, -2.4297, -0.1310, -1.7969, -0.5603,  0.9169,  0.0423,\n",
            "         -0.0205,  0.2081,  0.7121, -0.7543, -1.6761,  0.6430,  0.0780,  0.6150,\n",
            "         -0.2703, -0.2620,  1.6763,  0.9997, -0.1362, -1.2802, -1.4307, -1.2849,\n",
            "         -0.8925,  0.5512, -0.5527, -1.3258, -0.1855, -0.3131,  0.1500,  2.4338,\n",
            "         -0.5937]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0143, 0.0984, 0.0519, 0.0051, 0.0141, 0.0043, 0.0490, 0.0049, 0.0082,\n",
            "         0.0056, 0.0100, 0.0013, 0.0107, 0.0119, 0.0049, 0.0115, 0.0139, 0.0029,\n",
            "         0.0102, 0.0117, 0.0268, 0.0109, 0.0354, 0.0048, 0.0207, 0.0055, 0.0200,\n",
            "         0.0097, 0.0072, 0.0028, 0.0073, 0.0085, 0.0250, 0.0124, 0.0009, 0.0092,\n",
            "         0.0017, 0.0060, 0.0261, 0.0109, 0.0102, 0.0129, 0.0213, 0.0049, 0.0020,\n",
            "         0.0199, 0.0113, 0.0193, 0.0080, 0.0080, 0.0558, 0.0284, 0.0091, 0.0029,\n",
            "         0.0025, 0.0029, 0.0043, 0.0181, 0.0060, 0.0028, 0.0087, 0.0076, 0.0121,\n",
            "         0.1191, 0.0058]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[63]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [ 0.3140,  2.2434,  1.6029,  ...,  0.1500,  2.4338, -0.5937],\n",
            "         [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.8109,  0.2410, -0.1139,  0.0756, -0.0163,  0.3881,  0.5838, -0.6104,\n",
            "         -0.1469, -0.5621, -0.7861,  1.0935, -0.0676,  0.4039, -1.9338,  0.5682,\n",
            "         -0.7478, -0.6925,  1.8697, -0.3118, -0.0237,  1.3093,  0.3176, -0.0404,\n",
            "         -1.3968, -0.7704,  1.2933,  0.2997, -0.7422,  0.7763, -0.9370,  0.1350,\n",
            "         -0.3210, -0.8011,  0.0812, -1.1384, -0.4476,  0.2270,  1.5843,  1.1594,\n",
            "          0.9263,  0.3165, -0.5596,  1.1860,  0.0711, -1.8267,  1.9844,  0.7043,\n",
            "          0.2018,  0.0149,  1.3187,  0.7898,  0.4566, -2.8259, -1.3648, -0.7705,\n",
            "         -1.9718,  0.3732,  0.9726,  0.7603, -0.7497,  0.0475,  1.4509,  0.1836,\n",
            "          0.3064]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0045, 0.0129, 0.0090, 0.0109, 0.0100, 0.0149, 0.0181, 0.0055, 0.0087,\n",
            "         0.0058, 0.0046, 0.0302, 0.0095, 0.0151, 0.0015, 0.0179, 0.0048, 0.0051,\n",
            "         0.0656, 0.0074, 0.0099, 0.0375, 0.0139, 0.0097, 0.0025, 0.0047, 0.0369,\n",
            "         0.0136, 0.0048, 0.0220, 0.0040, 0.0116, 0.0073, 0.0045, 0.0110, 0.0032,\n",
            "         0.0065, 0.0127, 0.0493, 0.0322, 0.0255, 0.0139, 0.0058, 0.0331, 0.0109,\n",
            "         0.0016, 0.0736, 0.0205, 0.0124, 0.0103, 0.0378, 0.0223, 0.0160, 0.0006,\n",
            "         0.0026, 0.0047, 0.0014, 0.0147, 0.0268, 0.0216, 0.0048, 0.0106, 0.0432,\n",
            "         0.0122, 0.0137]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[39]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.3140,  2.2434,  1.6029,  ...,  0.1500,  2.4338, -0.5937],\n",
            "         [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1513,  1.0539,  3.4105, -0.9621, -1.1720,  0.5953, -0.4098,  1.4256,\n",
            "         -1.2171, -1.6845,  0.5385,  1.8967, -0.2745,  0.2787, -0.6473, -2.6276,\n",
            "         -1.3731, -1.2415,  0.7076, -0.4946,  1.1809,  0.5424, -0.8578,  0.5198,\n",
            "          0.1509, -0.0399,  1.0038, -1.1435,  1.8040, -0.0290, -0.8131,  0.9093,\n",
            "         -1.1375,  0.5140, -0.4895, -0.0806,  0.9151, -0.5481,  1.1071, -0.3505,\n",
            "          0.6674, -0.0894,  0.2723,  0.6034,  0.2319,  1.5473, -0.6886, -0.4414,\n",
            "          1.2790, -0.9959, -0.4363, -0.8700, -0.0538,  1.1496,  1.0411,  0.0580,\n",
            "         -1.6868,  0.4005,  1.0880, -0.4828, -0.0709,  1.0966, -0.5686,  0.9079,\n",
            "         -0.1701]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0243, 0.0220, 0.2327, 0.0029, 0.0024, 0.0139, 0.0051, 0.0320, 0.0023,\n",
            "         0.0014, 0.0132, 0.0512, 0.0058, 0.0102, 0.0040, 0.0006, 0.0019, 0.0022,\n",
            "         0.0156, 0.0047, 0.0250, 0.0132, 0.0033, 0.0129, 0.0089, 0.0074, 0.0210,\n",
            "         0.0024, 0.0467, 0.0075, 0.0034, 0.0191, 0.0025, 0.0128, 0.0047, 0.0071,\n",
            "         0.0192, 0.0044, 0.0232, 0.0054, 0.0150, 0.0070, 0.0101, 0.0141, 0.0097,\n",
            "         0.0361, 0.0039, 0.0049, 0.0276, 0.0028, 0.0050, 0.0032, 0.0073, 0.0243,\n",
            "         0.0218, 0.0081, 0.0014, 0.0115, 0.0228, 0.0047, 0.0072, 0.0230, 0.0044,\n",
            "         0.0191, 0.0065]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[53]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1324, -0.5489,  0.1024, -0.6916,  0.3507,  1.6147,  1.8203,  0.5122,\n",
            "          1.5810, -2.0063, -1.2925,  0.1268,  1.1099, -0.6592,  0.8084,  1.9072,\n",
            "         -0.3260, -0.3438, -1.4415, -0.1828, -0.8804, -0.6192, -1.4047, -0.8584,\n",
            "         -0.3830, -0.5372, -1.2176, -1.9403, -0.3094,  0.1790,  1.2859,  0.3039,\n",
            "          1.8110,  0.6350, -0.0820, -2.1208,  1.2516, -0.6826,  0.3838,  0.0150,\n",
            "         -0.2801,  1.4896, -0.4646, -1.9210, -0.1062,  1.0614,  0.9308,  3.1170,\n",
            "         -1.5428, -2.2848,  0.5755, -0.8040,  0.8010,  0.0088, -0.4751, -0.9630,\n",
            "         -0.5078,  0.1018,  1.9141, -1.9252, -1.5554, -0.1878, -0.8599, -1.6050,\n",
            "         -0.6985]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0075, 0.0049, 0.0095, 0.0043, 0.0121, 0.0430, 0.0528, 0.0143, 0.0415,\n",
            "         0.0011, 0.0023, 0.0097, 0.0259, 0.0044, 0.0192, 0.0576, 0.0062, 0.0061,\n",
            "         0.0020, 0.0071, 0.0035, 0.0046, 0.0021, 0.0036, 0.0058, 0.0050, 0.0025,\n",
            "         0.0012, 0.0063, 0.0102, 0.0309, 0.0116, 0.0523, 0.0161, 0.0079, 0.0010,\n",
            "         0.0299, 0.0043, 0.0125, 0.0087, 0.0065, 0.0379, 0.0054, 0.0013, 0.0077,\n",
            "         0.0247, 0.0217, 0.1930, 0.0018, 0.0009, 0.0152, 0.0038, 0.0190, 0.0086,\n",
            "         0.0053, 0.0033, 0.0051, 0.0095, 0.0580, 0.0012, 0.0018, 0.0071, 0.0036,\n",
            "         0.0017, 0.0043]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[8]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701],\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1407,  0.8935, -2.4000, -1.3420, -1.0023, -1.9870, -0.1217, -0.8074,\n",
            "          0.9255, -1.4814,  0.1291, -1.7058,  2.0330, -0.5062, -0.1016, -0.3016,\n",
            "          1.2501, -0.2137, -1.1994, -0.8466, -0.3351,  1.0041,  0.8656,  0.1688,\n",
            "         -0.2352, -0.2586,  0.0131,  0.8719,  0.9102, -0.1875,  0.7229,  1.4742,\n",
            "         -0.4048, -1.2273,  0.3382,  0.3641, -1.4508, -0.3814,  0.7220,  0.3461,\n",
            "          0.9967,  0.3575,  0.1187, -0.1062,  0.2990,  0.1199, -1.2433,  1.7859,\n",
            "          0.9191,  0.2326,  0.4591,  0.2556, -0.3542,  0.6690,  0.7535, -0.5359,\n",
            "         -1.0277,  0.5347, -0.7958,  0.4780,  0.1588, -2.3106,  0.3227,  1.5431,\n",
            "         -1.0392]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0328, 0.0256, 0.0010, 0.0027, 0.0038, 0.0014, 0.0093, 0.0047, 0.0265,\n",
            "         0.0024, 0.0119, 0.0019, 0.0801, 0.0063, 0.0095, 0.0078, 0.0366, 0.0085,\n",
            "         0.0032, 0.0045, 0.0075, 0.0286, 0.0249, 0.0124, 0.0083, 0.0081, 0.0106,\n",
            "         0.0251, 0.0261, 0.0087, 0.0216, 0.0458, 0.0070, 0.0031, 0.0147, 0.0151,\n",
            "         0.0025, 0.0072, 0.0216, 0.0148, 0.0284, 0.0150, 0.0118, 0.0094, 0.0141,\n",
            "         0.0118, 0.0030, 0.0625, 0.0263, 0.0132, 0.0166, 0.0135, 0.0074, 0.0205,\n",
            "         0.0223, 0.0061, 0.0038, 0.0179, 0.0047, 0.0169, 0.0123, 0.0010, 0.0145,\n",
            "         0.0491, 0.0037]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[55]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.2542,  0.0077, -1.5728,  0.6528, -2.0244, -1.3731,  1.8886,  2.6879,\n",
            "          0.9940, -1.9079, -0.8043, -0.3358,  0.4116,  0.5577, -0.8911, -1.0478,\n",
            "         -1.9065, -0.5476, -1.2786, -0.1582,  1.5599, -0.1496, -1.4406,  0.6488,\n",
            "         -1.3412,  0.3569, -0.9536, -1.8299, -0.2695,  0.1350,  0.7850,  0.5161,\n",
            "         -0.3392,  0.4127,  1.1458, -1.2133, -0.2370,  0.1126,  0.0860, -0.4971,\n",
            "          0.6583, -0.8797, -0.0450,  0.1431,  1.6021, -0.8150,  0.3507, -0.2239,\n",
            "         -2.4013, -0.7117, -0.3782,  0.9890, -1.2497,  0.2198,  0.9143, -0.1592,\n",
            "         -1.6053, -1.1741,  0.6289, -0.9825, -1.4075, -1.3757, -0.1028, -1.5216,\n",
            "         -0.4975]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0033, 0.0117, 0.0024, 0.0223, 0.0015, 0.0029, 0.0769, 0.1710, 0.0314,\n",
            "         0.0017, 0.0052, 0.0083, 0.0176, 0.0203, 0.0048, 0.0041, 0.0017, 0.0067,\n",
            "         0.0032, 0.0099, 0.0554, 0.0100, 0.0028, 0.0223, 0.0030, 0.0166, 0.0045,\n",
            "         0.0019, 0.0089, 0.0133, 0.0255, 0.0195, 0.0083, 0.0176, 0.0366, 0.0035,\n",
            "         0.0092, 0.0130, 0.0127, 0.0071, 0.0225, 0.0048, 0.0111, 0.0134, 0.0577,\n",
            "         0.0051, 0.0165, 0.0093, 0.0011, 0.0057, 0.0080, 0.0313, 0.0033, 0.0145,\n",
            "         0.0290, 0.0099, 0.0023, 0.0036, 0.0218, 0.0044, 0.0028, 0.0029, 0.0105,\n",
            "         0.0025, 0.0071]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[44]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975],\n",
            "         [ 1.0541,  1.5018, -0.5266,  ...,  1.8574,  1.5249,  1.3035]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.0541,  1.5018, -0.5266,  0.2354,  1.3178,  1.5074, -0.6100, -0.4258,\n",
            "          0.3269,  0.4072,  0.9861, -1.3452,  2.2328,  0.1249,  1.5514,  0.3271,\n",
            "         -1.5620, -0.9595,  1.8561,  0.1731, -0.8251,  1.4378, -1.6157, -2.0174,\n",
            "         -0.6959,  0.8446,  2.3401,  0.5713, -1.4307, -0.5201,  1.8445,  1.4158,\n",
            "         -0.1498,  0.3753, -0.2322, -0.6855,  1.5919,  0.4873,  1.0210,  1.3580,\n",
            "         -1.6462, -1.6728, -0.7227, -0.1755,  1.0629, -0.9612, -1.2697,  1.2132,\n",
            "         -0.1040,  1.0264,  1.6527, -0.1772, -1.4692, -0.3195,  0.1992,  1.8428,\n",
            "          0.2244, -0.1926,  0.2210,  0.7663, -0.2157, -0.9844,  1.8574,  1.5249,\n",
            "          1.3035]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0197, 0.0309, 0.0041, 0.0087, 0.0257, 0.0310, 0.0037, 0.0045, 0.0095,\n",
            "         0.0103, 0.0184, 0.0018, 0.0641, 0.0078, 0.0324, 0.0095, 0.0014, 0.0026,\n",
            "         0.0440, 0.0082, 0.0030, 0.0289, 0.0014, 0.0009, 0.0034, 0.0160, 0.0714,\n",
            "         0.0122, 0.0016, 0.0041, 0.0435, 0.0283, 0.0059, 0.0100, 0.0054, 0.0035,\n",
            "         0.0338, 0.0112, 0.0191, 0.0267, 0.0013, 0.0013, 0.0033, 0.0058, 0.0199,\n",
            "         0.0026, 0.0019, 0.0231, 0.0062, 0.0192, 0.0359, 0.0058, 0.0016, 0.0050,\n",
            "         0.0084, 0.0434, 0.0086, 0.0057, 0.0086, 0.0148, 0.0055, 0.0026, 0.0440,\n",
            "         0.0316, 0.0253]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[64]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.2542,  0.0077, -1.5728,  ..., -0.1028, -1.5216, -0.4975],\n",
            "         [ 1.0541,  1.5018, -0.5266,  ...,  1.8574,  1.5249,  1.3035],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.4322, -0.2810, -2.2789, -1.5010, -0.5178, -0.0930,  0.7448,  0.2769,\n",
            "         -1.3683, -0.1367,  0.5261,  0.8502,  0.5255, -1.4073, -0.8778,  1.5681,\n",
            "          0.5790, -1.0601, -0.1289,  0.0574, -2.1171,  0.5979, -0.8894, -0.1832,\n",
            "          2.1316,  0.4207, -1.9636, -0.4431,  2.0773, -0.8678,  0.4456, -0.8511,\n",
            "         -0.9897, -0.1547, -0.3183,  0.9285,  0.7569,  0.9505, -1.4028, -0.5422,\n",
            "          0.3932, -1.1699,  0.9138,  1.2533, -0.5639, -0.4533, -0.5694, -1.3843,\n",
            "         -0.1265,  1.6687,  0.4180,  1.1220, -0.4981,  1.7805, -0.3438,  0.0917,\n",
            "          1.4146, -0.9541,  0.4243, -0.4152, -0.9518, -0.9530, -0.5551,  1.0666,\n",
            "          0.5364]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0077, 0.0010, 0.0023, 0.0060, 0.0092, 0.0214, 0.0134, 0.0026,\n",
            "         0.0089, 0.0172, 0.0237, 0.0172, 0.0025, 0.0042, 0.0487, 0.0181, 0.0035,\n",
            "         0.0089, 0.0107, 0.0012, 0.0185, 0.0042, 0.0084, 0.0855, 0.0155, 0.0014,\n",
            "         0.0065, 0.0810, 0.0043, 0.0158, 0.0043, 0.0038, 0.0087, 0.0074, 0.0257,\n",
            "         0.0216, 0.0263, 0.0025, 0.0059, 0.0150, 0.0031, 0.0253, 0.0355, 0.0058,\n",
            "         0.0064, 0.0057, 0.0025, 0.0089, 0.0538, 0.0154, 0.0312, 0.0062, 0.0602,\n",
            "         0.0072, 0.0111, 0.0418, 0.0039, 0.0155, 0.0067, 0.0039, 0.0039, 0.0058,\n",
            "         0.0295, 0.0173]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.0541,  1.5018, -0.5266,  ...,  1.8574,  1.5249,  1.3035],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[3]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.3915e+00,  1.0785e+00, -6.1495e-01, -4.5885e-01,  5.6748e-01,\n",
            "          1.8289e-02, -1.6608e+00,  1.1169e+00,  5.1965e-01, -1.2423e+00,\n",
            "         -9.6182e-01, -8.4998e-02,  1.1854e-01,  2.9843e-01, -7.2636e-01,\n",
            "         -3.1187e-01, -4.5604e-01,  6.4407e-01,  6.0728e-01,  1.2397e+00,\n",
            "          7.3249e-01,  5.0418e-01,  8.7135e-01, -2.7416e-01, -7.4689e-01,\n",
            "         -5.8324e-01,  3.6988e-01, -5.5562e-01, -3.9828e-01, -5.8188e-01,\n",
            "         -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,  8.2161e-01,\n",
            "          3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,  6.1587e-01,\n",
            "         -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01, -4.1474e-01,\n",
            "          1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,  4.2716e-01,\n",
            "         -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,  1.8299e-01,\n",
            "         -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00, -5.2756e-01,\n",
            "          1.0807e+00,  4.5618e-01, -1.0917e+00, -8.2073e-01,  1.8634e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0485, 0.0355, 0.0065, 0.0076, 0.0213, 0.0123, 0.0023, 0.0369, 0.0203,\n",
            "         0.0035, 0.0046, 0.0111, 0.0136, 0.0163, 0.0058, 0.0088, 0.0077, 0.0230,\n",
            "         0.0222, 0.0417, 0.0251, 0.0200, 0.0289, 0.0092, 0.0057, 0.0067, 0.0175,\n",
            "         0.0069, 0.0081, 0.0067, 0.0097, 0.0122, 0.0089, 0.0117, 0.0275, 0.0121,\n",
            "         0.0077, 0.0274, 0.0026, 0.0224, 0.0019, 0.0045, 0.0129, 0.0077, 0.0080,\n",
            "         0.0540, 0.0116, 0.0054, 0.0038, 0.0185, 0.0091, 0.0119, 0.0050, 0.0087,\n",
            "         0.0145, 0.0048, 0.0035, 0.0370, 0.0018, 0.0071, 0.0356, 0.0191, 0.0041,\n",
            "         0.0053, 0.0778]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[37]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634],\n",
            "         [-0.9862, -0.0379,  1.5067,  ...,  0.6420,  0.1551,  1.0719]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-9.8621e-01, -3.7870e-02,  1.5067e+00, -3.3953e-01, -5.9732e-01,\n",
            "          3.7160e-01,  1.0398e+00,  9.3845e-02,  4.5692e-01,  6.8518e-01,\n",
            "          2.7331e-01, -1.1856e+00, -6.8176e-01,  7.8288e-02, -7.3696e-02,\n",
            "          1.9823e+00,  1.0637e+00, -4.0367e-01, -1.1271e-01,  1.5287e+00,\n",
            "          1.7637e-03,  1.4420e+00, -1.2854e+00,  9.8712e-01,  4.5210e-02,\n",
            "          6.2373e-01,  4.5656e-01, -1.1861e+00, -4.5271e-01,  1.4427e+00,\n",
            "         -1.6307e+00, -2.1066e-01,  6.1826e-02,  1.1956e+00, -6.0344e-01,\n",
            "          1.7504e+00,  2.1311e-02, -9.9240e-01,  3.8935e-01,  7.0092e-01,\n",
            "         -1.1757e+00, -5.4395e-01,  1.0870e+00,  3.4960e-01, -1.0106e+00,\n",
            "          5.6473e-01,  9.1057e-01,  5.6835e-01,  4.1694e-01, -1.0416e+00,\n",
            "         -7.0460e-01, -5.3473e-01, -6.5754e-01,  1.2442e-01,  2.3169e-01,\n",
            "         -4.9331e-01, -1.3182e+00,  4.5378e-01, -3.1542e-01,  5.1277e-01,\n",
            "         -1.0591e+00,  1.8920e+00,  6.4197e-01,  1.5511e-01,  1.0719e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0035, 0.0091, 0.0425, 0.0067, 0.0052, 0.0136, 0.0266, 0.0103, 0.0149,\n",
            "         0.0187, 0.0124, 0.0029, 0.0048, 0.0102, 0.0087, 0.0683, 0.0273, 0.0063,\n",
            "         0.0084, 0.0434, 0.0094, 0.0398, 0.0026, 0.0253, 0.0098, 0.0176, 0.0149,\n",
            "         0.0029, 0.0060, 0.0398, 0.0018, 0.0076, 0.0100, 0.0311, 0.0051, 0.0542,\n",
            "         0.0096, 0.0035, 0.0139, 0.0190, 0.0029, 0.0055, 0.0279, 0.0133, 0.0034,\n",
            "         0.0166, 0.0234, 0.0166, 0.0143, 0.0033, 0.0047, 0.0055, 0.0049, 0.0107,\n",
            "         0.0119, 0.0057, 0.0025, 0.0148, 0.0069, 0.0157, 0.0033, 0.0624, 0.0179,\n",
            "         0.0110, 0.0275]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634],\n",
            "         [-0.9862, -0.0379,  1.5067,  ...,  0.6420,  0.1551,  1.0719],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[3]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.9862, -0.0379,  1.5067,  ...,  0.6420,  0.1551,  1.0719],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.3915e+00,  1.0785e+00, -6.1495e-01, -4.5885e-01,  5.6748e-01,\n",
            "          1.8289e-02, -1.6608e+00,  1.1169e+00,  5.1965e-01, -1.2423e+00,\n",
            "         -9.6182e-01, -8.4998e-02,  1.1854e-01,  2.9843e-01, -7.2636e-01,\n",
            "         -3.1187e-01, -4.5604e-01,  6.4407e-01,  6.0728e-01,  1.2397e+00,\n",
            "          7.3249e-01,  5.0418e-01,  8.7135e-01, -2.7416e-01, -7.4689e-01,\n",
            "         -5.8324e-01,  3.6988e-01, -5.5562e-01, -3.9828e-01, -5.8188e-01,\n",
            "         -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,  8.2161e-01,\n",
            "          3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,  6.1587e-01,\n",
            "         -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01, -4.1474e-01,\n",
            "          1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,  4.2716e-01,\n",
            "         -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,  1.8299e-01,\n",
            "         -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00, -5.2756e-01,\n",
            "          1.0807e+00,  4.5618e-01, -1.0917e+00, -8.2073e-01,  1.8634e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0485, 0.0355, 0.0065, 0.0076, 0.0213, 0.0123, 0.0023, 0.0369, 0.0203,\n",
            "         0.0035, 0.0046, 0.0111, 0.0136, 0.0163, 0.0058, 0.0088, 0.0077, 0.0230,\n",
            "         0.0222, 0.0417, 0.0251, 0.0200, 0.0289, 0.0092, 0.0057, 0.0067, 0.0175,\n",
            "         0.0069, 0.0081, 0.0067, 0.0097, 0.0122, 0.0089, 0.0117, 0.0275, 0.0121,\n",
            "         0.0077, 0.0274, 0.0026, 0.0224, 0.0019, 0.0045, 0.0129, 0.0077, 0.0080,\n",
            "         0.0540, 0.0116, 0.0054, 0.0038, 0.0185, 0.0091, 0.0119, 0.0050, 0.0087,\n",
            "         0.0145, 0.0048, 0.0035, 0.0370, 0.0018, 0.0071, 0.0356, 0.0191, 0.0041,\n",
            "         0.0053, 0.0778]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[64]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.4322, -0.2810, -2.2789, -1.5010, -0.5178, -0.0930,  0.7448,  0.2769,\n",
            "         -1.3683, -0.1367,  0.5261,  0.8502,  0.5255, -1.4073, -0.8778,  1.5681,\n",
            "          0.5790, -1.0601, -0.1289,  0.0574, -2.1171,  0.5979, -0.8894, -0.1832,\n",
            "          2.1316,  0.4207, -1.9636, -0.4431,  2.0773, -0.8678,  0.4456, -0.8511,\n",
            "         -0.9897, -0.1547, -0.3183,  0.9285,  0.7569,  0.9505, -1.4028, -0.5422,\n",
            "          0.3932, -1.1699,  0.9138,  1.2533, -0.5639, -0.4533, -0.5694, -1.3843,\n",
            "         -0.1265,  1.6687,  0.4180,  1.1220, -0.4981,  1.7805, -0.3438,  0.0917,\n",
            "          1.4146, -0.9541,  0.4243, -0.4152, -0.9518, -0.9530, -0.5551,  1.0666,\n",
            "          0.5364]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0077, 0.0010, 0.0023, 0.0060, 0.0092, 0.0214, 0.0134, 0.0026,\n",
            "         0.0089, 0.0172, 0.0237, 0.0172, 0.0025, 0.0042, 0.0487, 0.0181, 0.0035,\n",
            "         0.0089, 0.0107, 0.0012, 0.0185, 0.0042, 0.0084, 0.0855, 0.0155, 0.0014,\n",
            "         0.0065, 0.0810, 0.0043, 0.0158, 0.0043, 0.0038, 0.0087, 0.0074, 0.0257,\n",
            "         0.0216, 0.0263, 0.0025, 0.0059, 0.0150, 0.0031, 0.0253, 0.0355, 0.0058,\n",
            "         0.0064, 0.0057, 0.0025, 0.0089, 0.0538, 0.0154, 0.0312, 0.0062, 0.0602,\n",
            "         0.0072, 0.0111, 0.0418, 0.0039, 0.0155, 0.0067, 0.0039, 0.0039, 0.0058,\n",
            "         0.0295, 0.0173]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[18]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1935, -0.6324, -0.2059,  2.4028,  0.7788,  0.1370, -1.2904,  1.1833,\n",
            "          1.4331, -1.8232, -0.8942,  0.7385, -0.4643, -0.2165,  0.7709,  0.6257,\n",
            "         -0.2308,  1.0764, -0.0478,  0.1224, -1.9634,  0.7606,  0.9647, -0.6278,\n",
            "         -0.1233,  1.6911, -0.1114, -0.0188,  0.0890, -0.6228, -0.2247, -0.2451,\n",
            "         -0.1528,  0.6118,  0.9507,  0.7443,  0.3612, -0.5346,  0.6162,  0.1405,\n",
            "          1.6079, -1.2249,  1.0849, -1.1393,  0.0502, -1.1192,  1.0587, -0.3821,\n",
            "         -0.4479,  0.0083,  1.4887, -1.0974, -0.0379,  1.5241,  0.2110,  1.3096,\n",
            "         -0.6448,  0.3564,  0.8986,  0.6359, -0.4193, -0.0367, -0.5336, -1.3621,\n",
            "          0.3269]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0076, 0.0049, 0.0075, 0.1023, 0.0202, 0.0106, 0.0025, 0.0302, 0.0388,\n",
            "         0.0015, 0.0038, 0.0194, 0.0058, 0.0075, 0.0200, 0.0173, 0.0073, 0.0272,\n",
            "         0.0088, 0.0105, 0.0013, 0.0198, 0.0243, 0.0049, 0.0082, 0.0502, 0.0083,\n",
            "         0.0091, 0.0101, 0.0050, 0.0074, 0.0072, 0.0079, 0.0171, 0.0239, 0.0195,\n",
            "         0.0133, 0.0054, 0.0171, 0.0107, 0.0462, 0.0027, 0.0274, 0.0030, 0.0097,\n",
            "         0.0030, 0.0267, 0.0063, 0.0059, 0.0093, 0.0410, 0.0031, 0.0089, 0.0425,\n",
            "         0.0114, 0.0343, 0.0049, 0.0132, 0.0227, 0.0175, 0.0061, 0.0089, 0.0054,\n",
            "         0.0024, 0.0128]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[7]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
            "         [ 0.2410, -1.6206,  0.4488,  ..., -0.6825, -1.6026, -0.1336]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 2.4102e-01, -1.6206e+00,  4.4878e-01,  5.0098e-01, -7.7269e-01,\n",
            "         -1.0796e+00,  3.5018e-03,  6.8618e-04,  7.9731e-02,  1.9934e-01,\n",
            "         -1.5677e+00, -2.1165e+00, -1.1813e+00,  7.5047e-02,  6.5496e-01,\n",
            "         -5.0057e-01, -2.1613e-01,  2.8637e-02, -7.7717e-02,  6.6956e-03,\n",
            "         -1.2177e+00,  1.1479e+00, -1.5735e+00,  1.3876e+00,  7.2512e-01,\n",
            "          6.4547e-01, -3.3132e-01, -1.0390e+00,  9.1116e-01,  1.2984e+00,\n",
            "          5.5509e-01, -4.6531e-01, -5.5186e-01,  1.1925e+00, -6.6420e-01,\n",
            "         -9.1165e-03, -1.1712e+00,  4.8306e-01,  3.5048e-01, -5.7443e-01,\n",
            "          1.2531e+00, -6.7409e-01,  3.9710e-01,  1.9287e-01, -2.1749e+00,\n",
            "          1.6730e+00, -4.2359e-02, -1.1758e-01,  1.0546e+00, -1.5694e-02,\n",
            "          2.4782e-01,  5.0760e-01, -9.0286e-01,  1.7872e+00,  8.9457e-02,\n",
            "         -3.7475e-01, -4.7815e-01, -6.0669e-01,  1.8328e+00,  2.9308e-01,\n",
            "          4.3631e-02,  2.1370e+00, -6.8247e-01, -1.6026e+00, -1.3362e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0131, 0.0020, 0.0161, 0.0170, 0.0048, 0.0035, 0.0103, 0.0103, 0.0111,\n",
            "         0.0126, 0.0021, 0.0012, 0.0032, 0.0111, 0.0198, 0.0062, 0.0083, 0.0106,\n",
            "         0.0095, 0.0104, 0.0030, 0.0324, 0.0021, 0.0412, 0.0213, 0.0196, 0.0074,\n",
            "         0.0036, 0.0256, 0.0377, 0.0179, 0.0065, 0.0059, 0.0339, 0.0053, 0.0102,\n",
            "         0.0032, 0.0167, 0.0146, 0.0058, 0.0360, 0.0052, 0.0153, 0.0125, 0.0012,\n",
            "         0.0548, 0.0099, 0.0092, 0.0296, 0.0101, 0.0132, 0.0171, 0.0042, 0.0615,\n",
            "         0.0113, 0.0071, 0.0064, 0.0056, 0.0644, 0.0138, 0.0108, 0.0872, 0.0052,\n",
            "         0.0021, 0.0090]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[61]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
            "         [ 0.2410, -1.6206,  0.4488,  ..., -0.6825, -1.6026, -0.1336],\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4897,  0.0655,  1.0370,  1.0680,  0.1121,  1.1379,  0.1987, -0.6042,\n",
            "          0.1626,  0.8063,  0.7040,  0.2668,  0.0244,  0.1020, -0.3747, -1.2485,\n",
            "         -1.6522,  0.4945,  0.2456, -0.2416, -0.4212,  0.3810,  0.2634,  0.6288,\n",
            "          0.1788,  0.1912,  1.6184,  0.9607, -0.2439,  0.3508, -0.4618, -1.0462,\n",
            "         -1.3135,  0.0185, -0.5131,  1.5241,  0.5632, -1.1132,  0.0729,  0.4962,\n",
            "          0.8535,  0.2138, -1.4130, -0.6337,  1.9594, -1.0523, -0.5277, -0.7341,\n",
            "         -1.9469, -0.0727, -0.3491, -1.3596, -0.3679,  0.7067, -0.1839, -1.0535,\n",
            "         -1.1778, -0.3159, -0.3419, -1.1736, -0.6405,  0.0713,  0.4397, -0.7343,\n",
            "         -0.1916]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0196, 0.0128, 0.0338, 0.0349, 0.0134, 0.0374, 0.0146, 0.0066, 0.0141,\n",
            "         0.0269, 0.0243, 0.0157, 0.0123, 0.0133, 0.0082, 0.0034, 0.0023, 0.0197,\n",
            "         0.0153, 0.0094, 0.0079, 0.0176, 0.0156, 0.0225, 0.0143, 0.0145, 0.0605,\n",
            "         0.0314, 0.0094, 0.0170, 0.0076, 0.0042, 0.0032, 0.0122, 0.0072, 0.0551,\n",
            "         0.0211, 0.0039, 0.0129, 0.0197, 0.0282, 0.0149, 0.0029, 0.0064, 0.0851,\n",
            "         0.0042, 0.0071, 0.0058, 0.0017, 0.0112, 0.0085, 0.0031, 0.0083, 0.0243,\n",
            "         0.0100, 0.0042, 0.0037, 0.0087, 0.0085, 0.0037, 0.0063, 0.0129, 0.0186,\n",
            "         0.0058, 0.0099]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[6]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.2410, -1.6206,  0.4488,  ..., -0.6825, -1.6026, -0.1336],\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916],\n",
            "         [ 0.4160,  0.3362, -0.4512,  ..., -1.6525, -0.8816, -1.4546]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4160,  0.3362, -0.4512, -0.6996,  0.9208, -0.9963,  1.2962, -2.2434,\n",
            "          0.5272, -0.1585,  0.0933,  0.0697, -1.1470, -1.0414, -0.2572, -0.7223,\n",
            "          0.1643, -1.3590,  0.9622, -0.7641, -1.7653,  0.6884, -0.2245,  0.2468,\n",
            "          0.1747,  0.5243,  0.3091,  1.1661, -2.1821, -1.0422,  1.0207,  3.2082,\n",
            "         -3.7624, -0.5330,  0.6630, -1.5717, -0.5622, -0.2964,  0.5512, -1.2364,\n",
            "          0.9409,  0.7608, -1.3756,  1.2168,  0.0268, -2.1902, -1.1730,  2.5181,\n",
            "          1.6212, -1.8134,  2.0867,  0.1535,  0.1135, -0.1979,  1.6621,  0.6151,\n",
            "          0.6763,  0.6228,  0.0943, -0.3156,  0.7850, -0.8699, -1.6525, -0.8816,\n",
            "         -1.4546]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[1.2383e-02, 1.1433e-02, 5.2020e-03, 4.0578e-03, 2.0513e-02, 3.0160e-03,\n",
            "         2.9859e-02, 8.6669e-04, 1.3839e-02, 6.9712e-03, 8.9674e-03, 8.7585e-03,\n",
            "         2.5942e-03, 2.8831e-03, 6.3157e-03, 3.9670e-03, 9.6274e-03, 2.0987e-03,\n",
            "         2.1380e-02, 3.8043e-03, 1.3979e-03, 1.6259e-02, 6.5257e-03, 1.0455e-02,\n",
            "         9.7282e-03, 1.3799e-02, 1.1127e-02, 2.6217e-02, 9.2140e-04, 2.8809e-03,\n",
            "         2.2668e-02, 2.0204e-01, 1.8974e-04, 4.7936e-03, 1.5852e-02, 1.6965e-03,\n",
            "         4.6558e-03, 6.0730e-03, 1.4176e-02, 2.3725e-03, 2.0930e-02, 1.7481e-02,\n",
            "         2.0640e-03, 2.7580e-02, 8.3901e-03, 9.1399e-04, 2.5277e-03, 1.0133e-01,\n",
            "         4.1324e-02, 1.3322e-03, 6.5823e-02, 9.5238e-03, 9.1507e-03, 6.7022e-03,\n",
            "         4.3051e-02, 1.5111e-02, 1.6065e-02, 1.5227e-02, 8.9762e-03, 5.9578e-03,\n",
            "         1.7908e-02, 3.4224e-03, 1.5648e-03, 3.3828e-03, 1.9074e-03]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[11]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4897,  0.0655,  1.0370,  ...,  0.4397, -0.7343, -0.1916],\n",
            "         [ 0.4160,  0.3362, -0.4512,  ..., -1.6525, -0.8816, -1.4546],\n",
            "         [-0.2359,  0.4054, -0.3333,  ..., -0.2710,  0.0300,  0.2400]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2359,  0.4054, -0.3333, -1.9416, -1.4176, -0.0061, -1.3411, -1.0705,\n",
            "          1.3173,  0.3719,  0.5198,  0.9829,  0.8048,  2.3235, -1.5808, -1.7794,\n",
            "          0.4094,  0.0929,  0.4970,  0.0585,  0.1033,  0.0720,  1.1080,  0.7293,\n",
            "          0.3665, -0.9269, -1.2297,  0.1063,  0.0582,  0.5353,  1.1746,  0.0854,\n",
            "          0.0113, -0.4325, -0.2694, -2.5596, -0.1753, -0.9741,  0.1251,  1.6380,\n",
            "         -0.5763, -1.4705, -0.2613, -0.5368, -0.7872,  0.0148,  0.4648,  0.6110,\n",
            "         -1.5670, -0.7868,  0.3967, -0.9755,  0.5122,  0.3330,  1.0995,  0.4034,\n",
            "         -1.1089, -0.0538, -1.2741, -0.7106,  0.7490,  0.6800, -0.2710,  0.0300,\n",
            "          0.2400]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0091, 0.0173, 0.0083, 0.0017, 0.0028, 0.0115, 0.0030, 0.0040, 0.0431,\n",
            "         0.0167, 0.0194, 0.0308, 0.0258, 0.1179, 0.0024, 0.0019, 0.0174, 0.0127,\n",
            "         0.0190, 0.0122, 0.0128, 0.0124, 0.0350, 0.0239, 0.0167, 0.0046, 0.0034,\n",
            "         0.0128, 0.0122, 0.0197, 0.0374, 0.0126, 0.0117, 0.0075, 0.0088, 0.0009,\n",
            "         0.0097, 0.0044, 0.0131, 0.0594, 0.0065, 0.0027, 0.0089, 0.0067, 0.0053,\n",
            "         0.0117, 0.0184, 0.0213, 0.0024, 0.0053, 0.0172, 0.0044, 0.0193, 0.0161,\n",
            "         0.0347, 0.0173, 0.0038, 0.0109, 0.0032, 0.0057, 0.0244, 0.0228, 0.0088,\n",
            "         0.0119, 0.0147]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[43]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4160,  0.3362, -0.4512,  ..., -1.6525, -0.8816, -1.4546],\n",
            "         [-0.2359,  0.4054, -0.3333,  ..., -0.2710,  0.0300,  0.2400],\n",
            "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.3323, -0.0872, -0.7470, -0.6074,  0.3418,  0.5343,  0.3957, -0.4919,\n",
            "         -0.0894, -1.3886,  1.2835, -0.3975,  2.0152,  1.6773, -0.3833,  1.5728,\n",
            "          1.9458,  0.7247, -0.4834, -0.3263,  0.3193, -0.4198, -0.6435, -0.3311,\n",
            "          0.7554, -1.2385,  0.4067,  0.9982, -0.6511,  1.2450,  0.2804,  0.8371,\n",
            "         -0.4119,  0.2115, -0.6240,  0.0203, -0.3418,  1.4934,  1.7307,  1.3354,\n",
            "         -0.2712,  0.4902,  0.6600, -1.6321, -0.7858,  1.7688,  2.6160, -0.5767,\n",
            "         -0.3628, -2.7428,  0.7428,  0.0737,  0.2050, -0.5497,  2.1261, -0.9240,\n",
            "          0.1048,  0.8324,  1.4287, -0.7789,  2.9275, -0.8525, -0.6716, -0.9572,\n",
            "         -0.9594]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0095, 0.0063, 0.0032, 0.0037, 0.0096, 0.0117, 0.0101, 0.0042, 0.0062,\n",
            "         0.0017, 0.0247, 0.0046, 0.0513, 0.0366, 0.0047, 0.0329, 0.0478, 0.0141,\n",
            "         0.0042, 0.0049, 0.0094, 0.0045, 0.0036, 0.0049, 0.0145, 0.0020, 0.0103,\n",
            "         0.0185, 0.0036, 0.0237, 0.0090, 0.0158, 0.0045, 0.0084, 0.0037, 0.0070,\n",
            "         0.0049, 0.0304, 0.0386, 0.0260, 0.0052, 0.0112, 0.0132, 0.0013, 0.0031,\n",
            "         0.0401, 0.0935, 0.0038, 0.0048, 0.0004, 0.0144, 0.0074, 0.0084, 0.0039,\n",
            "         0.0573, 0.0027, 0.0076, 0.0157, 0.0285, 0.0031, 0.1276, 0.0029, 0.0035,\n",
            "         0.0026, 0.0026]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[17]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.2359,  0.4054, -0.3333,  ..., -0.2710,  0.0300,  0.2400],\n",
            "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.4892, -2.5589,  1.4134,  0.3562, -0.9866, -0.9082, -1.6532,  0.8884,\n",
            "         -0.4134,  1.3855, -0.3127,  0.7171,  0.9241, -0.5131, -0.7612,  0.6070,\n",
            "          0.2605, -0.7458, -1.6201,  1.1018,  2.0153, -0.4154,  1.6371,  0.3695,\n",
            "         -0.8532, -0.4395,  0.1850, -0.8069,  0.1648,  1.6064,  2.0446, -0.8769,\n",
            "          0.3839, -0.7857, -0.3680, -0.6815,  1.6723,  0.5269,  0.8404,  1.2201,\n",
            "          0.4875,  0.2428, -1.2750,  0.4207, -1.2408, -2.2630, -0.2638, -0.2505,\n",
            "         -0.3959, -0.9208,  0.2549, -0.0755,  0.4162,  0.5739, -1.0490,  0.7784,\n",
            "          2.0459,  1.6576,  0.6625, -0.5261, -0.0290,  1.0471, -1.4296,  0.2347,\n",
            "         -1.2034]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0055, 0.0007, 0.0367, 0.0128, 0.0033, 0.0036, 0.0017, 0.0217, 0.0059,\n",
            "         0.0357, 0.0065, 0.0183, 0.0225, 0.0053, 0.0042, 0.0164, 0.0116, 0.0042,\n",
            "         0.0018, 0.0269, 0.0670, 0.0059, 0.0459, 0.0129, 0.0038, 0.0058, 0.0107,\n",
            "         0.0040, 0.0105, 0.0445, 0.0690, 0.0037, 0.0131, 0.0041, 0.0062, 0.0045,\n",
            "         0.0475, 0.0151, 0.0207, 0.0303, 0.0145, 0.0114, 0.0025, 0.0136, 0.0026,\n",
            "         0.0009, 0.0069, 0.0070, 0.0060, 0.0036, 0.0115, 0.0083, 0.0135, 0.0159,\n",
            "         0.0031, 0.0195, 0.0691, 0.0469, 0.0173, 0.0053, 0.0087, 0.0254, 0.0021,\n",
            "         0.0113, 0.0027]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[49]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
            "         [-0.2950, -0.6511,  1.4937,  ..., -0.5250,  1.2672,  2.6002]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2950, -0.6511,  1.4937,  1.1173,  0.7356, -0.0165, -0.2420, -0.2702,\n",
            "          0.0598, -0.3667,  0.2255, -0.9411, -1.6868, -0.9292, -1.2395, -1.0842,\n",
            "         -0.3648, -0.1092, -0.1591,  0.6822,  1.1651,  0.9528,  0.1548, -0.0579,\n",
            "         -1.0556, -0.7017,  0.4944,  1.1985, -0.5672, -0.3204,  1.5870,  1.5017,\n",
            "         -2.5039,  0.8525,  0.0561, -0.0418,  0.3568, -1.5907, -0.7374, -1.2256,\n",
            "          0.0682,  1.0599, -0.8286,  0.0083, -0.3394,  0.8296,  0.2789, -1.0439,\n",
            "         -1.4084, -0.2276, -0.6571, -0.5281, -0.0276, -0.9014,  1.4106,  1.1756,\n",
            "         -0.0158, -0.5705,  2.0617,  0.3756, -0.4315, -0.6968, -0.5250,  1.2672,\n",
            "          2.6002]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0071, 0.0050, 0.0427, 0.0293, 0.0200, 0.0094, 0.0075, 0.0073, 0.0102,\n",
            "         0.0066, 0.0120, 0.0037, 0.0018, 0.0038, 0.0028, 0.0032, 0.0067, 0.0086,\n",
            "         0.0082, 0.0190, 0.0308, 0.0249, 0.0112, 0.0091, 0.0033, 0.0048, 0.0157,\n",
            "         0.0318, 0.0054, 0.0070, 0.0469, 0.0431, 0.0008, 0.0225, 0.0101, 0.0092,\n",
            "         0.0137, 0.0020, 0.0046, 0.0028, 0.0103, 0.0277, 0.0042, 0.0097, 0.0068,\n",
            "         0.0220, 0.0127, 0.0034, 0.0023, 0.0076, 0.0050, 0.0057, 0.0093, 0.0039,\n",
            "         0.0393, 0.0311, 0.0094, 0.0054, 0.0754, 0.0140, 0.0062, 0.0048, 0.0057,\n",
            "         0.0341, 0.1292]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[64]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034],\n",
            "         [-0.2950, -0.6511,  1.4937,  ..., -0.5250,  1.2672,  2.6002],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.4322, -0.2810, -2.2789, -1.5010, -0.5178, -0.0930,  0.7448,  0.2769,\n",
            "         -1.3683, -0.1367,  0.5261,  0.8502,  0.5255, -1.4073, -0.8778,  1.5681,\n",
            "          0.5790, -1.0601, -0.1289,  0.0574, -2.1171,  0.5979, -0.8894, -0.1832,\n",
            "          2.1316,  0.4207, -1.9636, -0.4431,  2.0773, -0.8678,  0.4456, -0.8511,\n",
            "         -0.9897, -0.1547, -0.3183,  0.9285,  0.7569,  0.9505, -1.4028, -0.5422,\n",
            "          0.3932, -1.1699,  0.9138,  1.2533, -0.5639, -0.4533, -0.5694, -1.3843,\n",
            "         -0.1265,  1.6687,  0.4180,  1.1220, -0.4981,  1.7805, -0.3438,  0.0917,\n",
            "          1.4146, -0.9541,  0.4243, -0.4152, -0.9518, -0.9530, -0.5551,  1.0666,\n",
            "          0.5364]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0077, 0.0010, 0.0023, 0.0060, 0.0092, 0.0214, 0.0134, 0.0026,\n",
            "         0.0089, 0.0172, 0.0237, 0.0172, 0.0025, 0.0042, 0.0487, 0.0181, 0.0035,\n",
            "         0.0089, 0.0107, 0.0012, 0.0185, 0.0042, 0.0084, 0.0855, 0.0155, 0.0014,\n",
            "         0.0065, 0.0810, 0.0043, 0.0158, 0.0043, 0.0038, 0.0087, 0.0074, 0.0257,\n",
            "         0.0216, 0.0263, 0.0025, 0.0059, 0.0150, 0.0031, 0.0253, 0.0355, 0.0058,\n",
            "         0.0064, 0.0057, 0.0025, 0.0089, 0.0538, 0.0154, 0.0312, 0.0062, 0.0602,\n",
            "         0.0072, 0.0111, 0.0418, 0.0039, 0.0155, 0.0067, 0.0039, 0.0039, 0.0058,\n",
            "         0.0295, 0.0173]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[62]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.2950, -0.6511,  1.4937,  ..., -0.5250,  1.2672,  2.6002],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4222, -1.8111, -1.0118, -0.4569,  0.0202, -0.1555, -0.1721,  0.2664,\n",
            "         -0.2054, -1.3252,  0.6271,  1.4733,  0.9470, -1.0751,  0.9042, -1.4850,\n",
            "         -0.3449,  1.3128, -0.5799, -0.6507, -0.1817,  0.9045,  1.2905,  0.6977,\n",
            "         -1.1612,  0.0093, -1.0707,  0.1294, -0.4570,  0.9330, -0.2858, -0.9957,\n",
            "          0.5773, -0.5476, -2.0319, -0.1269, -0.5162,  0.2046,  1.4801,  1.6253,\n",
            "         -0.8076,  0.8477, -1.0219, -1.9241,  1.4480, -1.8130, -0.0638, -0.1206,\n",
            "          1.5924, -0.1682,  1.7089, -0.1853,  0.1268, -1.2130, -0.4556,  1.1074,\n",
            "          1.4224, -1.8213, -0.0451, -0.1755, -0.9631, -0.5043,  0.5462,  0.2788,\n",
            "          0.7280]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0161, 0.0017, 0.0038, 0.0067, 0.0108, 0.0090, 0.0089, 0.0138, 0.0086,\n",
            "         0.0028, 0.0197, 0.0460, 0.0272, 0.0036, 0.0261, 0.0024, 0.0075, 0.0392,\n",
            "         0.0059, 0.0055, 0.0088, 0.0261, 0.0383, 0.0212, 0.0033, 0.0106, 0.0036,\n",
            "         0.0120, 0.0067, 0.0268, 0.0079, 0.0039, 0.0188, 0.0061, 0.0014, 0.0093,\n",
            "         0.0063, 0.0129, 0.0463, 0.0536, 0.0047, 0.0246, 0.0038, 0.0015, 0.0449,\n",
            "         0.0017, 0.0099, 0.0093, 0.0518, 0.0089, 0.0583, 0.0088, 0.0120, 0.0031,\n",
            "         0.0067, 0.0319, 0.0437, 0.0017, 0.0101, 0.0088, 0.0040, 0.0064, 0.0182,\n",
            "         0.0139, 0.0218]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[48]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.5377, -0.2542, -1.8516,  1.3382, -1.0219, -0.9356,  0.8454, -1.3798,\n",
            "         -0.1421,  0.7084, -0.2751,  1.2128,  1.3650, -1.3301, -1.4832, -0.9809,\n",
            "         -1.5012, -1.7006,  1.2642, -1.1078,  0.5398, -0.7718,  0.6175,  2.1793,\n",
            "         -0.1047, -0.7940,  1.1206, -0.9039,  0.4935,  0.5804,  1.2005, -1.5786,\n",
            "         -0.5037, -0.7478, -1.3617, -0.3347,  0.7188, -1.7258, -0.0902, -0.0148,\n",
            "          0.9630, -1.7663, -0.3839, -0.0170, -1.0878,  0.6621, -0.8364, -0.8048,\n",
            "          0.0204, -0.5749, -0.4291,  1.1989,  0.0905, -0.4063,  0.1165, -1.2079,\n",
            "         -1.1770,  0.6919, -1.3267,  2.5186, -1.0516, -1.0338,  2.1974,  0.3053,\n",
            "         -1.1890]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0172, 0.0078, 0.0016, 0.0384, 0.0036, 0.0039, 0.0234, 0.0025, 0.0087,\n",
            "         0.0204, 0.0076, 0.0338, 0.0394, 0.0027, 0.0023, 0.0038, 0.0022, 0.0018,\n",
            "         0.0356, 0.0033, 0.0173, 0.0047, 0.0187, 0.0889, 0.0091, 0.0045, 0.0309,\n",
            "         0.0041, 0.0165, 0.0180, 0.0334, 0.0021, 0.0061, 0.0048, 0.0026, 0.0072,\n",
            "         0.0206, 0.0018, 0.0092, 0.0099, 0.0264, 0.0017, 0.0069, 0.0099, 0.0034,\n",
            "         0.0195, 0.0044, 0.0045, 0.0103, 0.0057, 0.0066, 0.0334, 0.0110, 0.0067,\n",
            "         0.0113, 0.0030, 0.0031, 0.0201, 0.0027, 0.1249, 0.0035, 0.0036, 0.0906,\n",
            "         0.0137, 0.0031]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[45]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 6.6348e-01,  2.6726e-01, -4.0968e-02, -4.1045e-01, -7.9258e-01,\n",
            "         -4.5052e-01, -1.2630e+00, -1.1049e-01, -1.5258e+00, -2.4088e+00,\n",
            "          6.2567e-01, -7.8628e-01, -1.3341e-01, -5.0673e-01,  4.9312e-01,\n",
            "          3.1957e+00, -6.9719e-01,  1.4158e-01,  1.1991e+00,  8.4574e-01,\n",
            "          1.1119e+00,  7.5411e-01,  5.6716e-01,  1.0343e+00,  4.2398e-01,\n",
            "         -3.9114e-01, -2.2213e+00,  1.7533e+00, -1.2363e+00,  1.1138e+00,\n",
            "          1.7952e+00,  2.8732e-01, -1.9710e-01,  1.2285e+00,  9.5273e-03,\n",
            "          2.2227e-01,  1.9963e+00,  1.3765e+00,  1.0229e+00, -1.3248e-03,\n",
            "          7.7858e-01, -2.9392e-01,  1.2563e+00, -7.8401e-01,  8.0610e-01,\n",
            "         -3.7246e-01, -8.1083e-01,  8.6826e-01,  7.9161e-01,  6.6330e-01,\n",
            "          1.8970e-01,  1.7075e+00,  7.7272e-01, -2.6976e-01, -7.1044e-01,\n",
            "          1.7779e+00, -7.2955e-01, -8.2731e-01, -2.5742e+00, -3.9104e-01,\n",
            "          2.3160e-02,  8.5039e-01, -5.8610e-01, -1.0893e+00,  1.9482e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0144, 0.0097, 0.0071, 0.0049, 0.0034, 0.0047, 0.0021, 0.0066, 0.0016,\n",
            "         0.0007, 0.0139, 0.0034, 0.0065, 0.0045, 0.0122, 0.1813, 0.0037, 0.0086,\n",
            "         0.0246, 0.0173, 0.0226, 0.0158, 0.0131, 0.0209, 0.0113, 0.0050, 0.0008,\n",
            "         0.0429, 0.0022, 0.0226, 0.0447, 0.0099, 0.0061, 0.0254, 0.0075, 0.0093,\n",
            "         0.0546, 0.0294, 0.0206, 0.0074, 0.0162, 0.0055, 0.0261, 0.0034, 0.0166,\n",
            "         0.0051, 0.0033, 0.0177, 0.0164, 0.0144, 0.0090, 0.0409, 0.0161, 0.0057,\n",
            "         0.0036, 0.0439, 0.0036, 0.0032, 0.0006, 0.0050, 0.0076, 0.0174, 0.0041,\n",
            "         0.0025, 0.0090]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[15]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.5377, -0.2542, -1.8516,  ...,  2.1974,  0.3053, -1.1890],\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948],\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2060,  1.5973,  0.1185, -1.2549,  1.3024,  0.4760, -0.8871,  1.3709,\n",
            "         -1.9473, -0.8017, -1.3055, -0.4910,  0.4430,  0.2178, -0.3297, -0.0192,\n",
            "          0.9225,  0.9187,  0.2998,  0.6106,  0.7791,  0.1237,  1.8620,  1.7080,\n",
            "         -1.6045,  0.3338, -2.0513,  0.5923,  0.4880, -1.4055, -0.6686, -0.4831,\n",
            "         -0.2298,  0.9043,  0.7631, -0.1606,  0.9156, -0.6908, -0.3065, -1.1809,\n",
            "          0.8175, -2.0392,  0.1558, -0.2996, -0.5391, -0.3657,  0.8282, -0.4826,\n",
            "          1.8330,  0.3421,  0.2154, -0.1029, -0.0946,  0.0070,  0.1484, -0.5403,\n",
            "         -1.9312, -0.7858, -0.6731, -0.0901,  0.2598, -0.5349,  0.5812, -0.5356,\n",
            "         -1.7944]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0089, 0.0540, 0.0123, 0.0031, 0.0402, 0.0176, 0.0045, 0.0430, 0.0016,\n",
            "         0.0049, 0.0030, 0.0067, 0.0170, 0.0136, 0.0079, 0.0107, 0.0275, 0.0274,\n",
            "         0.0147, 0.0201, 0.0238, 0.0124, 0.0703, 0.0603, 0.0022, 0.0153, 0.0014,\n",
            "         0.0198, 0.0178, 0.0027, 0.0056, 0.0067, 0.0087, 0.0270, 0.0234, 0.0093,\n",
            "         0.0273, 0.0055, 0.0080, 0.0034, 0.0248, 0.0014, 0.0128, 0.0081, 0.0064,\n",
            "         0.0076, 0.0250, 0.0067, 0.0683, 0.0154, 0.0136, 0.0099, 0.0099, 0.0110,\n",
            "         0.0127, 0.0064, 0.0016, 0.0050, 0.0056, 0.0100, 0.0142, 0.0064, 0.0195,\n",
            "         0.0064, 0.0018]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[23]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.6635,  0.2673, -0.0410,  ..., -0.5861, -1.0893,  0.1948],\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944],\n",
            "         [-1.5766,  0.6679, -1.0362,  ..., -1.2814, -0.4783,  0.2979]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.5766e+00,  6.6795e-01, -1.0362e+00,  1.6007e+00, -7.1171e-01,\n",
            "         -4.6747e-01, -1.1337e+00, -1.2783e+00, -4.3894e-01, -8.6417e-01,\n",
            "          7.7469e-01, -2.7729e-01, -2.6472e-01,  1.2861e+00,  1.0044e+00,\n",
            "          2.3730e-02,  5.7260e-01, -6.5357e-01, -6.6449e-01,  2.7637e-01,\n",
            "          1.9566e+00,  1.3431e+00,  1.6737e-01, -1.2707e+00, -1.0524e+00,\n",
            "         -8.6907e-02,  5.5795e-02, -1.3323e+00, -9.5061e-01, -1.1121e+00,\n",
            "          3.6341e-01,  8.0640e-01, -2.8513e-01,  7.1068e-01, -8.2941e-02,\n",
            "          9.5442e-04,  1.0919e-01, -5.2422e-01,  8.1311e-01, -1.1475e+00,\n",
            "          1.3358e+00, -6.1024e-01,  6.5213e-02, -8.1089e-01,  1.6524e-01,\n",
            "         -2.1920e+00,  1.3500e-01,  2.0137e-01, -1.2806e+00, -9.7899e-01,\n",
            "         -7.6353e-01,  8.9813e-02, -1.5046e+00,  1.8576e+00,  1.8842e+00,\n",
            "         -1.0679e+00, -9.7165e-01, -4.3131e-01,  9.0768e-01,  2.1939e+00,\n",
            "          3.9268e-01,  4.7371e-01, -1.2814e+00, -4.7826e-01,  2.9794e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0021, 0.0201, 0.0037, 0.0512, 0.0051, 0.0065, 0.0033, 0.0029, 0.0067,\n",
            "         0.0043, 0.0224, 0.0078, 0.0079, 0.0374, 0.0282, 0.0106, 0.0183, 0.0054,\n",
            "         0.0053, 0.0136, 0.0730, 0.0395, 0.0122, 0.0029, 0.0036, 0.0095, 0.0109,\n",
            "         0.0027, 0.0040, 0.0034, 0.0148, 0.0231, 0.0078, 0.0210, 0.0095, 0.0103,\n",
            "         0.0115, 0.0061, 0.0233, 0.0033, 0.0393, 0.0056, 0.0110, 0.0046, 0.0122,\n",
            "         0.0012, 0.0118, 0.0126, 0.0029, 0.0039, 0.0048, 0.0113, 0.0023, 0.0662,\n",
            "         0.0679, 0.0035, 0.0039, 0.0067, 0.0256, 0.0926, 0.0153, 0.0166, 0.0029,\n",
            "         0.0064, 0.0139]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[18]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944],\n",
            "         [-1.5766,  0.6679, -1.0362,  ..., -1.2814, -0.4783,  0.2979],\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1935, -0.6324, -0.2059,  2.4028,  0.7788,  0.1370, -1.2904,  1.1833,\n",
            "          1.4331, -1.8232, -0.8942,  0.7385, -0.4643, -0.2165,  0.7709,  0.6257,\n",
            "         -0.2308,  1.0764, -0.0478,  0.1224, -1.9634,  0.7606,  0.9647, -0.6278,\n",
            "         -0.1233,  1.6911, -0.1114, -0.0188,  0.0890, -0.6228, -0.2247, -0.2451,\n",
            "         -0.1528,  0.6118,  0.9507,  0.7443,  0.3612, -0.5346,  0.6162,  0.1405,\n",
            "          1.6079, -1.2249,  1.0849, -1.1393,  0.0502, -1.1192,  1.0587, -0.3821,\n",
            "         -0.4479,  0.0083,  1.4887, -1.0974, -0.0379,  1.5241,  0.2110,  1.3096,\n",
            "         -0.6448,  0.3564,  0.8986,  0.6359, -0.4193, -0.0367, -0.5336, -1.3621,\n",
            "          0.3269]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0076, 0.0049, 0.0075, 0.1023, 0.0202, 0.0106, 0.0025, 0.0302, 0.0388,\n",
            "         0.0015, 0.0038, 0.0194, 0.0058, 0.0075, 0.0200, 0.0173, 0.0073, 0.0272,\n",
            "         0.0088, 0.0105, 0.0013, 0.0198, 0.0243, 0.0049, 0.0082, 0.0502, 0.0083,\n",
            "         0.0091, 0.0101, 0.0050, 0.0074, 0.0072, 0.0079, 0.0171, 0.0239, 0.0195,\n",
            "         0.0133, 0.0054, 0.0171, 0.0107, 0.0462, 0.0027, 0.0274, 0.0030, 0.0097,\n",
            "         0.0030, 0.0267, 0.0063, 0.0059, 0.0093, 0.0410, 0.0031, 0.0089, 0.0425,\n",
            "         0.0114, 0.0343, 0.0049, 0.0132, 0.0227, 0.0175, 0.0061, 0.0089, 0.0054,\n",
            "         0.0024, 0.0128]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[15]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.5766,  0.6679, -1.0362,  ..., -1.2814, -0.4783,  0.2979],\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2060,  1.5973,  0.1185, -1.2549,  1.3024,  0.4760, -0.8871,  1.3709,\n",
            "         -1.9473, -0.8017, -1.3055, -0.4910,  0.4430,  0.2178, -0.3297, -0.0192,\n",
            "          0.9225,  0.9187,  0.2998,  0.6106,  0.7791,  0.1237,  1.8620,  1.7080,\n",
            "         -1.6045,  0.3338, -2.0513,  0.5923,  0.4880, -1.4055, -0.6686, -0.4831,\n",
            "         -0.2298,  0.9043,  0.7631, -0.1606,  0.9156, -0.6908, -0.3065, -1.1809,\n",
            "          0.8175, -2.0392,  0.1558, -0.2996, -0.5391, -0.3657,  0.8282, -0.4826,\n",
            "          1.8330,  0.3421,  0.2154, -0.1029, -0.0946,  0.0070,  0.1484, -0.5403,\n",
            "         -1.9312, -0.7858, -0.6731, -0.0901,  0.2598, -0.5349,  0.5812, -0.5356,\n",
            "         -1.7944]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0089, 0.0540, 0.0123, 0.0031, 0.0402, 0.0176, 0.0045, 0.0430, 0.0016,\n",
            "         0.0049, 0.0030, 0.0067, 0.0170, 0.0136, 0.0079, 0.0107, 0.0275, 0.0274,\n",
            "         0.0147, 0.0201, 0.0238, 0.0124, 0.0703, 0.0603, 0.0022, 0.0153, 0.0014,\n",
            "         0.0198, 0.0178, 0.0027, 0.0056, 0.0067, 0.0087, 0.0270, 0.0234, 0.0093,\n",
            "         0.0273, 0.0055, 0.0080, 0.0034, 0.0248, 0.0014, 0.0128, 0.0081, 0.0064,\n",
            "         0.0076, 0.0250, 0.0067, 0.0683, 0.0154, 0.0136, 0.0099, 0.0099, 0.0110,\n",
            "         0.0127, 0.0064, 0.0016, 0.0050, 0.0056, 0.0100, 0.0142, 0.0064, 0.0195,\n",
            "         0.0064, 0.0018]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[46]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1935, -0.6324, -0.2059,  ..., -0.5336, -1.3621,  0.3269],\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944],\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.0901,  0.2170, -2.9996,  1.4690, -0.1948, -0.1507,  0.2601, -0.9647,\n",
            "          0.1162, -0.8295, -0.2266,  0.0219, -0.2785, -0.4851, -1.8023, -0.7330,\n",
            "         -1.2828,  0.8863,  1.0515, -0.9823, -1.6369, -1.3499,  0.1830,  0.0532,\n",
            "         -1.1438, -0.2829, -0.5979,  1.4757,  0.4655, -3.0346,  0.5516,  1.3107,\n",
            "          0.1240, -1.8046,  0.2700, -0.4322,  0.2784, -0.5599,  1.2502,  0.7051,\n",
            "         -1.0169,  0.4854, -1.0808, -0.3128, -0.4189, -0.5718,  0.8215,  1.7384,\n",
            "          0.5578,  0.6167,  1.5260, -0.3508, -1.5615,  0.4548, -0.8935,  0.3642,\n",
            "          0.5714,  2.7072, -1.5443,  1.1288, -1.1217, -1.7328, -0.5472, -0.8017,\n",
            "          0.7761]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0303, 0.0126, 0.0005, 0.0442, 0.0084, 0.0087, 0.0132, 0.0039, 0.0114,\n",
            "         0.0044, 0.0081, 0.0104, 0.0077, 0.0063, 0.0017, 0.0049, 0.0028, 0.0247,\n",
            "         0.0291, 0.0038, 0.0020, 0.0026, 0.0122, 0.0107, 0.0032, 0.0077, 0.0056,\n",
            "         0.0445, 0.0162, 0.0005, 0.0177, 0.0377, 0.0115, 0.0017, 0.0133, 0.0066,\n",
            "         0.0134, 0.0058, 0.0355, 0.0206, 0.0037, 0.0165, 0.0035, 0.0074, 0.0067,\n",
            "         0.0057, 0.0231, 0.0579, 0.0178, 0.0188, 0.0468, 0.0072, 0.0021, 0.0160,\n",
            "         0.0042, 0.0146, 0.0180, 0.1524, 0.0022, 0.0315, 0.0033, 0.0018, 0.0059,\n",
            "         0.0046, 0.0221]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.2060,  1.5973,  0.1185,  ...,  0.5812, -0.5356, -1.7944],\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[2]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.3035, -0.4501,  1.3471,  1.6910, -0.1244, -1.6824, -0.0266,  0.0740,\n",
            "          1.0517,  0.6779,  0.3067, -0.7472,  0.7435,  0.8877,  2.2874,  0.9611,\n",
            "         -1.5297, -0.2912, -0.1140, -0.3137, -0.6293,  1.1385, -0.9913,  0.1700,\n",
            "          1.2249, -0.2345, -1.0572, -0.6543,  1.5909, -0.6995, -0.8961,  0.0662,\n",
            "         -0.0563,  2.3412, -2.7234,  0.5097, -0.8145, -0.2460,  0.0045,  2.0474,\n",
            "         -0.1575, -0.2187, -1.3519, -0.0573, -1.8540, -1.3849, -0.3454, -1.1625,\n",
            "          0.1445,  0.1663,  0.7507,  0.9132, -1.7277,  1.3055,  0.9593,  1.0600,\n",
            "          0.6299, -1.2867, -0.6875,  2.1382,  0.5114,  1.2191,  0.1910, -0.3425,\n",
            "          1.7955]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0288, 0.0050, 0.0301, 0.0424, 0.0069, 0.0015, 0.0076, 0.0084, 0.0224,\n",
            "         0.0154, 0.0106, 0.0037, 0.0164, 0.0190, 0.0770, 0.0204, 0.0017, 0.0058,\n",
            "         0.0070, 0.0057, 0.0042, 0.0244, 0.0029, 0.0093, 0.0266, 0.0062, 0.0027,\n",
            "         0.0041, 0.0384, 0.0039, 0.0032, 0.0084, 0.0074, 0.0813, 0.0005, 0.0130,\n",
            "         0.0035, 0.0061, 0.0079, 0.0606, 0.0067, 0.0063, 0.0020, 0.0074, 0.0012,\n",
            "         0.0020, 0.0055, 0.0024, 0.0090, 0.0092, 0.0166, 0.0195, 0.0014, 0.0288,\n",
            "         0.0204, 0.0226, 0.0147, 0.0022, 0.0039, 0.0663, 0.0130, 0.0265, 0.0095,\n",
            "         0.0056, 0.0471]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[47]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
            "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.6515, -0.0424, -0.7355, -0.7503, -0.7334, -0.4075,  0.4348,  1.8329,\n",
            "          1.8613,  0.3817,  0.0229, -0.6462,  1.1551,  1.1374,  1.1190,  0.7711,\n",
            "         -0.0498,  0.2669,  1.1184,  0.5508,  0.1840, -0.0272, -0.1389,  1.0381,\n",
            "         -1.8724,  1.6540,  1.2490,  1.7093,  1.6446, -1.2545,  1.0831, -0.4974,\n",
            "          0.4109, -2.0071, -0.0589,  0.9510,  0.0332, -0.8457, -0.5573, -0.0791,\n",
            "          1.1987, -0.0619, -0.9094, -1.3062, -0.7847,  0.1956, -0.2808, -0.5215,\n",
            "         -0.0914, -0.6785, -0.4260, -0.0618,  0.4917, -1.7436,  0.2129,  2.0566,\n",
            "          1.1735,  0.2264,  0.0785, -1.9412,  1.5760,  0.9797,  0.8682,  2.0593,\n",
            "         -0.8159]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0410, 0.0075, 0.0038, 0.0037, 0.0038, 0.0052, 0.0121, 0.0491, 0.0506,\n",
            "         0.0115, 0.0080, 0.0041, 0.0250, 0.0245, 0.0241, 0.0170, 0.0075, 0.0103,\n",
            "         0.0241, 0.0136, 0.0094, 0.0076, 0.0068, 0.0222, 0.0012, 0.0411, 0.0274,\n",
            "         0.0434, 0.0407, 0.0022, 0.0232, 0.0048, 0.0119, 0.0011, 0.0074, 0.0203,\n",
            "         0.0081, 0.0034, 0.0045, 0.0073, 0.0261, 0.0074, 0.0032, 0.0021, 0.0036,\n",
            "         0.0096, 0.0059, 0.0047, 0.0072, 0.0040, 0.0051, 0.0074, 0.0129, 0.0014,\n",
            "         0.0097, 0.0615, 0.0254, 0.0099, 0.0085, 0.0011, 0.0380, 0.0209, 0.0187,\n",
            "         0.0616, 0.0035]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[35]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
            "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.7681, -0.0430,  0.1073, -0.2059, -1.2616, -0.2270,  0.1066, -0.1683,\n",
            "          0.2433, -0.5709,  2.3408, -2.5665,  0.9935, -0.0124, -0.6971,  0.7903,\n",
            "          0.5051, -0.6004, -0.3287, -0.2818,  0.2799,  0.9678, -1.0858,  1.0346,\n",
            "          0.0428, -0.3025,  0.5520, -0.5940,  0.7718, -0.2502, -0.4973, -0.1547,\n",
            "          1.4592, -0.4754,  0.0821,  2.2645, -1.2837, -0.4539, -0.9422, -0.6697,\n",
            "          0.0602,  0.1464,  0.3007, -0.6955, -1.0944,  0.9295,  0.3155, -0.3942,\n",
            "         -0.0968,  1.3060, -1.2617, -0.0670,  0.2999, -0.7043, -0.0390, -1.8838,\n",
            "         -0.6024, -0.3366,  0.3025, -0.4242,  0.2360,  0.5425, -0.6499,  0.6144,\n",
            "          0.1669]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0237, 0.0105, 0.0122, 0.0089, 0.0031, 0.0088, 0.0122, 0.0093, 0.0140,\n",
            "         0.0062, 0.1141, 0.0008, 0.0297, 0.0108, 0.0055, 0.0242, 0.0182, 0.0060,\n",
            "         0.0079, 0.0083, 0.0145, 0.0289, 0.0037, 0.0309, 0.0115, 0.0081, 0.0191,\n",
            "         0.0061, 0.0238, 0.0086, 0.0067, 0.0094, 0.0473, 0.0068, 0.0119, 0.1057,\n",
            "         0.0030, 0.0070, 0.0043, 0.0056, 0.0117, 0.0127, 0.0148, 0.0055, 0.0037,\n",
            "         0.0278, 0.0151, 0.0074, 0.0100, 0.0405, 0.0031, 0.0103, 0.0148, 0.0054,\n",
            "         0.0106, 0.0017, 0.0060, 0.0078, 0.0149, 0.0072, 0.0139, 0.0189, 0.0057,\n",
            "         0.0203, 0.0130]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[35]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669],\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.7681, -0.0430,  0.1073, -0.2059, -1.2616, -0.2270,  0.1066, -0.1683,\n",
            "          0.2433, -0.5709,  2.3408, -2.5665,  0.9935, -0.0124, -0.6971,  0.7903,\n",
            "          0.5051, -0.6004, -0.3287, -0.2818,  0.2799,  0.9678, -1.0858,  1.0346,\n",
            "          0.0428, -0.3025,  0.5520, -0.5940,  0.7718, -0.2502, -0.4973, -0.1547,\n",
            "          1.4592, -0.4754,  0.0821,  2.2645, -1.2837, -0.4539, -0.9422, -0.6697,\n",
            "          0.0602,  0.1464,  0.3007, -0.6955, -1.0944,  0.9295,  0.3155, -0.3942,\n",
            "         -0.0968,  1.3060, -1.2617, -0.0670,  0.2999, -0.7043, -0.0390, -1.8838,\n",
            "         -0.6024, -0.3366,  0.3025, -0.4242,  0.2360,  0.5425, -0.6499,  0.6144,\n",
            "          0.1669]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0237, 0.0105, 0.0122, 0.0089, 0.0031, 0.0088, 0.0122, 0.0093, 0.0140,\n",
            "         0.0062, 0.1141, 0.0008, 0.0297, 0.0108, 0.0055, 0.0242, 0.0182, 0.0060,\n",
            "         0.0079, 0.0083, 0.0145, 0.0289, 0.0037, 0.0309, 0.0115, 0.0081, 0.0191,\n",
            "         0.0061, 0.0238, 0.0086, 0.0067, 0.0094, 0.0473, 0.0068, 0.0119, 0.1057,\n",
            "         0.0030, 0.0070, 0.0043, 0.0056, 0.0117, 0.0127, 0.0148, 0.0055, 0.0037,\n",
            "         0.0278, 0.0151, 0.0074, 0.0100, 0.0405, 0.0031, 0.0103, 0.0148, 0.0054,\n",
            "         0.0106, 0.0017, 0.0060, 0.0078, 0.0149, 0.0072, 0.0139, 0.0189, 0.0057,\n",
            "         0.0203, 0.0130]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[8]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669],\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669],\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.1407,  0.8935, -2.4000, -1.3420, -1.0023, -1.9870, -0.1217, -0.8074,\n",
            "          0.9255, -1.4814,  0.1291, -1.7058,  2.0330, -0.5062, -0.1016, -0.3016,\n",
            "          1.2501, -0.2137, -1.1994, -0.8466, -0.3351,  1.0041,  0.8656,  0.1688,\n",
            "         -0.2352, -0.2586,  0.0131,  0.8719,  0.9102, -0.1875,  0.7229,  1.4742,\n",
            "         -0.4048, -1.2273,  0.3382,  0.3641, -1.4508, -0.3814,  0.7220,  0.3461,\n",
            "          0.9967,  0.3575,  0.1187, -0.1062,  0.2990,  0.1199, -1.2433,  1.7859,\n",
            "          0.9191,  0.2326,  0.4591,  0.2556, -0.3542,  0.6690,  0.7535, -0.5359,\n",
            "         -1.0277,  0.5347, -0.7958,  0.4780,  0.1588, -2.3106,  0.3227,  1.5431,\n",
            "         -1.0392]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0328, 0.0256, 0.0010, 0.0027, 0.0038, 0.0014, 0.0093, 0.0047, 0.0265,\n",
            "         0.0024, 0.0119, 0.0019, 0.0801, 0.0063, 0.0095, 0.0078, 0.0366, 0.0085,\n",
            "         0.0032, 0.0045, 0.0075, 0.0286, 0.0249, 0.0124, 0.0083, 0.0081, 0.0106,\n",
            "         0.0251, 0.0261, 0.0087, 0.0216, 0.0458, 0.0070, 0.0031, 0.0147, 0.0151,\n",
            "         0.0025, 0.0072, 0.0216, 0.0148, 0.0284, 0.0150, 0.0118, 0.0094, 0.0141,\n",
            "         0.0118, 0.0030, 0.0625, 0.0263, 0.0132, 0.0166, 0.0135, 0.0074, 0.0205,\n",
            "         0.0223, 0.0061, 0.0038, 0.0179, 0.0047, 0.0169, 0.0123, 0.0010, 0.0145,\n",
            "         0.0491, 0.0037]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[27]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669],\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.1600,  1.3981, -0.7047, -0.1852, -0.6243,  1.2858,  0.1368, -0.2757,\n",
            "         -0.4958, -0.4759,  0.6607, -1.4100, -1.8479, -0.4986,  0.2404,  0.3062,\n",
            "         -1.3790,  0.6002,  0.1567, -0.1926, -0.1057,  1.2514,  0.7167,  0.6792,\n",
            "          1.7737,  0.7020, -0.1789,  0.1785, -1.0140, -0.1913,  0.2432, -1.2693,\n",
            "          0.9723, -0.2344,  0.2829,  0.4270,  0.6493, -0.3012, -0.4901, -1.3679,\n",
            "          2.2490,  0.5682,  1.5880, -0.7335, -1.6787, -0.0336, -1.5213,  0.3886,\n",
            "          1.0050, -1.2381,  1.3319,  0.1538,  0.6376, -0.7428,  1.6414, -0.2680,\n",
            "         -0.4543,  0.7176,  0.3635, -1.1256,  0.9422,  0.8838, -1.9908,  0.8574,\n",
            "         -2.1603]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0084, 0.0397, 0.0049, 0.0082, 0.0053, 0.0355, 0.0113, 0.0075, 0.0060,\n",
            "         0.0061, 0.0190, 0.0024, 0.0015, 0.0060, 0.0125, 0.0133, 0.0025, 0.0179,\n",
            "         0.0115, 0.0081, 0.0088, 0.0343, 0.0201, 0.0194, 0.0579, 0.0198, 0.0082,\n",
            "         0.0117, 0.0036, 0.0081, 0.0125, 0.0028, 0.0260, 0.0078, 0.0130, 0.0150,\n",
            "         0.0188, 0.0073, 0.0060, 0.0025, 0.0931, 0.0173, 0.0480, 0.0047, 0.0018,\n",
            "         0.0095, 0.0021, 0.0145, 0.0268, 0.0028, 0.0372, 0.0115, 0.0186, 0.0047,\n",
            "         0.0507, 0.0075, 0.0062, 0.0201, 0.0141, 0.0032, 0.0252, 0.0238, 0.0013,\n",
            "         0.0231, 0.0011]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[40]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.1407,  0.8935, -2.4000,  ...,  0.3227,  1.5431, -1.0392],\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
            "         [ 1.4311,  0.4160, -2.2246,  ...,  0.7330,  0.3551,  0.1472]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.4311,  0.4160, -2.2246,  0.3890,  0.2493, -0.0899, -0.7845, -0.9330,\n",
            "         -1.4031,  0.3225,  0.5351, -0.2257,  1.7677,  0.2006, -0.8241, -1.4160,\n",
            "          0.4026,  0.2599, -0.5641,  0.3911,  1.7038,  0.5841, -0.1505, -0.0125,\n",
            "          0.7221, -0.9629, -2.0578,  1.9740,  0.7434,  1.1139,  0.6926,  0.0296,\n",
            "          0.6405, -1.6464,  0.4935,  0.7485,  0.9238, -0.4940,  0.4814, -0.3859,\n",
            "         -0.3094,  1.1066, -0.2891,  0.1891,  2.0440, -0.7945, -0.4331,  0.3007,\n",
            "          1.4317,  0.2881, -0.4343,  0.4280,  1.2469,  1.4047, -0.3404, -2.2190,\n",
            "          0.4893,  0.0398, -0.2717, -2.2400, -0.0029, -1.4251,  0.7330,  0.3551,\n",
            "          0.1472]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0392, 0.0142, 0.0010, 0.0138, 0.0120, 0.0086, 0.0043, 0.0037, 0.0023,\n",
            "         0.0129, 0.0160, 0.0075, 0.0548, 0.0114, 0.0041, 0.0023, 0.0140, 0.0121,\n",
            "         0.0053, 0.0138, 0.0514, 0.0168, 0.0081, 0.0092, 0.0193, 0.0036, 0.0012,\n",
            "         0.0674, 0.0197, 0.0285, 0.0187, 0.0096, 0.0178, 0.0018, 0.0153, 0.0198,\n",
            "         0.0236, 0.0057, 0.0151, 0.0064, 0.0069, 0.0283, 0.0070, 0.0113, 0.0723,\n",
            "         0.0042, 0.0061, 0.0126, 0.0392, 0.0125, 0.0061, 0.0144, 0.0326, 0.0381,\n",
            "         0.0067, 0.0010, 0.0153, 0.0097, 0.0071, 0.0010, 0.0093, 0.0023, 0.0195,\n",
            "         0.0134, 0.0108]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[64]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.1600,  1.3981, -0.7047,  ..., -1.9908,  0.8574, -2.1603],\n",
            "         [ 1.4311,  0.4160, -2.2246,  ...,  0.7330,  0.3551,  0.1472],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.4322, -0.2810, -2.2789, -1.5010, -0.5178, -0.0930,  0.7448,  0.2769,\n",
            "         -1.3683, -0.1367,  0.5261,  0.8502,  0.5255, -1.4073, -0.8778,  1.5681,\n",
            "          0.5790, -1.0601, -0.1289,  0.0574, -2.1171,  0.5979, -0.8894, -0.1832,\n",
            "          2.1316,  0.4207, -1.9636, -0.4431,  2.0773, -0.8678,  0.4456, -0.8511,\n",
            "         -0.9897, -0.1547, -0.3183,  0.9285,  0.7569,  0.9505, -1.4028, -0.5422,\n",
            "          0.3932, -1.1699,  0.9138,  1.2533, -0.5639, -0.4533, -0.5694, -1.3843,\n",
            "         -0.1265,  1.6687,  0.4180,  1.1220, -0.4981,  1.7805, -0.3438,  0.0917,\n",
            "          1.4146, -0.9541,  0.4243, -0.4152, -0.9518, -0.9530, -0.5551,  1.0666,\n",
            "          0.5364]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0024, 0.0077, 0.0010, 0.0023, 0.0060, 0.0092, 0.0214, 0.0134, 0.0026,\n",
            "         0.0089, 0.0172, 0.0237, 0.0172, 0.0025, 0.0042, 0.0487, 0.0181, 0.0035,\n",
            "         0.0089, 0.0107, 0.0012, 0.0185, 0.0042, 0.0084, 0.0855, 0.0155, 0.0014,\n",
            "         0.0065, 0.0810, 0.0043, 0.0158, 0.0043, 0.0038, 0.0087, 0.0074, 0.0257,\n",
            "         0.0216, 0.0263, 0.0025, 0.0059, 0.0150, 0.0031, 0.0253, 0.0355, 0.0058,\n",
            "         0.0064, 0.0057, 0.0025, 0.0089, 0.0538, 0.0154, 0.0312, 0.0062, 0.0602,\n",
            "         0.0072, 0.0111, 0.0418, 0.0039, 0.0155, 0.0067, 0.0039, 0.0039, 0.0058,\n",
            "         0.0295, 0.0173]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[16]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 1.4311,  0.4160, -2.2246,  ...,  0.7330,  0.3551,  0.1472],\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-1.9280, -0.2083, -0.6648, -0.2641,  1.7087, -1.3229, -0.1711,  0.6874,\n",
            "          1.9919,  1.2724,  0.2194, -1.2614, -1.0279,  0.4589, -0.6532,  1.1831,\n",
            "         -0.3666,  0.4472, -0.2843,  0.2079,  1.5236,  0.7154,  0.6585,  0.4589,\n",
            "         -0.5357,  1.2628, -1.5580,  0.4196, -0.3806, -0.7044, -0.3884, -1.8927,\n",
            "          0.8618,  1.2285, -0.1141,  1.7779,  0.8297,  0.3457, -0.3843, -1.5953,\n",
            "         -0.1820, -0.5678,  0.6511, -0.4463, -0.2109, -0.9229,  0.2308, -0.7586,\n",
            "         -1.6410, -1.3728, -1.3389, -0.5246,  1.9338,  0.3034,  0.2951,  1.5918,\n",
            "         -1.6319, -1.1146, -0.3941,  0.4972, -2.0602, -1.8428,  0.8573,  1.3835,\n",
            "          0.4217]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0014, 0.0079, 0.0050, 0.0075, 0.0541, 0.0026, 0.0082, 0.0195, 0.0717,\n",
            "         0.0349, 0.0122, 0.0028, 0.0035, 0.0155, 0.0051, 0.0320, 0.0068, 0.0153,\n",
            "         0.0074, 0.0121, 0.0449, 0.0200, 0.0189, 0.0155, 0.0057, 0.0346, 0.0021,\n",
            "         0.0149, 0.0067, 0.0048, 0.0066, 0.0015, 0.0232, 0.0334, 0.0087, 0.0579,\n",
            "         0.0224, 0.0138, 0.0067, 0.0020, 0.0082, 0.0055, 0.0188, 0.0063, 0.0079,\n",
            "         0.0039, 0.0123, 0.0046, 0.0019, 0.0025, 0.0026, 0.0058, 0.0677, 0.0133,\n",
            "         0.0131, 0.0481, 0.0019, 0.0032, 0.0066, 0.0161, 0.0012, 0.0016, 0.0231,\n",
            "         0.0390, 0.0149]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[52]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364],\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.2103,  0.4481,  1.2381,  0.6091,  0.4209, -1.3998, -0.4007, -0.1462,\n",
            "         -1.1387, -0.0134, -1.9390,  0.6582,  0.6734,  0.7523, -0.7533, -0.1611,\n",
            "          0.8354,  1.7862, -0.7115,  0.2381, -1.2085,  0.0717,  0.1532, -2.2039,\n",
            "          0.6705,  0.2791, -0.2735,  0.3476, -0.2101,  2.2659,  1.0809, -0.4287,\n",
            "         -0.2424, -0.2258, -0.3756,  0.3991,  0.7816,  1.2656,  1.3015,  0.1370,\n",
            "          0.9264, -1.1001, -1.0641,  0.6243, -0.2436,  0.1079,  0.4256,  0.1838,\n",
            "         -0.1147,  1.7112,  1.2666,  0.7957,  0.9475,  1.3239,  2.0084,  1.4341,\n",
            "         -0.4606, -0.0487, -1.1202,  1.3071,  0.0508,  0.2770,  1.3597, -0.0821,\n",
            "          0.3909]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0067, 0.0130, 0.0286, 0.0153, 0.0127, 0.0020, 0.0056, 0.0072, 0.0027,\n",
            "         0.0082, 0.0012, 0.0160, 0.0163, 0.0176, 0.0039, 0.0071, 0.0192, 0.0496,\n",
            "         0.0041, 0.0105, 0.0025, 0.0089, 0.0097, 0.0009, 0.0162, 0.0110, 0.0063,\n",
            "         0.0118, 0.0067, 0.0801, 0.0245, 0.0054, 0.0065, 0.0066, 0.0057, 0.0124,\n",
            "         0.0181, 0.0294, 0.0305, 0.0095, 0.0210, 0.0028, 0.0029, 0.0155, 0.0065,\n",
            "         0.0093, 0.0127, 0.0100, 0.0074, 0.0460, 0.0295, 0.0184, 0.0214, 0.0312,\n",
            "         0.0619, 0.0348, 0.0052, 0.0079, 0.0027, 0.0307, 0.0087, 0.0110, 0.0324,\n",
            "         0.0077, 0.0123]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[62]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-1.9280, -0.2083, -0.6648,  ...,  0.8573,  1.3835,  0.4217],\n",
            "         [-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.4222, -1.8111, -1.0118, -0.4569,  0.0202, -0.1555, -0.1721,  0.2664,\n",
            "         -0.2054, -1.3252,  0.6271,  1.4733,  0.9470, -1.0751,  0.9042, -1.4850,\n",
            "         -0.3449,  1.3128, -0.5799, -0.6507, -0.1817,  0.9045,  1.2905,  0.6977,\n",
            "         -1.1612,  0.0093, -1.0707,  0.1294, -0.4570,  0.9330, -0.2858, -0.9957,\n",
            "          0.5773, -0.5476, -2.0319, -0.1269, -0.5162,  0.2046,  1.4801,  1.6253,\n",
            "         -0.8076,  0.8477, -1.0219, -1.9241,  1.4480, -1.8130, -0.0638, -0.1206,\n",
            "          1.5924, -0.1682,  1.7089, -0.1853,  0.1268, -1.2130, -0.4556,  1.1074,\n",
            "          1.4224, -1.8213, -0.0451, -0.1755, -0.9631, -0.5043,  0.5462,  0.2788,\n",
            "          0.7280]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0161, 0.0017, 0.0038, 0.0067, 0.0108, 0.0090, 0.0089, 0.0138, 0.0086,\n",
            "         0.0028, 0.0197, 0.0460, 0.0272, 0.0036, 0.0261, 0.0024, 0.0075, 0.0392,\n",
            "         0.0059, 0.0055, 0.0088, 0.0261, 0.0383, 0.0212, 0.0033, 0.0106, 0.0036,\n",
            "         0.0120, 0.0067, 0.0268, 0.0079, 0.0039, 0.0188, 0.0061, 0.0014, 0.0093,\n",
            "         0.0063, 0.0129, 0.0463, 0.0536, 0.0047, 0.0246, 0.0038, 0.0015, 0.0449,\n",
            "         0.0017, 0.0099, 0.0093, 0.0518, 0.0089, 0.0583, 0.0088, 0.0120, 0.0031,\n",
            "         0.0067, 0.0319, 0.0437, 0.0017, 0.0101, 0.0088, 0.0040, 0.0064, 0.0182,\n",
            "         0.0139, 0.0218]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[13]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 0.2073,  0.8193,  0.3750,  ..., -0.9352, -0.6125, -0.3161]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.2073,  0.8193,  0.3750, -0.0866, -1.9617,  1.4797, -1.2323,  1.5381,\n",
            "          0.5985,  0.1292,  1.5930, -1.0964, -0.5616,  0.6586, -1.3565, -1.8218,\n",
            "         -0.0433, -0.2505, -0.7493,  0.4368,  0.4685,  0.4910,  1.0993,  0.1871,\n",
            "          1.2346, -0.8830, -0.7533, -0.4114, -0.4392, -0.3994, -0.0385,  0.5464,\n",
            "          0.6994,  2.3432,  0.7323, -0.2561,  1.4587, -0.3098, -1.7464,  0.5360,\n",
            "         -0.3436,  2.3933,  0.2183, -1.3731, -0.9189, -1.9113, -0.0046,  0.0700,\n",
            "         -2.7447,  1.0874, -0.9978,  0.8634, -1.2265, -0.5305,  0.0983, -0.3731,\n",
            "          0.3765, -0.9880, -0.1968, -0.6044, -0.5986, -0.5245, -0.9352, -0.6125,\n",
            "         -0.3161]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0124, 0.0229, 0.0147, 0.0093, 0.0014, 0.0443, 0.0029, 0.0470, 0.0184,\n",
            "         0.0115, 0.0496, 0.0034, 0.0058, 0.0195, 0.0026, 0.0016, 0.0097, 0.0079,\n",
            "         0.0048, 0.0156, 0.0161, 0.0165, 0.0303, 0.0122, 0.0347, 0.0042, 0.0048,\n",
            "         0.0067, 0.0065, 0.0068, 0.0097, 0.0174, 0.0203, 0.1051, 0.0210, 0.0078,\n",
            "         0.0434, 0.0074, 0.0018, 0.0173, 0.0072, 0.1105, 0.0126, 0.0026, 0.0040,\n",
            "         0.0015, 0.0100, 0.0108, 0.0006, 0.0299, 0.0037, 0.0239, 0.0030, 0.0059,\n",
            "         0.0111, 0.0069, 0.0147, 0.0038, 0.0083, 0.0055, 0.0055, 0.0060, 0.0040,\n",
            "         0.0055, 0.0074]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[1]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
            "         [ 0.2073,  0.8193,  0.3750,  ..., -0.9352, -0.6125, -0.3161],\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.5978, -0.0514, -0.0646, -0.4970,  0.4658, -0.2573, -1.0673,  2.0089,\n",
            "         -0.5370,  0.2228,  0.6971, -1.4267,  0.9059,  0.1446,  0.2280,  2.4900,\n",
            "         -1.2237,  1.0107,  0.5560, -1.5935, -1.2706,  0.6903, -0.1961,  0.3449,\n",
            "         -0.3419,  0.4759, -0.7663, -0.4190, -0.4370, -1.0012, -0.4094, -1.6669,\n",
            "         -1.3651, -0.1655,  0.9623,  0.0315, -0.7419, -0.2978,  0.0172, -0.1772,\n",
            "         -0.1334,  0.2940,  1.3850,  0.1209,  2.5418, -0.6405, -1.9740, -0.3296,\n",
            "          0.0080,  0.9262, -1.8846,  0.1670,  0.4586, -1.7662,  0.5860,  1.7510,\n",
            "          0.2807,  0.3110, -0.6538, -0.6576,  0.3184, -0.5496, -1.4649, -2.0555,\n",
            "          1.8275]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0172, 0.0090, 0.0088, 0.0057, 0.0150, 0.0073, 0.0032, 0.0703, 0.0055,\n",
            "         0.0118, 0.0189, 0.0023, 0.0233, 0.0109, 0.0118, 0.1138, 0.0028, 0.0259,\n",
            "         0.0164, 0.0019, 0.0026, 0.0188, 0.0078, 0.0133, 0.0067, 0.0152, 0.0044,\n",
            "         0.0062, 0.0061, 0.0035, 0.0063, 0.0018, 0.0024, 0.0080, 0.0247, 0.0097,\n",
            "         0.0045, 0.0070, 0.0096, 0.0079, 0.0083, 0.0127, 0.0377, 0.0106, 0.1198,\n",
            "         0.0050, 0.0013, 0.0068, 0.0095, 0.0238, 0.0014, 0.0111, 0.0149, 0.0016,\n",
            "         0.0170, 0.0543, 0.0125, 0.0129, 0.0049, 0.0049, 0.0130, 0.0054, 0.0022,\n",
            "         0.0012, 0.0587]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[25]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.2073,  0.8193,  0.3750,  ..., -0.9352, -0.6125, -0.3161],\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 0.0691,  0.2990, -1.4717,  0.9950,  0.3608,  0.3161,  0.3504, -1.7823,\n",
            "          0.1339, -2.0973,  1.9108,  1.6555,  2.0254,  0.6044, -0.7006,  0.8141,\n",
            "          0.2263, -0.8224, -1.1513,  0.1186, -0.3123, -0.6024, -0.1058, -0.5325,\n",
            "          0.1415, -0.0339, -0.6461,  0.5560, -0.0698, -0.7516, -1.7028, -0.6811,\n",
            "         -1.2044, -0.2007,  1.3154, -0.4974, -0.2338, -0.9047,  0.4135, -0.6663,\n",
            "          1.2759,  0.3141, -1.1177, -1.1179, -0.4851,  1.4299, -0.2522, -1.0614,\n",
            "          1.2459,  2.1203,  1.7902,  0.8941,  0.0293, -0.2685,  1.7898,  0.6121,\n",
            "         -0.3263,  0.5659, -1.1200,  0.3157, -1.4606, -0.6966,  0.1517,  0.8528,\n",
            "          0.0604]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0100, 0.0126, 0.0021, 0.0253, 0.0134, 0.0128, 0.0133, 0.0016, 0.0107,\n",
            "         0.0011, 0.0631, 0.0489, 0.0708, 0.0171, 0.0046, 0.0211, 0.0117, 0.0041,\n",
            "         0.0030, 0.0105, 0.0068, 0.0051, 0.0084, 0.0055, 0.0108, 0.0090, 0.0049,\n",
            "         0.0163, 0.0087, 0.0044, 0.0017, 0.0047, 0.0028, 0.0076, 0.0348, 0.0057,\n",
            "         0.0074, 0.0038, 0.0141, 0.0048, 0.0334, 0.0128, 0.0031, 0.0031, 0.0057,\n",
            "         0.0390, 0.0073, 0.0032, 0.0325, 0.0778, 0.0559, 0.0228, 0.0096, 0.0071,\n",
            "         0.0559, 0.0172, 0.0067, 0.0164, 0.0030, 0.0128, 0.0022, 0.0047, 0.0109,\n",
            "         0.0219, 0.0099]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[57]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25, 57]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
            "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
            "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
            "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
            "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
            "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
            "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
            "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
            "          1.6014]], grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0051, 0.0114, 0.0253, 0.0627, 0.0186, 0.0037, 0.0199, 0.0189, 0.0050,\n",
            "         0.0076, 0.0166, 0.0044, 0.0013, 0.0181, 0.0733, 0.0246, 0.0041, 0.0679,\n",
            "         0.0026, 0.0039, 0.0097, 0.0034, 0.0153, 0.0015, 0.0025, 0.0057, 0.0033,\n",
            "         0.0033, 0.0066, 0.0031, 0.0033, 0.0074, 0.0140, 0.0026, 0.0256, 0.0048,\n",
            "         0.0087, 0.0011, 0.0081, 0.0057, 0.0042, 0.0211, 0.0062, 0.0067, 0.0030,\n",
            "         0.0076, 0.0020, 0.0199, 0.0043, 0.0224, 0.0201, 0.0129, 0.0009, 0.0059,\n",
            "         0.0043, 0.0131, 0.1246, 0.0253, 0.0165, 0.0607, 0.0099, 0.0080, 0.0084,\n",
            "         0.0190, 0.0425]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[3]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3]])\n",
            "logits in generate tensor([[[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
            "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
            "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
            "         ...,\n",
            "         [ 0.0691,  0.2990, -1.4717,  ...,  0.1517,  0.8528,  0.0604],\n",
            "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
            "         [ 1.3915,  1.0785, -0.6150,  ..., -1.0917, -0.8207,  1.8634]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "logits in generate after -1 tensor([[ 1.3915e+00,  1.0785e+00, -6.1495e-01, -4.5885e-01,  5.6748e-01,\n",
            "          1.8289e-02, -1.6608e+00,  1.1169e+00,  5.1965e-01, -1.2423e+00,\n",
            "         -9.6182e-01, -8.4998e-02,  1.1854e-01,  2.9843e-01, -7.2636e-01,\n",
            "         -3.1187e-01, -4.5604e-01,  6.4407e-01,  6.0728e-01,  1.2397e+00,\n",
            "          7.3249e-01,  5.0418e-01,  8.7135e-01, -2.7416e-01, -7.4689e-01,\n",
            "         -5.8324e-01,  3.6988e-01, -5.5562e-01, -3.9828e-01, -5.8188e-01,\n",
            "         -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,  8.2161e-01,\n",
            "          3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,  6.1587e-01,\n",
            "         -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01, -4.1474e-01,\n",
            "          1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,  4.2716e-01,\n",
            "         -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,  1.8299e-01,\n",
            "         -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00, -5.2756e-01,\n",
            "          1.0807e+00,  4.5618e-01, -1.0917e+00, -8.2073e-01,  1.8634e+00]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "probs in generate after softmax tensor([[0.0485, 0.0355, 0.0065, 0.0076, 0.0213, 0.0123, 0.0023, 0.0369, 0.0203,\n",
            "         0.0035, 0.0046, 0.0111, 0.0136, 0.0163, 0.0058, 0.0088, 0.0077, 0.0230,\n",
            "         0.0222, 0.0417, 0.0251, 0.0200, 0.0289, 0.0092, 0.0057, 0.0067, 0.0175,\n",
            "         0.0069, 0.0081, 0.0067, 0.0097, 0.0122, 0.0089, 0.0117, 0.0275, 0.0121,\n",
            "         0.0077, 0.0274, 0.0026, 0.0224, 0.0019, 0.0045, 0.0129, 0.0077, 0.0080,\n",
            "         0.0540, 0.0116, 0.0054, 0.0038, 0.0185, 0.0091, 0.0119, 0.0050, 0.0087,\n",
            "         0.0145, 0.0048, 0.0035, 0.0370, 0.0018, 0.0071, 0.0356, 0.0191, 0.0041,\n",
            "         0.0053, 0.0778]], grad_fn=<SoftmaxBackward0>)\n",
            "idx_next in generate tensor([[9]])\n",
            "final idx in generate after cat tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
            "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
            "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
            "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
            "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
            "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rZUOADMtXuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.65630578994751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "oTo.JUZ!!zqe!\n",
            "xBP qbs$Gy'AcOmrLwwt\n",
            "p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\n",
            "rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\n",
            "ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\n",
            "SV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\n",
            "tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\n",
            "pSPYgCuCJrIFtb\n",
            "jQXg\n",
            "pA.P LP,SPJi\n",
            "DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\n",
            "D.?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4198, val loss 2.4340\n",
            "step 400: train loss 2.3503, val loss 2.3567\n",
            "step 500: train loss 2.2970, val loss 2.3136\n",
            "step 600: train loss 2.2410, val loss 2.2506\n",
            "step 700: train loss 2.2062, val loss 2.2198\n",
            "step 800: train loss 2.1638, val loss 2.1871\n",
            "step 900: train loss 2.1232, val loss 2.1494\n",
            "step 1000: train loss 2.1020, val loss 2.1293\n",
            "step 1100: train loss 2.0704, val loss 2.1196\n",
            "step 1200: train loss 2.0382, val loss 2.0798\n",
            "step 1300: train loss 2.0249, val loss 2.0640\n",
            "step 1400: train loss 1.9922, val loss 2.0354\n",
            "step 1500: train loss 1.9707, val loss 2.0308\n",
            "step 1600: train loss 1.9614, val loss 2.0474\n",
            "step 1700: train loss 1.9393, val loss 2.0130\n",
            "step 1800: train loss 1.9070, val loss 1.9943\n",
            "step 1900: train loss 1.9057, val loss 1.9871\n",
            "step 2000: train loss 1.8834, val loss 1.9954\n",
            "step 2100: train loss 1.8719, val loss 1.9758\n",
            "step 2200: train loss 1.8582, val loss 1.9623\n",
            "step 2300: train loss 1.8546, val loss 1.9517\n",
            "step 2400: train loss 1.8410, val loss 1.9476\n",
            "step 2500: train loss 1.8167, val loss 1.9455\n",
            "step 2600: train loss 1.8263, val loss 1.9401\n",
            "step 2700: train loss 1.8108, val loss 1.9340\n",
            "step 2800: train loss 1.8040, val loss 1.9247\n",
            "step 2900: train loss 1.8044, val loss 1.9304\n",
            "step 3000: train loss 1.7963, val loss 1.9242\n",
            "step 3100: train loss 1.7687, val loss 1.9147\n",
            "step 3200: train loss 1.7547, val loss 1.9102\n",
            "step 3300: train loss 1.7557, val loss 1.9037\n",
            "step 3400: train loss 1.7547, val loss 1.8946\n",
            "step 3500: train loss 1.7385, val loss 1.8968\n",
            "step 3600: train loss 1.7260, val loss 1.8914\n",
            "step 3700: train loss 1.7257, val loss 1.8808\n",
            "step 3800: train loss 1.7204, val loss 1.8919\n",
            "step 3900: train loss 1.7215, val loss 1.8788\n",
            "step 4000: train loss 1.7146, val loss 1.8639\n",
            "step 4100: train loss 1.7095, val loss 1.8724\n",
            "step 4200: train loss 1.7079, val loss 1.8707\n",
            "step 4300: train loss 1.7035, val loss 1.8502\n",
            "step 4400: train loss 1.7043, val loss 1.8693\n",
            "step 4500: train loss 1.6914, val loss 1.8522\n",
            "step 4600: train loss 1.6853, val loss 1.8357\n",
            "step 4700: train loss 1.6862, val loss 1.8483\n",
            "step 4800: train loss 1.6671, val loss 1.8434\n",
            "step 4900: train loss 1.6736, val loss 1.8415\n",
            "step 4999: train loss 1.6635, val loss 1.8226\n",
            "\n",
            "FlY BOLINGLO:\n",
            "Them thrumply towiter arts the\n",
            "muscue rike begatt the sea it\n",
            "What satell in rowers that some than othis Marrity.\n",
            "\n",
            "LUCENTVO:\n",
            "But userman these that, where can is not diesty rege;\n",
            "What and see to not. But's eyes. What?\n",
            "\n",
            "JOHN MARGARET:\n",
            "Than up I wark, what out, I ever of and love,\n",
            "one these do sponce, vois I me;\n",
            "But my pray sape to ries all to the not erralied in may.\n",
            "\n",
            "BENVOLIO:\n",
            "To spits as stold's bewear I would and say mesby all\n",
            "on sworn make he anough\n",
            "As cousins the solle, whose be my conforeful may lie them yet\n",
            "nobe allimely untraled to be thre I say be,\n",
            "Notham a brotes theme an make come,\n",
            "And that his reach to the duke ento\n",
            "the grmeants bell! and now there king-liff-or grief?\n",
            "\n",
            "GLOUCESTER:\n",
            "All the bettle dreene, for To his like thou thron!\n",
            "\n",
            "MENENIUS:\n",
            "Then, if I knom her all.\n",
            "My lord, but terruly friend\n",
            "Rish of the ploceiness and wilt tends sure?\n",
            "Is you knows a fasir wead\n",
            "That with him my spaut,\n",
            "I shall not tas where's not, becomity; my coulds sting,\n",
            "then the wit be dong to tyget our hereefore,\n",
            "Who strop me, mend here, if agains, bitten, thy lack.\n",
            "The but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\n",
            "How the enclady: It you and how,\n",
            "I am in him, And ladderle:\n",
            "Their hand whose wife, it my hithre,\n",
            "Roman and where sposs gives'd you.\n",
            "\n",
            "TROMIOLANUS:\n",
            "But livants you great, I shom mistrot come, for to she to lot\n",
            "for smy to men ventry mehus. Gazise;\n",
            "Full't were some the cause, and stouch set,\n",
            "Or promises, which a kingsasted to your gove them; and sterrer,\n",
            "And that wae love him.\n",
            "\n",
            "BRUTUS:\n",
            "You shape with these sweet.\n",
            "\n",
            "CORTENGONO:\n",
            "Lo, where 'twon elmes, 'morth young agres;\n",
            "Sir, azavoust to striel accurded we missery sets crave.\n",
            "\n",
            "ANGOLUM:\n",
            "For is Henry to have gleise the dreason\n",
            "That I ant shorfold wefth their servy in enscy.\n",
            "\n",
            "ISABELLA:\n",
            "O, I better you eyse such formfetrews.\n",
            "\n",
            "BUCKINGHARENT:\n",
            "Qead my lightle this righanneds flase them\n",
            "Wam which an take was our some pleasurs,\n",
            "Lovisoname to me, then fult me?--have it?\n",
            "\n",
            "HENRY BOLINGBROY:\n",
            "That wha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}